{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyFlink Installation\n",
    "\n",
    "### By Fatty\n",
    "\n",
    "### Version 2023.11.15\n",
    "\n",
    "## Anaconda3 Installation\n",
    "\n",
    "Download the Anaconda3 package from TUNA first.\n",
    "\n",
    "```Bash\n",
    "wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-2023.09-0-Linux-x86_64.sh\n",
    "sh Anaconda3-2023.09-0-Linux-x86_64.sh\n",
    "```\n",
    "During the installation, please use the default settings.\n",
    "It should be installed at `~/anaconda3`.\n",
    "\n",
    "## Python 3.9 Installation\n",
    "\n",
    "Python 3.9 installed by conda will be easy and reliable.\n",
    "\n",
    "```Bash\n",
    "conda create -n pyflink_39 python=3.9\n",
    "conda activate pyflink_39\n",
    "```\n",
    "\n",
    "## Apache-Flink Installation\n",
    "\n",
    "Then install the apache-flink package with pip.\n",
    "\n",
    "```Bash\n",
    "pip install apache-flink\n",
    "```\n",
    "\n",
    "## Some Tests\n",
    "\n",
    "The following code comes from the official [documents version 1.18](https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/dev/python/datastream_tutorial/). \n",
    "And there might be some tiny modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\HP\\Downloads\\备课\\于-流数据分析讲义PPT\\PyFlink_A_Brief_Tutorial.ipynb Cell 2\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HP/Downloads/%E5%A4%87%E8%AF%BE/%E4%BA%8E-%E6%B5%81%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E8%AE%B2%E4%B9%89PPT/PyFlink_A_Brief_Tutorial.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HP/Downloads/%E5%A4%87%E8%AF%BE/%E4%BA%8E-%E6%B5%81%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E8%AE%B2%E4%B9%89PPT/PyFlink_A_Brief_Tutorial.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Get current absolute path\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/HP/Downloads/%E5%A4%87%E8%AF%BE/%E4%BA%8E-%E6%B5%81%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E8%AE%B2%E4%B9%89PPT/PyFlink_A_Brief_Tutorial.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m current_file_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mabspath(\u001b[39m__file__\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HP/Downloads/%E5%A4%87%E8%AF%BE/%E4%BA%8E-%E6%B5%81%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E8%AE%B2%E4%B9%89PPT/PyFlink_A_Brief_Tutorial.ipynb#W1sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Get current dir path\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HP/Downloads/%E5%A4%87%E8%AF%BE/%E4%BA%8E-%E6%B5%81%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E8%AE%B2%E4%B9%89PPT/PyFlink_A_Brief_Tutorial.ipynb#W1sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m current_dir_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(current_file_path)\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "# Test with a Flink Python DataStream API Program\n",
    "# The following code comes from the official [documents version 1.18](https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/dev/python/datastream_tutorial/).\n",
    "# Save the code below as `DataStream_API_word_count.py`.\n",
    "# import os\n",
    "# # Get current absolute path\n",
    "# current_file_path = os.path.abspath(__file__)\n",
    "# # Get current dir path\n",
    "# current_dir_path = os.path.dirname(current_file_path)\n",
    "# # Change into current dir path\n",
    "# os.chdir(current_dir_path)\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from pyflink.table import StreamTableEnvironment\n",
    "from pyflink.common import WatermarkStrategy, Encoder, Types\n",
    "from pyflink.datastream import StreamExecutionEnvironment, RuntimeExecutionMode\n",
    "from pyflink.datastream.connectors.file_system import FileSource, StreamFormat, FileSink, OutputFileConfig, RollingPolicy\n",
    "\n",
    "\n",
    "word_count_data = [\"To be, or not to be,--that is the question:--\",\n",
    "                   \"Whether 'tis nobler in the mind to suffer\",\n",
    "                   \"The slings and arrows of outrageous fortune\",\n",
    "                   \"Or to take arms against a sea of troubles,\",\n",
    "                   \"And by opposing end them?--To die,--to sleep,--\",\n",
    "                   \"No more; and by a sleep to say we end\",\n",
    "                   \"The heartache, and the thousand natural shocks\",\n",
    "                   \"That flesh is heir to,--'tis a consummation\",\n",
    "                   \"Devoutly to be wish'd. To die,--to sleep;--\",\n",
    "                   \"To sleep! perchance to dream:--ay, there's the rub;\",\n",
    "                   \"For in that sleep of death what dreams may come,\",\n",
    "                   \"When we have shuffled off this mortal coil,\",\n",
    "                   \"Must give us pause: there's the respect\",\n",
    "                   \"That makes calamity of so long life;\",\n",
    "                   \"For who would bear the whips and scorns of time,\",\n",
    "                   \"The oppressor's wrong, the proud man's contumely,\",\n",
    "                   \"The pangs of despis'd love, the law's delay,\",\n",
    "                   \"The insolence of office, and the spurns\",\n",
    "                   \"That patient merit of the unworthy takes,\",\n",
    "                   \"When he himself might his quietus make\",\n",
    "                   \"With a bare bodkin? who would these fardels bear,\",\n",
    "                   \"To grunt and sweat under a weary life,\",\n",
    "                   \"But that the dread of something after death,--\",\n",
    "                   \"The undiscover'd country, from whose bourn\",\n",
    "                   \"No traveller returns,--puzzles the will,\",\n",
    "                   \"And makes us rather bear those ills we have\",\n",
    "                   \"Than fly to others that we know not of?\",\n",
    "                   \"Thus conscience does make cowards of us all;\",\n",
    "                   \"And thus the native hue of resolution\",\n",
    "                   \"Is sicklied o'er with the pale cast of thought;\",\n",
    "                   \"And enterprises of great pith and moment,\",\n",
    "                   \"With this regard, their currents turn awry,\",\n",
    "                   \"And lose the name of action.--Soft you now!\",\n",
    "                   \"The fair Ophelia!--Nymph, in thy orisons\",\n",
    "                   \"Be all my sins remember'd.\"]\n",
    "\n",
    "\n",
    "def word_count(input_path, output_path):\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()\n",
    "    env.set_runtime_mode(RuntimeExecutionMode.BATCH)\n",
    "    # write all the data to one file\n",
    "    env.set_parallelism(1)\n",
    "\n",
    "    # define the source\n",
    "    if input_path is not None:\n",
    "        ds = env.from_source(\n",
    "            source=FileSource.for_record_stream_format(StreamFormat.text_line_format(),\n",
    "                                                       input_path)\n",
    "                             .process_static_file_set().build(),\n",
    "            watermark_strategy=WatermarkStrategy.for_monotonous_timestamps(),\n",
    "            source_name=\"file_source\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"Executing word_count example with default input data set.\")\n",
    "        print(\"Use --input to specify file input.\")\n",
    "        ds = env.from_collection(word_count_data)\n",
    "\n",
    "    def split(line):\n",
    "        yield from line.split()\n",
    "\n",
    "    # compute word count\n",
    "    ds = ds.flat_map(split) \\\n",
    "        .map(lambda i: (i, 1), output_type=Types.TUPLE([Types.STRING(), Types.INT()])) \\\n",
    "        .key_by(lambda i: i[0]) \\\n",
    "        .reduce(lambda i, j: (i[0], i[1] + j[1]))\n",
    "\n",
    "    # define the sink\n",
    "    if output_path is not None:\n",
    "        ds.sink_to(\n",
    "            sink=FileSink.for_row_format(\n",
    "                base_path=output_path,\n",
    "                encoder=Encoder.simple_string_encoder())\n",
    "            .with_output_file_config(\n",
    "                OutputFileConfig.builder()\n",
    "                .with_part_prefix(\"prefix\")\n",
    "                .with_part_suffix(\".ext\")\n",
    "                .build())\n",
    "            .with_rolling_policy(RollingPolicy.default_rolling_policy())\n",
    "            .build()\n",
    "        )\n",
    "    else:\n",
    "        print(\"Printing result to stdout. Use --output to specify output path.\")\n",
    "        # ds.print()\n",
    "\n",
    "        # Step 1: Create a `StreamTableEnvironment` object.\n",
    "        t_env = StreamTableEnvironment.create(env)\n",
    "\n",
    "        # Step 2: Convert the `DataStream` object to a `Table` object.\n",
    "        table = t_env.from_data_stream(ds)\n",
    "\n",
    "        # Step 3: Convert the `Table` object to a `pandas` dataframe.\n",
    "        df = table.to_pandas()\n",
    "        \n",
    "        df.to_csv('./DataStream_API_word_count.csv', index=False)\n",
    "        print(df)\n",
    "\n",
    "    # submit for execution\n",
    "    env.execute()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=\"%(message)s\")\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--input',\n",
    "        dest='input',\n",
    "        required=False,\n",
    "        help='Input file to process.')\n",
    "    parser.add_argument(\n",
    "        '--output',\n",
    "        dest='output',\n",
    "        required=False,\n",
    "        help='Output file to write results to.')\n",
    "\n",
    "    argv = sys.argv[1:]\n",
    "    known_args, _ = parser.parse_known_args(argv)\n",
    "\n",
    "    word_count(known_args.input, known_args.output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing word_count example with default input data set.\n",
      "Use --input to specify file input.\n",
      "Printing result to stdout. Use --output to specify output path.\n"
     ]
    }
   ],
   "source": [
    "# Test with a Flink Python Table API Program\n",
    "# The following code comes from the official [documents version 1.18](https://nightlies.apache.org/flink/flink-docs-release-1.18/zh/docs/dev/python/table_api_tutorial/).\n",
    "# Save the code below as `Table_API_word_count.py`.\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from pyflink.common import Row\n",
    "from pyflink.table import (EnvironmentSettings, TableEnvironment, TableDescriptor, Schema,\n",
    "                           DataTypes, FormatDescriptor)\n",
    "from pyflink.table.expressions import lit, col\n",
    "from pyflink.table.udf import udtf\n",
    "\n",
    "word_count_data = [\"To be, or not to be,--that is the question:--\",\n",
    "                   \"Whether 'tis nobler in the mind to suffer\",\n",
    "                   \"The slings and arrows of outrageous fortune\",\n",
    "                   \"Or to take arms against a sea of troubles,\",\n",
    "                   \"And by opposing end them?--To die,--to sleep,--\",\n",
    "                   \"No more; and by a sleep to say we end\",\n",
    "                   \"The heartache, and the thousand natural shocks\",\n",
    "                   \"That flesh is heir to,--'tis a consummation\",\n",
    "                   \"Devoutly to be wish'd. To die,--to sleep;--\",\n",
    "                   \"To sleep! perchance to dream:--ay, there's the rub;\",\n",
    "                   \"For in that sleep of death what dreams may come,\",\n",
    "                   \"When we have shuffled off this mortal coil,\",\n",
    "                   \"Must give us pause: there's the respect\",\n",
    "                   \"That makes calamity of so long life;\",\n",
    "                   \"For who would bear the whips and scorns of time,\",\n",
    "                   \"The oppressor's wrong, the proud man's contumely,\",\n",
    "                   \"The pangs of despis'd love, the law's delay,\",\n",
    "                   \"The insolence of office, and the spurns\",\n",
    "                   \"That patient merit of the unworthy takes,\",\n",
    "                   \"When he himself might his quietus make\",\n",
    "                   \"With a bare bodkin? who would these fardels bear,\",\n",
    "                   \"To grunt and sweat under a weary life,\",\n",
    "                   \"But that the dread of something after death,--\",\n",
    "                   \"The undiscover'd country, from whose bourn\",\n",
    "                   \"No traveller returns,--puzzles the will,\",\n",
    "                   \"And makes us rather bear those ills we have\",\n",
    "                   \"Than fly to others that we know not of?\",\n",
    "                   \"Thus conscience does make cowards of us all;\",\n",
    "                   \"And thus the native hue of resolution\",\n",
    "                   \"Is sicklied o'er with the pale cast of thought;\",\n",
    "                   \"And enterprises of great pith and moment,\",\n",
    "                   \"With this regard, their currents turn awry,\",\n",
    "                   \"And lose the name of action.--Soft you now!\",\n",
    "                   \"The fair Ophelia!--Nymph, in thy orisons\",\n",
    "                   \"Be all my sins remember'd.\"]\n",
    "\n",
    "\n",
    "def word_count(input_path, output_path):\n",
    "    t_env = TableEnvironment.create(EnvironmentSettings.in_streaming_mode())\n",
    "    # write all the data to one file\n",
    "    t_env.get_config().set(\"parallelism.default\", \"1\")\n",
    "\n",
    "    # define the source\n",
    "    if input_path is not None:\n",
    "        t_env.create_temporary_table(\n",
    "            'source',\n",
    "            TableDescriptor.for_connector('filesystem')\n",
    "                .schema(Schema.new_builder()\n",
    "                        .column('word', DataTypes.STRING())\n",
    "                        .build())\n",
    "                .option('path', input_path)\n",
    "                .format('csv')\n",
    "                .build())\n",
    "        tab = t_env.from_path('source')\n",
    "    else:\n",
    "        print(\"Executing word_count example with default input data set.\")\n",
    "        print(\"Use --input to specify file input.\")\n",
    "        tab = t_env.from_elements(map(lambda i: (i,), word_count_data),\n",
    "                                  DataTypes.ROW([DataTypes.FIELD('line', DataTypes.STRING())]))\n",
    "\n",
    "    # define the sink\n",
    "    if output_path is not None:\n",
    "        t_env.create_temporary_table(\n",
    "            'sink',\n",
    "            TableDescriptor.for_connector('filesystem')\n",
    "                .schema(Schema.new_builder()\n",
    "                        .column('word', DataTypes.STRING())\n",
    "                        .column('count', DataTypes.BIGINT())\n",
    "                        .build())\n",
    "                .option('path', output_path)\n",
    "                .format(FormatDescriptor.for_format('canal-json')\n",
    "                        .build())\n",
    "                .build())\n",
    "    else:\n",
    "        print(\"Printing result to stdout. Use --output to specify output path.\")\n",
    "        t_env.create_temporary_table(\n",
    "            'sink',\n",
    "            TableDescriptor.for_connector('print')\n",
    "                .schema(Schema.new_builder()\n",
    "                        .column('word', DataTypes.STRING())\n",
    "                        .column('count', DataTypes.BIGINT())\n",
    "                        .build())\n",
    "                .build())\n",
    "\n",
    "    @udtf(result_types=[DataTypes.STRING()])\n",
    "    def split(line: Row):\n",
    "        for s in line[0].split():\n",
    "            yield Row(s)\n",
    "\n",
    "    # compute word count\n",
    "    tab.flat_map(split).alias('word') \\\n",
    "        .group_by(col('word')) \\\n",
    "        .select(col('word'), lit(1).count) \\\n",
    "        .execute_insert('sink') \\\n",
    "        .wait()\n",
    "        \n",
    "    # remove .wait if submitting to a remote cluster, refer to\n",
    "    # https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/python/faq/#wait-for-jobs-to-finish-when-executing-jobs-in-mini-cluster\n",
    "    # for more details\n",
    "    # Convert the `Table` object to a `pandas` dataframe.\n",
    "    df = tab.to_pandas()\n",
    "    df.to_csv('./Table_API_word_count.csv', index=False)\n",
    "    print(df)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=\"%(message)s\")\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--input',\n",
    "        dest='input',\n",
    "        required=False,\n",
    "        help='Input file to process.')\n",
    "    parser.add_argument(\n",
    "        '--output',\n",
    "        dest='output',\n",
    "        required=False,\n",
    "        help='Output file to write results to.')\n",
    "\n",
    "    argv = sys.argv[1:]\n",
    "    known_args, _ = parser.parse_known_args(argv)\n",
    "\n",
    "    word_count(known_args.input, known_args.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Docker to build a local Kafka cluster.\n",
    "\n",
    "1. Install Docker and Docker Compose:\n",
    "```Bash\n",
    "sudo apt install Docker Docker-compose\n",
    "```\n",
    "2. Create a local `docker-compose.yml` file with following contents:\n",
    "\n",
    "```yaml\n",
    "version: '3'\n",
    "services:\n",
    "  zookeeper:\n",
    "    image: 'bitnami/zookeeper:latest'\n",
    "    environment:\n",
    "      - ALLOW_ANONYMOUS_LOGIN=yes\n",
    "  kafka:\n",
    "    image: 'bitnami/kafka:latest'\n",
    "    ports:\n",
    "      - '9092:9092'\n",
    "    environment:\n",
    "      - KAFKA_ADVERTISED_HOST_NAME=localhost\n",
    "      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181\n",
    "      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092\n",
    "      - KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9092\n",
    "      - KAFKA_CREATE_TOPICS=test:1:1\n",
    "      - ALLOW_PLAINTEXT_LISTENER=yes\n",
    "    depends_on:\n",
    "      - zookeeper\n",
    "```\n",
    "\n",
    "3. Locate the `docker-compose.yml` and run the following command:\n",
    "\n",
    "```Bash\n",
    "docker-compose up -d\n",
    "```\n",
    "\n",
    "This will run a local Kafka cluster containing a Zookeeper Instance and a Kafka Instance, which will run on port 9092 of localhost.\n",
    "\n",
    "Next time after downloading those needed files, you can run the following command to start the Kafka cluster:\n",
    "```Bash\n",
    "docker run -d -p 9092:9092 \\\n",
    "-e ALLOW_ANONYMOUS_LOGIN=yes \\\n",
    "-e KAFKA_ADVERTISED_HOST_NAME=localhost \\\n",
    "-e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \\\n",
    "-e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092 \\\n",
    "-e KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9092 \\\n",
    "-e KAFKA_CREATE_TOPICS=test:1:1 \\\n",
    "-e ALLOW_PLAINTEXT_LISTENER=yes \\\n",
    "bitnami/kafka:latest\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Following code uses kafka-python module to send data to a local Kafka cluster. \n",
    "#This code opens a text file named `hamlet.txt` and sends its contents as a stream to a specified Kafka Topic `hamlet`:\n",
    "#Following code uses kafka-python module to send data to a local Kafka cluster. \n",
    "#This code opens a text file named `hamlet.txt` and sends its contents as a stream to a specified Kafka Topic `hamlet`:\n",
    "\n",
    "from kafka import KafkaProducer\n",
    "import time\n",
    "import os\n",
    "\n",
    "def send_file_to_kafka(file_path: str, topic: str, bootstrap_servers: str):\n",
    "    # Create a KafkaProducer object with the given bootstrap servers\n",
    "    producer = KafkaProducer(bootstrap_servers=bootstrap_servers)\n",
    "    # Get the size of the file in bytes\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    # Open the file in read binary mode\n",
    "    while True:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            # Read the file in chunks of 1024 bytes\n",
    "            while True:\n",
    "                data = f.read(1024)\n",
    "                # If no data is read, break out of the loop\n",
    "                if not data:\n",
    "                    break\n",
    "                # Send the data to the given topic\n",
    "                producer.send(topic, data)\n",
    "                # Print the number of bytes sent to the topic\n",
    "                bytes_sent = len(data)\n",
    "                print(f\"Sent {bytes_sent} bytes to Kafka topic {topic}\")\n",
    "                # Calculate the percentage of the file sent\n",
    "                percent_sent = (f.tell() / file_size) * 100\n",
    "                # Print the percentage of the file sent\n",
    "                print(f\"{percent_sent:.2f}% of the file sent\")\n",
    "                # Wait for 3 seconds\n",
    "                time.sleep(3)\n",
    "        # Wait for user input to continue or exit\n",
    "        user_input = input(\"Press 'c' to continue sending the file or 'q' to quit: \")\n",
    "        if user_input == \"q\":\n",
    "            break\n",
    "\n",
    "\n",
    "# Call the function with the file path, topic, and bootstrap servers\n",
    "send_file_to_kafka(\"./hamlet.txt\",  \"hamlet\", \"localhost:9092\")\n",
    "\n",
    "# In this code, the send_file_to_kafka function accepts three parameters: file_path, topic, and bootstrap_servers. \n",
    "# file_path is the path to the local file, topic is the Kafka topic to which the data should be sent, and bootstrap_servers is the address of the Kafka cluster. \n",
    "# The function uses a with statement to open the file, reads its contents, and sends them as streaming data to the specified Kafka topic. \n",
    "# During the sending process, it prints out the transmission progress and uses the time.sleep method to pause for 0.1 seconds to control the sending rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Kafka on Ubuntu 22.04:\n",
    "\n",
    "```Bash\n",
    "sudo apt update\n",
    "sudo apt install openjdk-11-jdk -y\n",
    "wget https://downloads.apache.org/kafka/3.5.1/kafka_2.13-3.5.1.tgz\n",
    "tar xvf kafka_2.13-3.5.1.tgz\n",
    "sudo nano /etc/systemd/system/zookeeper.service\n",
    "sudo nano /etc/systemd/system/kafka.service\n",
    "sudo systemctl start zookeeper\n",
    "sudo systemctl start kafka\n",
    "```\n",
    "\n",
    "First, download the Kafka connector jar package from the official Apache Flink website or from the Maven repository 1.\n",
    "Next, copy the downloaded jar file to the lib directory of PyFlink. The path of the lib directory is generally /usr/local/lib/python3.8.2/site-packages/pyflink/lib 2.\n",
    "Finally, you can use the Kafka connector in PyFlink by importing the required classes and creating an instance of the FlinkKafkaConsumer class 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A easy way to show the stream with kafka-python\n",
    "\n",
    "from kafka import KafkaConsumer\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    \"hamlet\",\n",
    "    bootstrap_servers=[\"localhost:9092\"],\n",
    "    auto_offset_reset=\"earliest\",\n",
    "    enable_auto_commit=True,\n",
    "    group_id=\"my-group\",\n",
    "    value_deserializer=lambda x: x.decode(\"utf-8\")\n",
    ")\n",
    "\n",
    "for message in consumer:\n",
    "    # Print the number of bytes received from the Kafka topic\n",
    "    print(f\"Received {len(message.value)} bytes from Kafka topic {message.topic}\")\n",
    "    # Print the contents of the message\n",
    "    print(f\"{message.value}\")\n",
    "\n",
    "\n",
    "# In the above code, we use the `KafkaConsumer` class to create a consumer object. \n",
    "# We pass `hamlet` as the topic name to the constructor. \n",
    "# We also pass `localhost:9092` as the address of the bootstrap server. \n",
    "# We use the `value_deserializer` parameter to decode the messages received from the Kafka topic. \n",
    "# We use a `for` loop to iterate over the consumer object and use the `print` function to print the contents of the message. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A easy way to show the stream with pyflink\n",
    "# The word counting part is not ready yet\n",
    "import os\n",
    "# Get current absolute path\n",
    "current_file_path = os.path.abspath(__file__)\n",
    "# Get current dir path\n",
    "current_dir_path = os.path.dirname(current_file_path)\n",
    "# Change into current dir path\n",
    "os.chdir(current_dir_path)\n",
    "output_path = current_dir_path\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from pyflink.table import StreamTableEnvironment\n",
    "from pyflink.common import WatermarkStrategy, Encoder, Types\n",
    "from pyflink.datastream import StreamExecutionEnvironment, RuntimeExecutionMode\n",
    "from pyflink.datastream.connectors.file_system import FileSource, StreamFormat, FileSink, OutputFileConfig, RollingPolicy\n",
    "from pyflink.common import Types, SimpleStringSchema\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.datastream.connectors.kafka import FlinkKafkaProducer, FlinkKafkaConsumer\n",
    "\n",
    "def split(line):\n",
    "    yield from line.split()\n",
    "\n",
    "def read_from_kafka():\n",
    "    # Create a Flink execution environment\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()    \n",
    "\n",
    "    # Add the Flink SQL Kafka connector jar file to the classpath\n",
    "    env.add_jars(\"file:///home/hadoop/Desktop/PyFlink-Tutorial/flink-sql-connector-kafka-3.1-SNAPSHOT.jar\")\n",
    "\n",
    "    # Print a message to indicate that data reading from Kafka has started\n",
    "    print(\"start reading data from kafka\")\n",
    "\n",
    "    # Create a Kafka consumer\n",
    "    kafka_consumer = FlinkKafkaConsumer(\n",
    "        topics='hamlet', # The topic to consume messages from\n",
    "        deserialization_schema= SimpleStringSchema('UTF-8'), # The schema to deserialize messages\n",
    "        properties={'bootstrap.servers': 'localhost:9092', 'group.id': 'my-group'} # The Kafka broker address and consumer group ID\n",
    "    )\n",
    "\n",
    "    # Start reading messages from the earliest offset\n",
    "    kafka_consumer.set_start_from_earliest()\n",
    "\n",
    "    # Add the Kafka consumer as a source to the Flink execution environment and print the messages to the console\n",
    "    env.add_source(kafka_consumer).print()\n",
    "\n",
    "    # # Start the Flink job\n",
    "    # env.execute()\n",
    "    \n",
    "    # ds = env.from_source(source=kafka_consumer, source_name= \"kafka_consumer\",watermark_strategy=WatermarkStrategy.for_monotonous_timestamps() )\n",
    "    # # write all the data to one file\n",
    "    \n",
    "    # # compute word count\n",
    "    # ds = ds.flat_map(split) \\\n",
    "    #     .map(lambda i: (i, 1), output_type=Types.TUPLE([Types.STRING(), Types.INT()])) \\\n",
    "    #     .key_by(lambda i: i[0]) \\\n",
    "    #     .reduce(lambda i, j: (i[0], i[1] + j[1]))\n",
    "\n",
    "    #     # define the sink\n",
    "    # if output_path is not None:\n",
    "    #     ds.sink_to(\n",
    "    #         sink=FileSink.for_row_format(\n",
    "    #             base_path=output_path,\n",
    "    #             encoder=Encoder.simple_string_encoder())\n",
    "    #         .with_output_file_config(\n",
    "    #             OutputFileConfig.builder()\n",
    "    #             .with_part_prefix(\"prefix\")\n",
    "    #             .with_part_suffix(\".ext\")\n",
    "    #             .build())\n",
    "    #         .with_rolling_policy(RollingPolicy.default_rolling_policy())\n",
    "    #         .build()\n",
    "    #     )\n",
    "    # else:\n",
    "    #     print(\"Printing result to stdout. Use --output to specify output path.\")\n",
    "    #     # ds.print()\n",
    "\n",
    "    #     # Step 1: Create a `StreamTableEnvironment` object.\n",
    "    #     t_env = StreamTableEnvironment.create(env)\n",
    "\n",
    "    #     # Step 2: Convert the `DataStream` object to a `Table` object.\n",
    "    #     table = t_env.from_data_stream(ds)\n",
    "\n",
    "    #     # Step 3: Convert the `Table` object to a `pandas` dataframe.\n",
    "    #     df = table.to_pandas()\n",
    "        \n",
    "    #     df.to_csv('./DataStream_API_word_count.csv', index=False)\n",
    "    #     print(df)\n",
    "\n",
    "    # submit for execution\n",
    "    env.execute()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Set up logging\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=\"%(message)s\")\n",
    "\n",
    "    # Call the read_from_kafka function\n",
    "    read_from_kafka()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
