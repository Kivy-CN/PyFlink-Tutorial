{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyFlink Installation\n",
    "\n",
    "### By Fatty\n",
    "\n",
    "### Version 2023.11.15\n",
    "\n",
    "## Anaconda3 Installation\n",
    "\n",
    "Download the Anaconda3 package from TUNA first.\n",
    "\n",
    "```Bash\n",
    "wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-2023.09-0-Linux-x86_64.sh\n",
    "sh Anaconda3-2023.09-0-Linux-x86_64.sh\n",
    "```\n",
    "During the installation, please use the default settings.\n",
    "It should be installed at `~/anaconda3`.\n",
    "\n",
    "## Python 3.9 Installation\n",
    "\n",
    "Python 3.9 installed by conda will be easy and reliable.\n",
    "\n",
    "```Bash\n",
    "conda create -n pyflink_39 python=3.9\n",
    "conda activate pyflink_39\n",
    "```\n",
    "\n",
    "## Apache-Flink Installation\n",
    "\n",
    "Then install the apache-flink package with pip.\n",
    "\n",
    "```Bash\n",
    "pip install apache-flink\n",
    "```\n",
    "\n",
    "## Some Tests\n",
    "\n",
    "The following code comes from the official [documents version 1.18](https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/dev/python/datastream_tutorial/). \n",
    "And there might be some tiny modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\HP\\Downloads\\备课\\于-流数据分析讲义PPT\\PyFlink_A_Brief_Tutorial.ipynb Cell 2\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HP/Downloads/%E5%A4%87%E8%AF%BE/%E4%BA%8E-%E6%B5%81%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E8%AE%B2%E4%B9%89PPT/PyFlink_A_Brief_Tutorial.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HP/Downloads/%E5%A4%87%E8%AF%BE/%E4%BA%8E-%E6%B5%81%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E8%AE%B2%E4%B9%89PPT/PyFlink_A_Brief_Tutorial.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Get current absolute path\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/HP/Downloads/%E5%A4%87%E8%AF%BE/%E4%BA%8E-%E6%B5%81%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E8%AE%B2%E4%B9%89PPT/PyFlink_A_Brief_Tutorial.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m current_file_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mabspath(\u001b[39m__file__\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HP/Downloads/%E5%A4%87%E8%AF%BE/%E4%BA%8E-%E6%B5%81%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E8%AE%B2%E4%B9%89PPT/PyFlink_A_Brief_Tutorial.ipynb#W1sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Get current dir path\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HP/Downloads/%E5%A4%87%E8%AF%BE/%E4%BA%8E-%E6%B5%81%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E8%AE%B2%E4%B9%89PPT/PyFlink_A_Brief_Tutorial.ipynb#W1sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m current_dir_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(current_file_path)\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "# Test with a Flink Python DataStream API Program\n",
    "# The following code comes from the official [documents version 1.18](https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/dev/python/datastream_tutorial/).\n",
    "# Save the code below as `DataStream_API_word_count.py`.\n",
    "# import os\n",
    "# # Get current absolute path\n",
    "# current_file_path = os.path.abspath(__file__)\n",
    "# # Get current dir path\n",
    "# current_dir_path = os.path.dirname(current_file_path)\n",
    "# # Change into current dir path\n",
    "# os.chdir(current_dir_path)\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from pyflink.table import StreamTableEnvironment\n",
    "from pyflink.common import WatermarkStrategy, Encoder, Types\n",
    "from pyflink.datastream import StreamExecutionEnvironment, RuntimeExecutionMode\n",
    "from pyflink.datastream.connectors.file_system import FileSource, StreamFormat, FileSink, OutputFileConfig, RollingPolicy\n",
    "\n",
    "\n",
    "word_count_data = [\"To be, or not to be,--that is the question:--\",\n",
    "                   \"Whether 'tis nobler in the mind to suffer\",\n",
    "                   \"The slings and arrows of outrageous fortune\",\n",
    "                   \"Or to take arms against a sea of troubles,\",\n",
    "                   \"And by opposing end them?--To die,--to sleep,--\",\n",
    "                   \"No more; and by a sleep to say we end\",\n",
    "                   \"The heartache, and the thousand natural shocks\",\n",
    "                   \"That flesh is heir to,--'tis a consummation\",\n",
    "                   \"Devoutly to be wish'd. To die,--to sleep;--\",\n",
    "                   \"To sleep! perchance to dream:--ay, there's the rub;\",\n",
    "                   \"For in that sleep of death what dreams may come,\",\n",
    "                   \"When we have shuffled off this mortal coil,\",\n",
    "                   \"Must give us pause: there's the respect\",\n",
    "                   \"That makes calamity of so long life;\",\n",
    "                   \"For who would bear the whips and scorns of time,\",\n",
    "                   \"The oppressor's wrong, the proud man's contumely,\",\n",
    "                   \"The pangs of despis'd love, the law's delay,\",\n",
    "                   \"The insolence of office, and the spurns\",\n",
    "                   \"That patient merit of the unworthy takes,\",\n",
    "                   \"When he himself might his quietus make\",\n",
    "                   \"With a bare bodkin? who would these fardels bear,\",\n",
    "                   \"To grunt and sweat under a weary life,\",\n",
    "                   \"But that the dread of something after death,--\",\n",
    "                   \"The undiscover'd country, from whose bourn\",\n",
    "                   \"No traveller returns,--puzzles the will,\",\n",
    "                   \"And makes us rather bear those ills we have\",\n",
    "                   \"Than fly to others that we know not of?\",\n",
    "                   \"Thus conscience does make cowards of us all;\",\n",
    "                   \"And thus the native hue of resolution\",\n",
    "                   \"Is sicklied o'er with the pale cast of thought;\",\n",
    "                   \"And enterprises of great pith and moment,\",\n",
    "                   \"With this regard, their currents turn awry,\",\n",
    "                   \"And lose the name of action.--Soft you now!\",\n",
    "                   \"The fair Ophelia!--Nymph, in thy orisons\",\n",
    "                   \"Be all my sins remember'd.\"]\n",
    "\n",
    "\n",
    "def word_count(input_path, output_path):\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()\n",
    "    env.set_runtime_mode(RuntimeExecutionMode.BATCH)\n",
    "    # write all the data to one file\n",
    "    env.set_parallelism(1)\n",
    "\n",
    "    # define the source\n",
    "    if input_path is not None:\n",
    "        ds = env.from_source(\n",
    "            source=FileSource.for_record_stream_format(StreamFormat.text_line_format(),\n",
    "                                                       input_path)\n",
    "                             .process_static_file_set().build(),\n",
    "            watermark_strategy=WatermarkStrategy.for_monotonous_timestamps(),\n",
    "            source_name=\"file_source\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"Executing word_count example with default input data set.\")\n",
    "        print(\"Use --input to specify file input.\")\n",
    "        ds = env.from_collection(word_count_data)\n",
    "\n",
    "    def split(line):\n",
    "        yield from line.split()\n",
    "\n",
    "    # compute word count\n",
    "    ds = ds.flat_map(split) \\\n",
    "        .map(lambda i: (i, 1), output_type=Types.TUPLE([Types.STRING(), Types.INT()])) \\\n",
    "        .key_by(lambda i: i[0]) \\\n",
    "        .reduce(lambda i, j: (i[0], i[1] + j[1]))\n",
    "\n",
    "    # define the sink\n",
    "    if output_path is not None:\n",
    "        ds.sink_to(\n",
    "            sink=FileSink.for_row_format(\n",
    "                base_path=output_path,\n",
    "                encoder=Encoder.simple_string_encoder())\n",
    "            .with_output_file_config(\n",
    "                OutputFileConfig.builder()\n",
    "                .with_part_prefix(\"prefix\")\n",
    "                .with_part_suffix(\".ext\")\n",
    "                .build())\n",
    "            .with_rolling_policy(RollingPolicy.default_rolling_policy())\n",
    "            .build()\n",
    "        )\n",
    "    else:\n",
    "        print(\"Printing result to stdout. Use --output to specify output path.\")\n",
    "        # ds.print()\n",
    "\n",
    "        # Step 1: Create a `StreamTableEnvironment` object.\n",
    "        t_env = StreamTableEnvironment.create(env)\n",
    "\n",
    "        # Step 2: Convert the `DataStream` object to a `Table` object.\n",
    "        table = t_env.from_data_stream(ds)\n",
    "\n",
    "        # Step 3: Convert the `Table` object to a `pandas` dataframe.\n",
    "        df = table.to_pandas()\n",
    "        \n",
    "        df.to_csv('./DataStream_API_word_count.csv', index=False)\n",
    "        print(df)\n",
    "\n",
    "    # submit for execution\n",
    "    env.execute()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Configure logging to write to the standard output stream\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=\"%(message)s\")\n",
    "\n",
    "    # Create an argument parser to parse command line arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Add an argument for the input file\n",
    "    parser.add_argument(\n",
    "        '--input',\n",
    "        dest='input',\n",
    "        required=False,\n",
    "        help='Input file to process.')\n",
    "    # Add an argument for the output file\n",
    "    parser.add_argument(\n",
    "        '--output',\n",
    "        dest='output',\n",
    "        required=False,\n",
    "        help='Output file to write results to.')\n",
    "\n",
    "    # Parse the command line arguments\n",
    "    argv = sys.argv[1:]\n",
    "    known_args, _ = parser.parse_known_args(argv)\n",
    "\n",
    "    # Call the word_count function with the input and output arguments\n",
    "    word_count(known_args.input, known_args.output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing word_count example with default input data set.\n",
      "Use --input to specify file input.\n",
      "Printing result to stdout. Use --output to specify output path.\n"
     ]
    }
   ],
   "source": [
    "# Test with a Flink Python Table API Program\n",
    "# The following code comes from the official [documents version 1.18](https://nightlies.apache.org/flink/flink-docs-release-1.18/zh/docs/dev/python/table_api_tutorial/).\n",
    "# Save the code below as `Table_API_word_count.py`.\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from pyflink.common import Row\n",
    "from pyflink.table import (EnvironmentSettings, TableEnvironment, TableDescriptor, Schema,\n",
    "                           DataTypes, FormatDescriptor)\n",
    "from pyflink.table.expressions import lit, col\n",
    "from pyflink.table.udf import udtf\n",
    "\n",
    "word_count_data = [\"To be, or not to be,--that is the question:--\",\n",
    "                   \"Whether 'tis nobler in the mind to suffer\",\n",
    "                   \"The slings and arrows of outrageous fortune\",\n",
    "                   \"Or to take arms against a sea of troubles,\",\n",
    "                   \"And by opposing end them?--To die,--to sleep,--\",\n",
    "                   \"No more; and by a sleep to say we end\",\n",
    "                   \"The heartache, and the thousand natural shocks\",\n",
    "                   \"That flesh is heir to,--'tis a consummation\",\n",
    "                   \"Devoutly to be wish'd. To die,--to sleep;--\",\n",
    "                   \"To sleep! perchance to dream:--ay, there's the rub;\",\n",
    "                   \"For in that sleep of death what dreams may come,\",\n",
    "                   \"When we have shuffled off this mortal coil,\",\n",
    "                   \"Must give us pause: there's the respect\",\n",
    "                   \"That makes calamity of so long life;\",\n",
    "                   \"For who would bear the whips and scorns of time,\",\n",
    "                   \"The oppressor's wrong, the proud man's contumely,\",\n",
    "                   \"The pangs of despis'd love, the law's delay,\",\n",
    "                   \"The insolence of office, and the spurns\",\n",
    "                   \"That patient merit of the unworthy takes,\",\n",
    "                   \"When he himself might his quietus make\",\n",
    "                   \"With a bare bodkin? who would these fardels bear,\",\n",
    "                   \"To grunt and sweat under a weary life,\",\n",
    "                   \"But that the dread of something after death,--\",\n",
    "                   \"The undiscover'd country, from whose bourn\",\n",
    "                   \"No traveller returns,--puzzles the will,\",\n",
    "                   \"And makes us rather bear those ills we have\",\n",
    "                   \"Than fly to others that we know not of?\",\n",
    "                   \"Thus conscience does make cowards of us all;\",\n",
    "                   \"And thus the native hue of resolution\",\n",
    "                   \"Is sicklied o'er with the pale cast of thought;\",\n",
    "                   \"And enterprises of great pith and moment,\",\n",
    "                   \"With this regard, their currents turn awry,\",\n",
    "                   \"And lose the name of action.--Soft you now!\",\n",
    "                   \"The fair Ophelia!--Nymph, in thy orisons\",\n",
    "                   \"Be all my sins remember'd.\"]\n",
    "\n",
    "\n",
    "def word_count(input_path, output_path):\n",
    "    # Create a TableEnvironment object\n",
    "    t_env = TableEnvironment.create(EnvironmentSettings.in_streaming_mode())\n",
    "    # write all the data to one file\n",
    "    # Set the default parallelism to 1\n",
    "    t_env.get_config().set(\"parallelism.default\", \"1\")\n",
    "\n",
    "    # define the source\n",
    "    # Define the source\n",
    "    if input_path is not None:\n",
    "        # Create a temporary table from the input file\n",
    "        t_env.create_temporary_table(\n",
    "            'source',\n",
    "            TableDescriptor.for_connector('filesystem')\n",
    "                .schema(Schema.new_builder()\n",
    "                        .column('word', DataTypes.STRING())\n",
    "                        .build())\n",
    "                .option('path', input_path)\n",
    "                .format('csv')\n",
    "                .build())\n",
    "        # Create a Table object from the temporary table\n",
    "        tab = t_env.from_path('source')\n",
    "    else:\n",
    "        print(\"Executing word_count example with default input data set.\")\n",
    "        print(\"Use --input to specify file input.\")\n",
    "        # Create a Table object from the word_count_data\n",
    "        tab = t_env.from_elements(map(lambda i: (i,), word_count_data),\n",
    "                                  DataTypes.ROW([DataTypes.FIELD('line', DataTypes.STRING())]))\n",
    "\n",
    "    # define the sink\n",
    "    # Define the sink\n",
    "    if output_path is not None:\n",
    "        # Create a temporary table to write the results to\n",
    "        t_env.create_temporary_table(\n",
    "            'sink',\n",
    "            TableDescriptor.for_connector('filesystem')\n",
    "                .schema(Schema.new_builder()\n",
    "                        .column('word', DataTypes.STRING())\n",
    "                        .column('count', DataTypes.BIGINT())\n",
    "                        .build())\n",
    "                .option('path', output_path)\n",
    "                .format(FormatDescriptor.for_format('canal-json')\n",
    "                        .build())\n",
    "                .build())\n",
    "    else:\n",
    "        print(\"Printing result to stdout. Use --output to specify output path.\")\n",
    "        # Create a temporary table to write the results to\n",
    "        t_env.create_temporary_table(\n",
    "            'sink',\n",
    "            TableDescriptor.for_connector('print')\n",
    "                .schema(Schema.new_builder()\n",
    "                        .column('word', DataTypes.STRING())\n",
    "                        .column('count', DataTypes.BIGINT())\n",
    "                        .build())\n",
    "                .build())\n",
    "\n",
    "    # Create a User Defined Function to split the input string\n",
    "    @udtf(result_types=[DataTypes.STRING()])\n",
    "    def split(line: Row):\n",
    "        for s in line[0].split():\n",
    "            yield Row(s)\n",
    "\n",
    "    # compute word count\n",
    "    # Compute word count\n",
    "    tab.flat_map(split).alias('word') \\\n",
    "        .group_by(col('word')) \\\n",
    "        .select(col('word'), lit(1).count) \\\n",
    "        .execute_insert('sink') \\\n",
    "        .wait()\n",
    "        \n",
    "    # remove .wait if submitting to a remote cluster, refer to\n",
    "    # Convert the Table object to a pandas dataframe\n",
    "    # Remove .wait if submitting to a remote cluster, refer to\n",
    "    # https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/python/faq/#wait-for-jobs-to-finish-when-executing-jobs-in-mini-cluster\n",
    "    # for more details\n",
    "    # Convert the `Table` object to a `pandas` dataframe.\n",
    "    df = tab.to_pandas()\n",
    "    df.to_csv('./Table_API_word_count.csv', index=False)\n",
    "    print(df)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Setup logging\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=\"%(message)s\")\n",
    "\n",
    "    # Create an argument parser\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Add an argument for the input file\n",
    "    parser.add_argument(\n",
    "        '--input',\n",
    "        dest='input',\n",
    "        required=False,\n",
    "        help='Input file to process.')\n",
    "    # Add an argument for the output file\n",
    "    parser.add_argument(\n",
    "        '--output',\n",
    "        dest='output',\n",
    "        required=False,\n",
    "        help='Output file to write results to.')\n",
    "\n",
    "    # Parse the arguments\n",
    "    argv = sys.argv[1:]\n",
    "    known_args, _ = parser.parse_known_args(argv)\n",
    "\n",
    "    # Call the word_count function with the arguments\n",
    "    word_count(known_args.input, known_args.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Docker to build a local Kafka cluster\n",
    "\n",
    "Operating System use Ubuntu 22.04.3.\n",
    "\n",
    "1. Install Docker and Docker Compose:\n",
    "```Bash\n",
    "sudo apt install Docker Docker-compose\n",
    "```\n",
    "2. Create a local `docker-compose.yml` file with following contents:\n",
    "\n",
    "```yaml\n",
    "version: '3'\n",
    "services:\n",
    "  zookeeper:\n",
    "    image: 'bitnami/zookeeper:latest'\n",
    "    environment:\n",
    "      - ALLOW_ANONYMOUS_LOGIN=yes\n",
    "  kafka:\n",
    "    image: 'bitnami/kafka:latest'\n",
    "    ports:\n",
    "      - '9092:9092'\n",
    "    environment:\n",
    "      - KAFKA_ADVERTISED_HOST_NAME=localhost\n",
    "      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181\n",
    "      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092\n",
    "      - KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9092\n",
    "      - KAFKA_CREATE_TOPICS=test:1:1\n",
    "      - ALLOW_PLAINTEXT_LISTENER=yes\n",
    "    depends_on:\n",
    "      - zookeeper\n",
    "```\n",
    "\n",
    "3. Locate the `docker-compose.yml` and run the following command:\n",
    "\n",
    "```Bash\n",
    "docker-compose up -d\n",
    "```\n",
    "\n",
    "This will run a local Kafka cluster containing a Zookeeper Instance and a Kafka Instance, which will run on port 9092 of localhost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A easy way to generate the stream with kafka-python\n",
    "\n",
    "# Following code uses kafka-python module to send data to a local Kafka cluster. \n",
    "# This code opens a text file named `hamlet.txt` and sends its contents as a stream to a specified Kafka Topic `hamlet`:\n",
    "\n",
    "from kafka import KafkaProducer\n",
    "import time\n",
    "import os\n",
    "\n",
    "def send_file_to_kafka(file_path: str, topic: str, bootstrap_servers: str):\n",
    "    # Create a KafkaProducer object with the given bootstrap servers\n",
    "    producer = KafkaProducer(bootstrap_servers=bootstrap_servers)\n",
    "    # Get the size of the file in bytes\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    # Open the file in read binary mode\n",
    "    while True:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            # Read the file in chunks of 1024 bytes\n",
    "            while True:\n",
    "                data = f.read(1024)\n",
    "                # If no data is read, break out of the loop\n",
    "                if not data:\n",
    "                    break\n",
    "                # Send the data to the given topic\n",
    "                producer.send(topic, data)\n",
    "                # Print the number of bytes sent to the topic\n",
    "                bytes_sent = len(data)\n",
    "                print(f\"Sent {bytes_sent} bytes to Kafka topic {topic}\")\n",
    "                # Calculate the percentage of the file sent\n",
    "                percent_sent = (f.tell() / file_size) * 100\n",
    "                # Print the percentage of the file sent\n",
    "                print(f\"{percent_sent:.2f}% of the file sent\")\n",
    "                # Wait for 3 seconds\n",
    "                time.sleep(3)\n",
    "        # Wait for user input to continue or exit\n",
    "        user_input = input(\"Press 'c' to continue sending the file or 'q' to quit: \")\n",
    "        if user_input == \"q\":\n",
    "            break\n",
    "\n",
    "\n",
    "# Call the function with the file path, topic, and bootstrap servers\n",
    "send_file_to_kafka(\"./hamlet.txt\",  \"hamlet\", \"localhost:9092\")\n",
    "\n",
    "# In this code, the send_file_to_kafka function accepts three parameters: file_path, topic, and bootstrap_servers. \n",
    "# file_path is the path to the local file, topic is the Kafka topic to which the data should be sent, and bootstrap_servers is the address of the Kafka cluster. \n",
    "# The function uses a with statement to open the file, reads its contents, and sends them as streaming data to the specified Kafka topic. \n",
    "# During the sending process, it prints out the transmission progress and uses the time.sleep method to pause for 3 seconds to control the sending rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A easy way to show the stream with kafka-python\n",
    "\n",
    "from kafka import KafkaConsumer\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    \"hamlet\",\n",
    "    bootstrap_servers=[\"localhost:9092\"],\n",
    "    auto_offset_reset=\"earliest\",\n",
    "    enable_auto_commit=True,\n",
    "    group_id=\"my-group\",\n",
    "    value_deserializer=lambda x: x.decode(\"utf-8\")\n",
    ")\n",
    "\n",
    "for message in consumer:\n",
    "    # Print the number of bytes received from the Kafka topic\n",
    "    print(f\"Received {len(message.value)} bytes from Kafka topic {message.topic}\")\n",
    "    # Print the contents of the message\n",
    "    print(f\"{message.value}\")\n",
    "\n",
    "\n",
    "# In the above code, we use the `KafkaConsumer` class to create a consumer object. \n",
    "# We pass `hamlet` as the topic name to the constructor. \n",
    "# We also pass `localhost:9092` as the address of the bootstrap server. \n",
    "# We use the `value_deserializer` parameter to decode the messages received from the Kafka topic. \n",
    "# We use a `for` loop to iterate over the consumer object and use the `print` function to print the contents of the message. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A easy way to show the stream with pyflink\n",
    "\n",
    "import os\n",
    "# Get current absolute path\n",
    "current_file_path = os.path.abspath(__file__)\n",
    "# Get current dir path\n",
    "current_dir_path = os.path.dirname(current_file_path)\n",
    "# Change into current dir path\n",
    "os.chdir(current_dir_path)\n",
    "output_path = current_dir_path\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from pyflink.table import StreamTableEnvironment\n",
    "from pyflink.common import WatermarkStrategy, Encoder, Types\n",
    "from pyflink.datastream import StreamExecutionEnvironment, RuntimeExecutionMode\n",
    "from pyflink.datastream.connectors.file_system import FileSource, StreamFormat, FileSink, OutputFileConfig, RollingPolicy\n",
    "from pyflink.common import Types, SimpleStringSchema\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.datastream.connectors.kafka import FlinkKafkaProducer, FlinkKafkaConsumer\n",
    "\n",
    "def split(line):\n",
    "    yield from line.split()\n",
    "\n",
    "def read_from_kafka():\n",
    "    # Create a Flink execution environment\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()    \n",
    "\n",
    "    # Add the Flink SQL Kafka connector jar file to the classpath\n",
    "    env.add_jars(\"file:///home/hadoop/Desktop/PyFlink-Tutorial/flink-sql-connector-kafka-3.1-SNAPSHOT.jar\")\n",
    "\n",
    "    # Print a message to indicate that data reading from Kafka has started\n",
    "    print(\"start reading data from kafka\")\n",
    "\n",
    "    # Create a Kafka consumer\n",
    "    kafka_consumer = FlinkKafkaConsumer(\n",
    "        topics='hamlet', # The topic to consume messages from\n",
    "        deserialization_schema= SimpleStringSchema('UTF-8'), # The schema to deserialize messages\n",
    "        properties={'bootstrap.servers': 'localhost:9092', 'group.id': 'my-group'} # The Kafka broker address and consumer group ID\n",
    "    )\n",
    "\n",
    "    # Start reading messages from the earliest offset\n",
    "    kafka_consumer.set_start_from_earliest()\n",
    "\n",
    "    # Add the Kafka consumer as a source to the Flink execution environment and print the messages to the console\n",
    "    env.add_source(kafka_consumer).print()\n",
    "    # submit for execution\n",
    "    env.execute()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Set up logging\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=\"%(message)s\")\n",
    "\n",
    "    # Call the read_from_kafka function\n",
    "    read_from_kafka()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple word count code\n",
    "\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "from pyflink.table import StreamTableEnvironment\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.datastream.connectors.kafka import FlinkKafkaConsumer\n",
    "from pyflink.common import SimpleStringSchema\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]','',text)\n",
    "\n",
    "def count_words(text):\n",
    "    words = text.split()\n",
    "    return Counter(words)\n",
    "\n",
    "def read_from_kafka():\n",
    "    # Create a Flink execution environment\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()    \n",
    "\n",
    "    # Add the Flink SQL Kafka connector jar file to the classpath\n",
    "    env.add_jars(\"file:///home/hadoop/Desktop/PyFlink-Tutorial/flink-sql-connector-kafka-3.1-SNAPSHOT.jar\")\n",
    "\n",
    "    # Print a message to indicate that data reading from Kafka has started\n",
    "    print(\"start reading data from kafka\")\n",
    "\n",
    "    # Create a Kafka consumer\n",
    "    kafka_consumer = FlinkKafkaConsumer(\n",
    "        topics='hamlet', # The topic to consume messages from\n",
    "        deserialization_schema= SimpleStringSchema('UTF-8'), # The schema to deserialize messages\n",
    "        properties={'bootstrap.servers': 'localhost:9092', 'group.id': 'my-group'} # The Kafka broker address and consumer group ID\n",
    "    )\n",
    "\n",
    "    # Start reading messages from the earliest offset\n",
    "    kafka_consumer.set_start_from_earliest()\n",
    "\n",
    "    # Add the Kafka consumer as a source to the Flink execution environment\n",
    "    stream = env.add_source(kafka_consumer)\n",
    "\n",
    "    # Remove punctuation from the text\n",
    "    stream_remove_punctuation = stream.map(lambda x: remove_punctuation(x))\n",
    "\n",
    "    # Count the words in the text\n",
    "    stream_count_words = stream_remove_punctuation.map(lambda x: count_words(x))\n",
    "\n",
    "    # Print the word counts to the console\n",
    "    stream_count_words.print()\n",
    "\n",
    "    # Start the Flink job\n",
    "    env.execute()\n",
    "\n",
    "read_from_kafka()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A more detailed version of word count\n",
    "\n",
    "import argparse\n",
    "\n",
    "#import necessary modules\n",
    "import io\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from io import StringIO\n",
    "from pyflink.common import SimpleStringSchema, Time\n",
    "from pyflink.datastream.connectors.kafka import FlinkKafkaConsumer\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.table import (DataTypes, EnvironmentSettings, FormatDescriptor,\n",
    "                           Schema, StreamTableEnvironment, TableDescriptor,\n",
    "                           TableEnvironment, udf)\n",
    "from pyflink.table.expressions import col, lit\n",
    "\n",
    "#define functions to remove punctuation and count bytes and words\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]','',text)\n",
    "\n",
    "def count_bytes(text):\n",
    "    return len(text.encode('utf-8'))\n",
    "\n",
    "def count_words(text):\n",
    "    words = text.split()\n",
    "    result = dict(Counter(words))\n",
    "    max_word = max(result, key=result.get)\n",
    "    return {'total_bytes': count_bytes(text), 'total_words': len(words), 'most_frequent_word': max_word, 'most_frequent_word_count': result[max_word]}\n",
    "\n",
    "#read data from kafka\n",
    "def read_from_kafka():\n",
    "    #create a StreamExecutionEnvironment\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()  \n",
    "    #add the kafka connector to the environment\n",
    "    env.add_jars(\"file:///home/hadoop/Desktop/PyFlink-Tutorial/flink-sql-connector-kafka-3.1-SNAPSHOT.jar\")\n",
    "    print(\"start reading data from kafka\")\n",
    "    #create a FlinkKafkaConsumer to read data from kafka\n",
    "    kafka_consumer = FlinkKafkaConsumer(\n",
    "        topics='hamlet', \n",
    "        deserialization_schema= SimpleStringSchema('UTF-8'), \n",
    "        properties={'bootstrap.servers': 'localhost:9092', 'group.id': 'my-group'} \n",
    "    )\n",
    "    \n",
    "    #set the start time to earliest\n",
    "    kafka_consumer.set_start_from_earliest()\n",
    "    #add the kafka consumer to the environment\n",
    "    stream_original_text = env.add_source(kafka_consumer)\n",
    "    #remove punctuation from the data\n",
    "    stream_remove_punctuation = stream_original_text.map(lambda x: remove_punctuation(x))\n",
    "    #count the bytes and words in the data\n",
    "    stream_count_words = stream_remove_punctuation.map(lambda x: count_words(x))\n",
    "    #print the results\n",
    "    stream_count_words.print()\n",
    "    #execute the environment\n",
    "    env.execute()\n",
    "\n",
    "#call the read_from_kafka function\n",
    "read_from_kafka()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play wit CSV\n",
    "\n",
    "Suppose we get a `data.csv` file with whatever inside and only the year data in this file is what we need.\n",
    "We firstly use the following code to generate a `StreamGeneratorCSV` to trans the `CSV` file into `Kafka Stream`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple stream generator from CSV\n",
    "\n",
    "#Following code uses kafka-python module to send data to a local Kafka cluster. \n",
    "#This code opens a text file named `hamlet.txt` and sends its contents as a stream to a specified Kafka Topic `hamlet`:\n",
    "\n",
    "from kafka import KafkaProducer\n",
    "import time\n",
    "import os\n",
    "import chardet\n",
    "\n",
    "def send_file_to_kafka(file_path: str, topic: str, bootstrap_servers: str):\n",
    "    # Create a KafkaProducer object with the given bootstrap servers\n",
    "    producer = KafkaProducer(bootstrap_servers=bootstrap_servers)\n",
    "    # Get the size of the file in bytes\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    # Get the total number of lines in the file\n",
    "\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        result = chardet.detect(f.read())\n",
    "        encoding = result[\"encoding\"]\n",
    "\n",
    "    with open(file_path, \"r\", encoding=encoding) as f:\n",
    "        lines_total = len(f.readlines())\n",
    "\n",
    "    # Initialize the number of lines sent to 0\n",
    "    lines_send = 0\n",
    "    # Open the file in read binary mode\n",
    "    # Open the file in read binary mode\n",
    "    while True:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            # Read the file in chunks of 1024 bytes\n",
    "            while True:\n",
    "                data = f.readlines(10)\n",
    "                # If no data is read, break out of the loop\n",
    "                if not data:\n",
    "                    break\n",
    "                # Convert the data to a string\n",
    "                data_str = str(data)\n",
    "                # Convert the string back to bytes\n",
    "                data_bytes = data_str.encode()\n",
    "                # Send the data to the given topic\n",
    "                producer.send(topic, data_bytes)\n",
    "                # Increment the number of lines sent\n",
    "                lines_send += 10\n",
    "                # Calculate the percentage of the file sent\n",
    "                percent_sent = (lines_send / lines_total) * 100                \n",
    "                # Print the number of bytes sent to the topic and thepercentage of the file sent\n",
    "                bytes_sent = len(data_bytes)\n",
    "                print(f\"Sent {bytes_sent} bytes {topic} {percent_sent:.2f}% sent\")\n",
    "                # Wait for 3 seconds\n",
    "                time.sleep(3)\n",
    "        # Wait for user input to continue or exit\n",
    "        user_input = input(\"Press 'c' to continue sending the file or 'q' to quit: \")\n",
    "        if user_input == \"q\":\n",
    "            break\n",
    "\n",
    "# Call the function with the file path, topic, and bootstrap servers\n",
    "send_file_to_kafka(\"./data.csv\",  \"data\", \"localhost:9092\")\n",
    "\n",
    "\n",
    "# In this code, the send_file_to_kafka function accepts three parameters: file_path, topic, and bootstrap_servers. \n",
    "# file_path is the path to the local file, topic is the Kafka topic to which the data should be sent, and bootstrap_servers is the address of the Kafka cluster. \n",
    "# The function uses a with statement to open the file, reads its contents, and sends them as streaming data to the specified Kafka topic. \n",
    "# During the sending process, it prints out the transmission progress and uses the time.sleep method to pause for 3 seconds to control the sending rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select only the years\n",
    "\n",
    "Then we use a `StreamShowerWithFlinkCSV.py` to do that.\n",
    "The following code in fact uses `re` to function.\n",
    "But that is not important, just take a journey on messing up with the `DataStream` generated from `CSV` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StreamShowerWithFlinkCSV.py\n",
    "\n",
    "import os\n",
    "# Get current absolute path\n",
    "current_file_path = os.path.abspath(__file__)\n",
    "# Get current dir path\n",
    "current_dir_path = os.path.dirname(current_file_path)\n",
    "# Change into current dir path\n",
    "os.chdir(current_dir_path)\n",
    "output_path = current_dir_path\n",
    "\n",
    "\n",
    "import re\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from pyflink.table import StreamTableEnvironment\n",
    "from pyflink.common import WatermarkStrategy, Encoder, Types\n",
    "from pyflink.datastream import StreamExecutionEnvironment, RuntimeExecutionMode\n",
    "from pyflink.datastream.connectors.file_system import FileSource, StreamFormat, FileSink, OutputFileConfig, RollingPolicy\n",
    "from pyflink.common import Types, SimpleStringSchema\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.datastream.connectors.kafka import FlinkKafkaProducer, FlinkKafkaConsumer\n",
    "\n",
    "def split(line):\n",
    "    yield from line.split()\n",
    "\n",
    "def read_from_kafka():\n",
    "    Year_Begin =1999\n",
    "    Year_End = 2023\n",
    "    # Create a Flink execution environment\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()    \n",
    "\n",
    "    # Add the Flink SQL Kafka connector jar file to the classpath\n",
    "    env.add_jars(\"file:///home/hadoop/Desktop/PyFlink-Tutorial/flink-sql-connector-kafka-3.1-SNAPSHOT.jar\")\n",
    "\n",
    "    # Print a message to indicate that data reading from Kafka has started\n",
    "    print(\"start reading data from kafka\")\n",
    "\n",
    "    # Create a Kafka consumer\n",
    "    kafka_consumer = FlinkKafkaConsumer(\n",
    "        topics='data', # The topic to consume messages from\n",
    "        deserialization_schema= SimpleStringSchema('UTF-8'), # The schema to deserialize messages\n",
    "        properties={'bootstrap.servers': 'localhost:9092', 'group.id': 'my-group'} # The Kafka broker address and consumer group ID\n",
    "    )\n",
    "\n",
    "    # Start reading messages from the earliest offset\n",
    "    kafka_consumer.set_start_from_earliest()\n",
    "\n",
    "    # Add the Kafka consumer as a source to the Flink execution environment and print the messages to the console\n",
    "    env.add_source(kafka_consumer).map(lambda x: ' '.join(re.findall(r'\\d+', x))).filter(lambda x: any([Year_Begin <= int(i) <= Year_End for i in x.split()])).map(lambda x:  [i for i in x.split() if Year_Begin <= int(i) <= Year_End][0]).print()\n",
    "    # submit for execution\n",
    "    env.execute()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=\"%(message)s\")\n",
    "    read_from_kafka()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MapFunction`: A function that takes one element as input and produces one element as output. It can be used to transform data streams by applying a transformation to each element.\n",
    "`FlatMapFunction`: A function that takes one element as input and produces zero, one, or more elements as output. It can be used to transform data streams by applying a transformation to each element.\n",
    "`FilterFunction`: A function that takes one element as input and returns a boolean value. It can be used to filter data streams by removing elements that do not meet a certain condition.\n",
    "`KeySelector`: A function that extracts a key from an element. It can be used to group elements in a data stream by key.\n",
    "`ReduceFunction`: A function that takes two elements as input and produces one element as output. It can be used to aggregate data streams by combining elements that share a common key.\n",
    "`WindowFunction`: A function that takes a window of elements as input and produces one or more elements as output. It can be used to define windows on data streams and apply transformations to the elements within each window."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
