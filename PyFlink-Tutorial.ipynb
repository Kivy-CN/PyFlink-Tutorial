{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 基于Python的流数据处理技术简介\n",
    "\n",
    "## 1.0 版本说明\n",
    "\n",
    "本文项目示例代码修改自官方[文档版本1.18](https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/dev/python/datastream_tutorial/)。\n",
    "\n",
    "## 1.1 课程定位和设计思路\n",
    "\n",
    "这学期是我们这门课的第一次开课。\n",
    "最初，我们想通过一个传统的编程语言Scala来实现流数据处理框架，并带领大家完成几个样例。\n",
    "但后来我们发现这不太妥当。古人说得好，授人以鱼不如授人以渔。\n",
    "如果我们只是把这个框架的使用方法告诉你，那么这样的教学意义不大。\n",
    "因此，我们决定向大家展示这种框架的来龙去脉，包括流数据处理的概念、目的等等。\n",
    "\n",
    "此外，我们不再引入其他编程语言。一方面，这会干扰基础思想的理解，而我们注重的是思想而非技术。\n",
    "另一方面，这些框架的发展速度很快。前几个版本可能支持Scala，但后几个版本可能就不支持了。\n",
    "有些函数和方法在某些版本上是实现的，但在后续版本中可能会被修改。\n",
    "在这种情况下，教大家记忆某个编程语言中的某个东西是否可修改，另一个是否不可修改，是没有意义的。\n",
    "\n",
    "例如，有一段代码最初使用for循环来实现，但速度很慢。后来改成列表推导式，速度提升了。\n",
    "但是，随着编程语言的发展和编译环境的改进，for循环有时可能比列表推导式更快。\n",
    "因此，如果考试要求判断哪个更快，就像背古诗词一样，这样的考试就没有意义，也不好玩。\n",
    "\n",
    "在我们这门课中，我们强调的是一个框架性的认知。\n",
    "就是关于这个流数据的整个过程，从他的基本概念思路，再到产生发送接收和处理。\n",
    "这个过程会有很多环节，需要大家动手去探索。有可能你们之前学过Linux操作系统，也学过虚拟机，也学过Python等等。\n",
    "之前学的那些基础知识，在我们这一面这种层次的课程就要开始应用了。\n",
    "\n",
    "本来，我已经为大家准备好了一个几乎完整的开发环境。虚拟机里面装好了所有的运行必备的组件以及编译器等等。\n",
    "当然，我会提供这个虚拟机给大家。但还是希望能够带着大家从头开始独立构建。\n",
    "这样能有机会了解每一步的操作，以便在以后的工作中能够独立完成。\n",
    "而不只是通过考试，却没有真正掌握这些知识。\n",
    "\n",
    "如果你以后要从事这方面的相关工作，听了这一门课就够了吗？\n",
    "那绝对不可能。因为任何一门大学的课程，一般都是一个概述。\n",
    "通过这种课程，我们的目的是让你了解这个领域的基础概念、基础思维和基础思路。\n",
    "然后，你需要进一步深入学习，可能去读研究生，专门从事学术领域的相关研究。\n",
    "或者在具体的生产环境中去探索这方面的更深入的开发工作。\n",
    "\n",
    "## 1.2 编程语言选择与代码难度\n",
    "\n",
    "\n",
    "我们选择了Python编程语言，而没有选择Java和Scala。\n",
    "首先是考虑到大家也都学过Python，上手的难度会比较低，对大家来说可能会比较友好。\n",
    "另外是考虑依赖包的安装问题，如果选择Java，有的时候需要用maven去下载一些包，有时候会有网络问题；\n",
    "然后还考虑到开发环境的搭建问题，Java的开发环境的配置相比Python也要复杂一点，工程的内容含量更高一些。\n",
    "本来我们也考虑过引入Scala，后来教研室几位老师反复商量，没有这么做。\n",
    "一方面是觉得引入一门全新的编程语言，对于大家来说还是有挑战。\n",
    "另外一方面是，Scala本身虽然性能更好，但有很大可能要被一些开源项目所放弃。\n",
    "因为针对它的专门维护可能需要一些额外的工作经历，但现在很多开源团队未必能有这么多的额外精力去继续维护它。\n",
    "综上所述，我们选择了对大家都友好，又好上手好配置，不用太担心依赖包安装问题的Python。\n",
    "\n",
    "\n",
    "| **编程语言** | **优势** | **劣势** |\n",
    "|------------|------------|------------|\n",
    "| Java       | - 跨平台- 可扩展、向后兼容、稳定、生产就绪- 支持各种经过试验和测试的库 | - 冗长- 不支持\"Read-Evaluate-Print-Loop（REPL）\" 中文翻译是 **读取-求值-输出-循环** |\n",
    "| Scala      | - 面向对象和函数式编程特性- 高度可扩展 | - 学习曲线较陡峭 |\n",
    "| Python     | - 易于学习、易于使用、易于阅读的语法- 有许多强大的库和框架 | - 解释性语言- 动态类型的语言 |\n",
    "\n",
    "以上是编程语言的选择。\n",
    "对于代码这部分，大家不要担心。\n",
    "\n",
    "首先在代码难度上，这堂课上涉及到的代码还是很友好的。\n",
    "一方面，基本上所有的代码都有做注释，大家自己阅读的时候也能比较容易地理解。\n",
    "另外一方面，授课的过程中，我们会拆解开来，一边演示一边讲解。\n",
    "代码经验不是很丰富的同学，相信是能够跟得上这种详细的讲解的。\n",
    "如果有学有余力的同学，也完全可以自己在深度上更进一步探索。\n",
    "\n",
    "另外在代码规模上，本课程的代码一定不会很长很累人。\n",
    "很多内容的复杂也就是思路和架构上的复杂性看上去吓人。\n",
    "实际上基本概念落实到代码中可能就有几百行代码顶多了。\n",
    "有的甚至就几十行代码就能完成。\n",
    "\n",
    "\n",
    "\n",
    "## 1.3 数据的基础认知\n",
    "\n",
    "### 1.3.1 描述而非定义\n",
    "\n",
    "首先，我们需要明确数据的定义。\n",
    "在这里，我们所说的数据并不是指计算机处理器里面的指令和数据那种狭义场景下的数据。\n",
    "而是一个更广泛意义上的数据。数据是可以被观测的，并且能够用数值记录下来的。\n",
    "这是一个描述，而不是一个严格的定义。\n",
    "虽然对数据的定义可能会有很多的争论，但我们做出的描述总是符合一定的经验的。\n",
    "\n",
    "### 1.3.2 数据描述了人类个体、人类社会、外在世界\n",
    "\n",
    "人的个体成长，作为一个自然过程，就伴随着各项数据的不断变化。身高、体重、体温等等。\n",
    "小朋友，刚出生的时候可能只有几公斤，等长到了几岁了，可能就有十几公斤甚至几十公斤。\n",
    "再长一些之后就有可能有上百公斤，比如我曾经就有100多公斤啊。\n",
    "随着时间发展，人的宏观上身高、体重和各种生理指标可能都会发生变化。\n",
    "而更细节上的体温、血相等等数据的异常波动可能预示了身体健康状况的问题。\n",
    "有的人可能要由于身体健康的原因做长期的数据观测，检测血压、血糖、心率等等。\n",
    "\n",
    "![](./images/head.gif)\n",
    "\n",
    "人的社会群体关系，也是一个充满数据的过程，可以用数据去描述。\n",
    "亲属关系、身份编号、手机号码、就读学校、所学专业等等，这些信息也都在有意或者无意中被数据记录下来。\n",
    "人类的复杂社会行为，比如商品供需关系、股票交易价格、货币汇率等等，也都得量化成数据。\n",
    "\n",
    "![](./images/stocks.gif)\n",
    "\n",
    "人去看待外在的自然世界，也要借助数据。\n",
    "外面天气的冷暖、风速的变化、降雨量的多少、地震的强度、建筑沉降的幅度、湖泊的面积、河水的流量、海水的含盐度等等。\n",
    "人最早对外界事物的系统的描述，大概就是度量衡，在日常生活中用于计量物体长短、容积、轻重。\n",
    "现在咱们也说衡量、度量、平衡等等，就这么来的。\n",
    "甚至在微观领域上，对一个分子或者原子的描述，也要用到数据。\n",
    "\n",
    "![](./images/hydrogen.gif)\n",
    "\n",
    "\n",
    "\n",
    "## 1.4 数据的形态划分这段话可以这样重构和润色：\n",
    "\n",
    "所有的数据都在不断变化，而传统的观察方式只能截取到一个截面或者累积若干个截面，难以应对不断变化的现实世界。\n",
    "\n",
    "成规模的数据有两种形态。一种是静态的数据，观测了一堆，不管这堆有多大，它是有固定的规模和有限的个数的，不会再进行改变。\n",
    "另一种情况比较复杂，需要一直关注和更新。\n",
    "这种情况有两种可能：\n",
    "一种是这东西是无限的，一直在被生成出来，随时在变化，需要一直观测；\n",
    "另一种可能是这东西规模太大，人力无法短时间内穷尽，即使是有限的，也需要持续观测和处理。这就是原生的流数据。\n",
    "\n",
    "人的观测能力是有限的，因为人作为动物，我们的感官本身是受限的。\n",
    "而且人记录的载体本身也存在物理上的上限。\n",
    "因此，面对一些场景，人力无法全部认识整体，只能一点一点来，这个过程中也会导致数据持续产生。\n",
    "\n",
    "静态的数据和流动的数据未必总是那么好区分。比如窗台边这有的是花岗岩，这些岩石形成的过程涉及到温度、压力和粘度等方面的变化。\n",
    "这些变化有的是物理变化，有的是化学变化，这个过程有的可能要花费大概一百万年。\n",
    "人现在认识这东西，都是测试平均的这么个成分上的，变成了一个静态的数据，但其实它的形成过程是一个动态的过程。\n",
    "\n",
    "具体的个体，因为它自身状态可能会随着时间发生不断的变化，可能会产生流动的数据。\n",
    "有限的群体，因为他们彼此之间的关系可能会发生不断的变化，也可能会产生流动的数据。\n",
    "如果是一段有限的数据集合，但它在时间或者空间上可以有不同的排布和次序，也能变成流动的数据。\n",
    "\n",
    "\n",
    "\n",
    "## 1.5 流数据的具体特征\n",
    "\n",
    "专门提出来这个流数据的分析和处理，就是因为有一些场景下需要这类思路解决对应的问题。\n",
    "而用传统的那种静态数据的处理思路，可能有一些解决不了的问题。\n",
    "流数据处理技术是基于现实需求而产生。\n",
    "\n",
    "事物是时刻不停的发展变化的，万事万物都非静态，运动是无处不在的。\n",
    "自然界中咱们所观测到的一切数据实际上都是原生的流数据，因为这些东西往往存在着各种的变化。\n",
    "只是变化的幅度可能会有差异，但这种变化是不停的。\n",
    "\n",
    "流数据具有如下特征：\n",
    "* 数据快速持续到达，潜在大小未知\n",
    "* 数据来源众多，格式复杂，类型多样\n",
    "* 数据量大，但不关注存储，经过处理后丢弃或归档存储\n",
    "* 注重数据的整体价值，不过分关注个别数据\n",
    "* 数据次序错乱，可能顺序颠倒或不完整，系统无法预测将处理的新到达数据元素的顺序\n",
    "\n",
    "#### 思考题 1 什么样的数据是静态的数据，什么样的数据是流数据？\n",
    "用自己的话通顺的表达出来，并且举一些例子。\n",
    "\n",
    "\n",
    "## 1.6 批计算与流计算\n",
    "\n",
    "简单来说，对静态数据就凑够一批来进行批计算，对于流数据就随着数据流进行流计算。\n",
    "这说的计算可以简单理解成就是加减乘除，甚至就是加法，当然还有逻辑运算。\n",
    "再说复杂点呢，就有映射、筛选、键值选择等等，这些后面再细说。\n",
    "\n",
    "批计算就是凑够一批，然后再算。\n",
    "流计算就是一边流着，一边计算。\n",
    "很直观的命名对不对？\n",
    "\n",
    "传统的批计算能不能解决上面的一些场景呢？其实也能。\n",
    "比如spark实际上就是用批计算，把流数据切成一小段一小段，然后每一段来进行处理。\n",
    "反过来流计算能去应对传统的批计算场景吗？其实也能。\n",
    "比如Flink也能做批计算，就是把批数据一条一条的发送过来，然后当成流数据来处理。\n",
    "但这里边有一个关键的问题，就是用批计算去模拟云计算，一定要涉及到最小的批规模的问题。\n",
    "要凑够一个最小的规模形成微批，这个最小可能也要几百毫秒。\n",
    "而如果直接用流计算，可能响应时间就能缩到几毫秒。\n",
    "在一些特定的商业场景，响应时间可能就很重要。\n",
    "比如电子商务、电子交易，实时外卖派单等等。\n",
    "当然也有的场景对于响应时间没有那么强的要求，反倒是对数据规模有要求。\n",
    "比如机器学习，大语言模型的训练等等场景。\n",
    "但综合来看，用流计算去模拟批计算是相对简单的。\n",
    "而反过来用批计算去模拟流计算总是要面对着响应时间的障碍。\n",
    "\n",
    "# 1.7 流计算的需求场景\n",
    "\n",
    "流计算秉承一个基本理念，即数据的价值随着时间的流逝而降低，如用户点击、商品浏览、天气数据等等。\n",
    "相比昨天的天气，更注重今天的天气。\n",
    "当事件出现时就应该立即进行处理，而不是缓存起来进行批量处理。\n",
    "\n",
    "自然环境中，数据的产生原本就是流式的。\n",
    "无论是来自 Web 服务器的事件数据，证券交易所的交易数据，还是来自工厂车间机器上的传感器数据，其数据都是流式的。\n",
    "当分析数据时，可以围绕 有界流（bounded）或 无界流（unbounded）两种模型来组织处理数据。\n",
    "选择不同的模型，程序的执行和处理方式也都会不同。\n",
    "\n",
    "![](./images/bounded-unbounded.png)\n",
    "\n",
    "批处理是有界数据流处理的范例。在这种模式下，你可以选择在计算结果输出之前输入整个数据集，这也就意味着你可以对整个数据集的数据进行排序、统计或汇总计算后再输出结果。\n",
    "\n",
    "流处理正相反，其涉及无界数据流。至少理论上来说，它的数据输入永远不会结束，因此程序必须持续不断地对到达的数据进行处理。\n",
    "\n",
    "为了及时处理流数据，就需要一个低延迟、可扩展、高可靠的处理引擎\n",
    "对于一个流计算系统来说，它应达到如下需求：\n",
    "* 高性能：处理大数据的基本要求，如每秒处理几十万条数据\n",
    "* 海量式：支持TB级甚至是PB级的数据规模\n",
    "* 实时性：保证较低的延迟时间，达到秒级别，甚至是毫秒级别\n",
    "* 分布式：支持大数据的基本架构，必须能够平滑扩展\n",
    "* 易用性：能够快速进行开发和部署\n",
    "* 可靠性：能可靠地处理流数据\n",
    "\n",
    "\n",
    "#### 思考题 2 对静态数据和流数据的处理，在计算模式上有什么不同？\t\n",
    "\n",
    "#### 思考题 3 使用批计算框架来处理流数据的思路是什么？\n",
    "\n",
    "#### 思考题 4 使用流计算框架来处理批数据的思路是什么？\n",
    "\n",
    "#### 思考题 5 适合流计算的场景可能是什么样的？请举例说明。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 流计算的框架选择\n",
    "\n",
    "\n",
    "## 2.1 传统架构的困境\n",
    "\n",
    "传统的大数据系统一般使用所谓的Lambda架构，如下图所示。\n",
    "其基本思路就是将数据进行分流：\n",
    "* 一部分作为实时数据，使用流处理框架，比如Storm或者Spark Streaming来处理；\n",
    "* 另一部分存入全量数据，使用MapReduce或者Spark来处理，当然现在主要是Spark。\n",
    "\n",
    "![](./images/lambda.svg)\n",
    "\n",
    "上面这种传统的Lambda架构有啥问题呢？\n",
    "首先就是架构复杂，技术栈多样，风险比较高。\n",
    "一旦某一个环节出问题，就可能影响到很多方面。\n",
    "由于整体架构复杂，又难以快速排查。\n",
    "另外是运营和维护成本高。\n",
    "每一条数据流程都需要有对应的单独的技术栈，可能就都需要对应的计算环境，就有可能带来资源浪费。\n",
    "另外每一个单独的技术栈也需要单独的技术团队来维护，人力成本和管理成本都上升了。\n",
    "\n",
    "\n",
    "## 2.2 流计算框架对比\n",
    "\n",
    "针对上面的Lambda架构的复杂度问题，解决思路自然是统一用一个技术栈。\n",
    "\n",
    "Spark的解决方案是使用Spark本体进行批计算，使用Spark Streaming进行流计算。\n",
    "但实际上Spark Streaming是将流数据分成非常小的微批来处理，因此难以达到毫秒级的响应。\n",
    "对Spark而言，它会使用一系列连续的微小批处理来模拟流处理，也就是说，它会在特定的时间间隔内发起一次计算，而不是每条数据都触发计算，这就相当于把无界数据集切分为多个小量的有界数据集。\n",
    "\n",
    "Flink的解决方案是全面流计算，将批数据也都当作有限流来处理。\n",
    "这样一来确实很好地降低了整体架构的复杂度。但是在机器学习等场景下还不如Spark好用。\n",
    "对Flink而言，它把有界数据集看成无界数据集的一个子集，因此，将批处理与流处理混合到同一套引擎当中，用户使用Flink引擎能够同时实现批处理与流处理任务。\n",
    "\n",
    "#### 思考题 6 Spark 和 Flink 设计思路有何差异？\n",
    "\n",
    "除此以外，关于流计算，还有传统的老牌实力派框架Storm。\n",
    "Storm的性能也可以，能做到毫秒级延迟，但问题是吞吐量不够高，而且数据一致性上有点问题。\n",
    "\n",
    "## 2.3 数据一致性\n",
    "\n",
    "数据一致性是个什么概念呢？\n",
    "\n",
    "当在分布式系统中引入状态时，自然也引入了一致性问题。根据正确性级别的不同，一致性可以分为如下三种形式：\n",
    "* 最多一次（at-most-once）可能会少：尽可能正确，但不保证一定正确。也就是说，当故障发生时，什么都不做，既不恢复丢失状态，也不重播丢失的数据。这就意味着，在系统发生故障以后，聚合结果可能会出错。\n",
    "* 至少一次（at-least-once）可能会多：在系统发生故障以后，聚合计算不会漏掉故障恢复之前窗口内的事件，但可能会重复计算某些事件，这通常用于实时性较高但准确性要求不高的场合。该模式意味着系统将以一种更加简单的方式来对算子的状态进行快照处理，系统崩溃后恢复时，算子的状态中有一些记录可能会被重放多次。例如，失败后恢复时，统计值将等于或者大于流中元素的真实值。\n",
    "* 精确一次（exactly-once）个数精准：在系统发生故障后，聚合结果与假定没有发生故障情况时一致。该模式意味着系统在进行恢复时，每条记录将在算子状态中只被重播一次。例如在一段数据流中，不管该系统崩溃或者重启了多少次，该统计结果将总是跟流中的元素的真实个数一致。这种语义加大了高吞吐和低延迟的实现难度。与“至少一次”模式相比，“精确一次”模式整体的处理速度会相对比较慢，因为在开启“精确一次”模式后，为了保证一致性，就会开启数据对齐，从而会影响系统的一些性能。\n",
    "\n",
    "| **数据一致性** | **基本含义** |**出错应对** |\n",
    "|------------|------------|------------|\n",
    "| 最多一次 at most once| 一条数据在数据流中最多出现一次| 出错即抛弃 |\n",
    "| 至少一次 at least once| 一条数据在数据流中最少出现一次| 一直补上去，多了也不管 |\n",
    "| 精确一次 exactly once| 一条数据在数据流中只出现一次| 出错了也要补上去，且保证只有一个|\n",
    "\n",
    "简单来说，数据一致性可以分为如下三种形式：\n",
    "* 最多一次（at-most-once）可能会少，当故障发生时，什么都不做，既不恢复丢失状态，也不重播丢失的数据。\n",
    "* 至少一次（at-least-once）可能会多，不会漏掉故障恢复之前窗口内的事件，但可能会重复计算某些事件，适合实时性较高但准确性要求不高的场合。\n",
    "* 精确一次（exactly-once）个数精准，每个事件用且仅用一次，适合对数据准确性要求较高的场合。\n",
    "\n",
    "#### 思考题 7 分布式计算框架里数据一致性一般有哪三种？各自适合什么场景？\n",
    "\n",
    "\n",
    "单独来看看流处理框架，曾经的主流是Storm，但随着技术发展，现在的舞台上主角已经是Spark和Flink了。\n",
    "Spark Streaming由于微批（Mini Batch）设计，难以实现毫秒级的响应，在对实时性要求比较高的场景下不太适用。\n",
    "不过由于Spark有多年积累，机器学习领域的应用很广泛。\n",
    "\n",
    "| **框架** | **数据一致性** | **吞吐量** | **延迟** | **适应场景** |\n",
    "|------------|------------|------------|------------|------------|\n",
    "| Storm      | - 至少一次 | - 低 | - 毫秒 | - 稍低吞吐量场景 |\n",
    "| Spark Streaming   | - 精确一次 | - 高 | - 100毫秒 |- 响应时间不敏感场景，如机器学习场景 |\n",
    "| Flink      | - 精确一次 | - 高 | - 毫秒 |- 响应时间敏感场景 |\n",
    "\n",
    "如果要真正实现流批一体处理，Flink更适合高吞吐量的快速响应场景。\n",
    "\n",
    "## 2.4 Flink 的发展历史\n",
    "\n",
    "Flink是Apache软件基金会的一个顶级项目，是为分布式、高性能、随时可用以及准确的流处理应用程序打造的开源流处理框架，并且可以同时支持实时计算和批量计算。\n",
    "\n",
    "Flink起源于Stratosphere 项目，该项目是在2010年到2014年间由柏林工业大学Technical University of Berlin、柏林洪堡大学 Humboldt University of Berlin和哈索普拉特纳研究所Hasso Plattner Institute联合开展的。\n",
    "\n",
    "Flink具有十分强大的功能，可以支持不同类型的应用程序。Flink的主要特性包括：批流一体化、精密的状态管理、事件时间支持以及精确一次的状态一致性保障等。\n",
    "Flink 不仅可以运行在包括 YARN、Kubernetes等在内的多种资源管理框架上，还支持在裸机集群上独立部署。\n",
    "在启用高可用选项的情况下，Flink不存在单点失效问题。\n",
    "Flink 已经可以扩展到数千核心，其状态可以达到 TB 级别，且仍能保持高吞吐、低延迟的特性。\n",
    "世界各地有很多要求严苛的流处理应用都运行在 Flink 之上。\n",
    "\n",
    "在国外，优步、网飞、微软和亚马逊等已经开始使用Flink。\n",
    "在国内，包括阿里巴巴、美团、滴滴等在内的知名互联网企业，都已经开始大规模使用Flink作为企业的分布式大数据处理引擎。\n",
    "\n",
    "在阿里巴巴，基于Flink搭建的平台于2016年正式上线，并从阿里巴巴的搜索和推荐这两大场景开始实现。\n",
    "阿里巴巴很多业务都采用了基于Flink搭建的实时计算平台，内部积累起来的状态数据，已经达到PB级别规模。\n",
    "每天在平台上处理的数据量已经超过万亿条，在峰值期间可以承担每秒超过4.72亿次的访问，最典型的应用场景是双11。\n",
    "\n",
    "2014年4月，Stratosphere代码被贡献给Apache软件基金会，成为Apache软件基金会孵化器项目。\n",
    "2014年12月，Flink项目成为Apache软件基金会顶级项目。\n",
    "\n",
    "## 2.5 Flink 的基本概念\n",
    "\n",
    "\n",
    "\n",
    "### 2.5.1 Flink 的抽象层次\n",
    "\n",
    "Flink 设计了四个抽象层次，从最基础到最高层依次是状态数据流、数据流和数据集核心接口、数据表接口、高级交互式查询语言接口。\n",
    "\n",
    "![](./images/levels_of_abstraction.svg)\n",
    "\n",
    "Flink API 最底层的抽象为有状态实时流处理。\n",
    "\n",
    "Flink API 第二层抽象是核心接口（Core APIs），也是大家一般要用到的。\n",
    "其中对咱们课程来说，最重要的就是数据流接口（DataStream API） 应用于有界/无界数据流场景。\n",
    "这一层 API 中处理的数据类型在每种编程语言中都有其对应的类。\n",
    "目前 PyFlink 官方样例没有看到提及 DataSet API（数据集接口）。\n",
    "\n",
    "Flink API 第三层抽象是数据表接口（Table API）。\n",
    "数据表接口（Table API） 是以表（Table）为中心的。\n",
    "流式数据场景下，可以表示一张正在动态改变的表。\n",
    "数据表（Table）拥有模式（schema），发音 “skee-muh” 或者“skee-mah”。\n",
    "类似于关系型数据库中的 schema，定义了组织和结构，包含了表、列、数据类型、视图、存储过程、关系等。\n",
    "数据表接口（Table API）也提供了类似于关系模型中的操作，比如 select、project、join、group-by 和 aggregate 等。\n",
    "\n",
    "在Flink中，数据表（Table）和数据流（DataStream）可切换，Flink 允许用户在编写应用程序时混合使用。\n",
    "\n",
    "Flink API 最顶层抽象是 SQL，其程序实现都是 SQL 查询表达式。\n",
    "SQL 查询语句可以在 Table API 中定义的表上执行。\n",
    "\n",
    "### 2.5.2 Flink 的架构设计\n",
    "\n",
    "Flink 可以运行在本地，也可以运行于集群之中。\n",
    "在集群上，Flink 有两类进程角色：至少一个工作管理节点（JobManager）和多个工作节点（Worker）或者也叫任务管理节点（TaskManager）。\n",
    "\n",
    "工作管理节点（JobManager）负责调度和资源管理，内部有三个不同组件：\n",
    "* 资源管理器（ResourceManager）\n",
    "* 作业分发器（Dispatcher）\n",
    "* 任务管理员（JobMaster）\n",
    "\n",
    "![](./images/processes.svg)\n",
    "\n",
    "增加工作管理节点，可以提高Flink集群系统的可用性。\n",
    "但多个工作管理节点之中，只有一个一直是首要节点（leader），其他的是待命状态（standby）\n",
    "下图展示的是一个三个工作管理节点的集群，其中首要节点发生故障后，任务进行了移交。\n",
    "\n",
    "![](./images/jobmanager_ha_overview.png)\n",
    "\n",
    "工作节点（Worker）/任务管理节点（TaskManager）执行作业流的人物（task），缓存和交换数据流。\n",
    "* 一个工作节点中资源调度的最小单位是任务槽（task slot）；\n",
    "* 一个工作节点并发处理任务的数量就是任务槽的总量；\n",
    "* 每一个任务槽可以执行多个算子。\n",
    "\n",
    "![](./images/tasks_chains.svg)\n",
    "\n",
    "客户端可以是单独的Java/Python程序，去访问已有的Flink集群。\n",
    "也可以在Flink环境下以命令方式运行。\n",
    "\n",
    "客户端访问Flink 集群有两种模式：\n",
    "* 分离模式下，客户端可以与集群断开连接；\n",
    "* 附加模式下，客户端可以与集群保持连接接收进程报告。\n",
    "\n",
    "\n",
    "\n",
    "### 2.5.3 Flink 的数据流\n",
    "\n",
    "在 Flink 中，应用程序由用户自定义算子转换而来的数据流（dataflows） 所组成。\n",
    "这些数据流形成了有向图，以一个或多个源（source）开始，并以一个或多个汇（sink）结束。\n",
    "在这个过程中，程序会对数据流进行各种变换（transformation），这些变换往往是由一个或者多个算子（operator）组成。\n",
    "\n",
    "![](./images/program_dataflow.svg)\n",
    "\n",
    "数据流的来源可以是文件、数据库、消息队列或者实时数据，比如来自Kafka，也可以是有界的历史数据。\n",
    "数据流被Flink进行变换操作后得到的结果可以是数据汇，也可以继续是新的数据流。\n",
    "新的数据流就可以发送到其他的程序过程中。\n",
    "![](./images/flink-application-sources-sinks.png)\n",
    "\n",
    "Flink 程序本质上是分布式并行程序。\n",
    "Flink 程序执行期间，一个流有一个或多个流分区（Stream Partition）；\n",
    "每个算子有一个或多个算子子任务（Operator Subtask）；\n",
    "每个子任务彼此独立，并在不同的线程、在不同的计算机或容器中运行。\n",
    "算子的子任务的个数就是其对应算子的并行度。\n",
    "在同一程序中，不同算子也可能具有不同的并行度。\n",
    "\n",
    "![](./images/parallel_dataflow.svg)\n",
    "\n",
    "\n",
    "Flink 可以将任何可序列化的对象转化为流，自带的序列化器有：\n",
    "* 基本类型，String、Long、Integer、Boolean、Array\n",
    "* 复合类型：Tuples、POJOs、Scala case classes\n",
    "\n",
    "将上面的各种数据转换成数据流之后，还要构建执行环境。\n",
    "每个 Flink 应用都需要有执行环境，在大多数示例代码中为 env。\n",
    "流式应用需要用到 StreamExecutionEnvironment。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 2.5.4 Flink 的数据表\n",
    "\n",
    "Flink提供了两种关系型接口，一种是数据表接口（Table API），另一种是Flink SQL。前者是用于Java/Python的查询接口，可以直观地对数据进行选取过滤等操作运算，后者是一种标准SQL接口。\n",
    "\n",
    "要注意，Flink 的数据表（Table）并不是传统意义的表，而是可以以不断变化修改的形态统一处理流和批的通用数据接口。\n",
    "\n",
    "数据表的来源可以是数据流，也可以是具体的文件，比如JSON文件或者CSV文件等等。\n",
    "在后续的代码环节，这些场景我们都会遇到。\n",
    "\n",
    "下面是 Flink 中数据表内容所支持的一些变量类型\n",
    "\n",
    "| **变量类型** | **简单解释** | \n",
    "|------------|------------|\n",
    "|CHAR\t|             |\n",
    "|VARCHAR|\t             |\n",
    "|STRING\t|             |\n",
    "|BOOLEAN|\t             |\n",
    "|BINARY\t|             |\n",
    "|VARBINARY|\t             |\n",
    "|BYTES\t|             |\n",
    "|DECIMAL|\tSupports fixed precision and scale.             |\n",
    "|TINYINT|\t             |\n",
    "|SMALLINT|\t             |\n",
    "|INTEGER|\t             |\n",
    "|BIGINT\t|             |\n",
    "|FLOAT\t|             |\n",
    "|DOUBLE\t|             |\n",
    "|DATE\t|             |\n",
    "|TIME\t|Supports only a precision of 0.             |\n",
    "|TIMESTAMP\t|             |\n",
    "|TIMESTAMP_LTZ|\t             |\n",
    "|INTERVAL|\tSupports only interval of MONTH and SECOND(3).             |\n",
    "|ARRAY\t|             |\n",
    "|MULTISET|\t             |\n",
    "|MAP\t|             |\n",
    "|ROW\t|             |\n",
    "|RAW\t|             |\n",
    "|Structured types|\tOnly exposed in user-defined functions yet.|  \n",
    "\n",
    "\n",
    "下面是这些变量类型的翻译，有的数据类型比如RAW我都没在计算机里面遇到过：\n",
    "\n",
    "| **变量类型** | **翻译** |\n",
    "|------------|--------|\n",
    "| CHAR       | 字符串   |\n",
    "| VARCHAR    | 可变长度字符串 |\n",
    "| STRING     | 字符串   |\n",
    "| BOOLEAN    | 布尔值   |\n",
    "| BINARY     | 二进制数据 |\n",
    "| VARBINARY  | 可变长度二进制数据 |\n",
    "| BYTES      | 字节    |\n",
    "| DECIMAL    | 十进制定点数 |\n",
    "| TINYINT    | 微整型   |\n",
    "| SMALLINT   | 小整型   |\n",
    "| INTEGER    | 整型    |\n",
    "| BIGINT     | 大整型   |\n",
    "| FLOAT      | 浮点数   |\n",
    "| DOUBLE     | 双精度浮点数 |\n",
    "| DATE       | 日期    |\n",
    "| TIME       | 无时区意义的时间 |\n",
    "| TIMESTAMP  | 时间戳   |\n",
    "| TIMESTAMP_LTZ | 本地时间戳，这个LTZ就是 local time zone |\n",
    "| INTERVAL   | 仅支持月和秒的间隔 |\n",
    "| ARRAY      | 数组    |\n",
    "| MULTISET   | 多集合  |\n",
    "| MAP        | 映射    |\n",
    "| ROW        | 行     |\n",
    "| RAW        | 原始数据  |\n",
    "|Structured types| 结构化类型, 仅在用户定义的函数中公开。 |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 环境搭建\n",
    "\n",
    "工欲善其事必先利其器，工具的选择是第一步。\n",
    "\n",
    "## 3.1 操作系统安装\n",
    "\n",
    "生产环境中，大规模集群普遍使用的是GNU/Linux操作系统，当然也有一些追求稳定性的场景可能会选用FreeBSD或者OpenBSD之类的BSD系统。\n",
    "对于初入相关领域的同学，可以选择从Debian系的发行版入手，如果以后对运维方面感兴趣，可以再自行探索RedHat系的系统发行版。\n",
    "\n",
    "本次课程选用的是 Ubuntu 22.04.3 LTS AMD64 版本操作系统，可以从[清华大学TUNA的下载地址](https://mirrors.tuna.tsinghua.edu.cn/ubuntu-releases/jammy/ubuntu-22.04.3-desktop-amd64.iso)来获取ISO文件。\n",
    "\n",
    "操作系统的安装有两种方式，物理机安装，或者虚拟机安装。\n",
    "\n",
    "物理机安装就是将操作系统安装到真实的物理实体的一天机器上。\n",
    "这样的好处是性能会充分发挥，与真实环境非常接近。\n",
    "适合有单独的一台机器可以用于安装的场景。\n",
    "但单独使用Linux系统可能要面对一些软件生态的挑战，因此不适合对Windows或者macOS软件生态有重度依赖的场景。\n",
    "\n",
    "虚拟机安装有两种方式。首先，对于Windows10/11的用户来说，可以试试用 WSL 或者 WSL2。\n",
    "WSL 是 Windows Subsystem for Linux 的缩写，意思为适用于 Linux 的 Windows 子系统。\n",
    "WSL2 是 WSL1 的升级版本，有更方便的文件访问权限。\n",
    "使用 WSL1/2 实际上是基于 Windows 10/11 内置的 Hyper-V 虚拟机，优势在于性能较好，且方便与宿主系统交互。\n",
    "缺点在于网络配置无法单独实现，通常只能使用宿主系统的网络地址转发。\n",
    "因此适合注重性能但不需要复杂集群网络结构的单节点场景。\n",
    "\n",
    "最常见的虚拟机安装方式，是基于 VMware Player 或者 VirtualBox 之类的桌面虚拟化软件来安装。\n",
    "这些桌面虚拟化软件安装虚拟机后，虚拟机的打开和运行就像是宿主机上的一个应用程序一样，非常方便。\n",
    "这种场景可能最适合大家初学时期的日常使用。\n",
    "VMware Player 的性能似乎更好些，但似乎仅在 Linux 宿主的版本中提供了进阶的网络配置功能，Windows上没有提供。\n",
    "VirtualBox 的性能表现稍差，但有非常全面的配置选项，尤其是网络自定义功能很方便，适合虚拟组网。\n",
    "\n",
    "还有一种虚拟机安装方式，是先安装专门用于虚拟化的平台系统，比如PVE（Proxmox Virtual Environment）或者 VMware ESXi 等。\n",
    "然后在安装好的虚拟化平台上来安装多个虚拟机系统。\n",
    "这种安装方式被用于很多企业的开发和生产环境中，与容器和资源管理调度等相结合，可以实现服务的冗余备份和无感知迁移。\n",
    "\n",
    "还有一种更为复杂一点的玩法，是使用嵌套虚拟化（nested virtualization），这需要你的处理器等硬件平台支持。\n",
    "对于Intel处理器，需要VT-x支持；对于AMD处理器，需要AMD-V支持。当然，最近几年的主流处理器可能都支持了。\n",
    "在硬件支持的前提下，可以用桌面虚拟化的方式借助 VMware Player 或者 VirtualBox，在其中安装 PVE 或者 ESXi 之类的虚拟化平台，然后再在平台上安装需要的虚拟机。\n",
    "当然，嵌套虚拟化可能要面对网络设置和存储资源分配等多方面较为复杂的配置问题。\n",
    "\n",
    "综合考虑，本次课程选择使用 [VirtualBox](https://www.virtualbox.org/wiki/Downloads) 这一虚拟机软件，在其中运行 [Ubuntu 22.04.3 操作系统](https://mirrors.tuna.tsinghua.edu.cn/ubuntu-releases/jammy/ubuntu-22.04.3-desktop-amd64.iso)。\n",
    "\n",
    "在安装的过程中，大家都尽量使用默认配置即可，另外需要注意的是，请尽量将用户名和密码暂且都设置为 hadoop，以保持与分布式课程的设置一致性。\n",
    "\n",
    "大家可以选择自行安装虚拟机系统，也可以直接使用本课程提供的虚拟镜像。\n",
    "虚拟机镜像中本课程项目的存储路径为`/home/hadoop/Desktop/PyFlink-Tutorial`。\n",
    "\n",
    "![](./images/VirtualMachine.png)\n",
    "\n",
    "\n",
    "## 3.2 依赖组件安装\n",
    "\n",
    "安装好操作系统之后，还需要安装一些基础组件。\n",
    "编程语言方面，本课程使用的是Python编程语言。\n",
    "其他组件主要有两个，一个适用于流数据传输与接收的 Apache Kafka，另一个是用于流数据的处理与计算的 Apache Flink。\n",
    "\n",
    "考虑到授课时长和复杂度等方面的因素，本次课程不再单独讲授 Apache Kafka 的安装，也不使用单独运行的 Kafka，而是使用 docker 镜像来运行一个本地实例。\n",
    "在 Python 下使用的 PyFlink 需要本地安装有 Apache Flink，只需要采取单节点模式即可。\n",
    "\n",
    "### 3.2.1 Anaconda3 安装\n",
    "\n",
    "考虑到版本管理和环境控制等方面的便利性，选择安装 Anaconda3。\n",
    "首先从TUNA下载Anaconda3安装包。\n",
    "这里使用的是特定的版本，目的是为了保证后面组件的兼容性。\n",
    "\n",
    "```Bash\n",
    "wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-2023.09-0-Linux-x86_64.sh\n",
    "sh Anaconda3-2023.09-0-Linux-x86_64.sh\n",
    "```\n",
    "安装过程中，请使用默认设置。\n",
    "应该安装在`~/anaconda3`。\n",
    "\n",
    "注意不要下载成其他操作系统或者其他处理器架构的版本。\n",
    "\n",
    "### 3.2.2 Python 3.9 安装\n",
    "\n",
    "Anaconda3 最新版默认安装的可能已经是 Python 3.11 甚至 Python 3.12。\n",
    "而我们所用的 PyFlink 还是适合运行在 Python 3.9 这个版本上。\n",
    "\n",
    "这里要强调的是，生产环境中，对于很多组件不能够盲目追求最新，而环境的稳定性才是最重要的。\n",
    "\n",
    "通过 conda 安装 Python 3.9 将变得简单可靠。\n",
    "\n",
    "```Bash\n",
    "conda create -n pyflink_39 python=3.9\n",
    "conda activate pyflink_39\n",
    "```\n",
    "\n",
    "\n",
    "### 3.2.3 Apache-Flink 安装\n",
    "\n",
    "先去[Apache 官网](https://dlcdn.apache.org/flink/)下载安装 flink，这里以 1.18.0 为例：\n",
    "\n",
    "```Bash\n",
    "wget https://dlcdn.apache.org/flink/flink-1.18.0/flink-1.18.0-bin-scala_2.12.tgz\n",
    "sudo tar -zxvf flink-1.18.0-bin-scala_2.12.tgz  -C /usr/local   \n",
    "```\n",
    "\n",
    "修改目录名称，并设置权限，命令如下：\n",
    "```Bash\n",
    "cd /usr/local\n",
    "sudo mv / flink-1.18.0 ./flink #这里是因为我这里下的是这个版本，读者需要酌情调整\n",
    "sudo chown -R hadoop:hadoop ./flink ##这里是因为我这里虚拟机的用户名是这个，读者需要酌情调整\n",
    "```\n",
    "\n",
    "Flink解压缩并且设置好权限后，直接就可以在本地模式运行，不需要修改任何配置。\n",
    "如果要做调整，可以编辑`“/usr/local/flink/conf/flink-conf.yam`这个文件。\n",
    "比如其中的`env.java.home`参就可以设置为本地Java的绝对路径\n",
    "不过一般不需要手动修改什么配置。\n",
    "\n",
    "不过，需要注意的是，Flink现在需要的是Java11，所以需要用下列命令手动安装一下：\n",
    "```Bash\n",
    "sudo apt install openjdk-11-jdk -y\n",
    "```\n",
    "\n",
    "接下来还需要修接下来还需要修改配置文件，添加环境变量：\n",
    "\n",
    "```Bash\n",
    "nano ~/.bashrc\n",
    "```\n",
    "\n",
    "文件中添加如下内容：\n",
    "```Bash\n",
    "export FLINK_HOME=/usr/local/flink\n",
    "export PATH=$FLINK_HOME/bin:$PATH\n",
    "```\n",
    "\n",
    "保存并退出.bashrc文件，然后执行如下命令让配置文件生效：\n",
    "```Bash\n",
    "source ~/.bashrc\n",
    "```\n",
    "\n",
    "### 3.2.4 Kafka-Python 和 PyFlink 以及依赖包\n",
    "\n",
    "然后使用 pip 安装 Kafka-python、apache-flink 以及依赖包。\n",
    "\n",
    "```Bash\n",
    "pip install kafka-python \n",
    "pip install apache-flink\n",
    "pip install chardet pandas numpy scipy simpy \n",
    "pip install matplotlib cython sympy xlrd pyopengl BeautifulSoup4 pyqt6 scikit-learn requests tensorflow torch keras tqdm gym DRL\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 流数据的生成与传输\n",
    "\n",
    "## 4.1 流数据的来源\n",
    "\n",
    "网站数据采集，用户行为产生，购物网站、社交网站。\n",
    "传感器采集，物联网传输，科研观测、探测器。\n",
    "人类在互联网上的几乎一切活动，都可以看作是流数据的生成过程。\n",
    "\n",
    "生成了流数据之后，如何使之流动，就成了问题。\n",
    "具体来说，要对其进行传输、存储、接收。\n",
    "\n",
    "## 4.2 Apache Kafka 基础\n",
    "\n",
    "本次课程我们使用的是 [Apache Kafka](https://kafka.apache.org/intro)。\n",
    "Kafka 是一个流数据传输平台，腾讯、字节跳动、Cisco、Oracle、Paypal、Spotify等等厂商都在使用。\n",
    "Kafka 主要具有三个主要的功能：\n",
    "* 写入（write）和读取（read）数据流；\n",
    "* 根据需求设计存储（store）数据流；\n",
    "* 对数据流进行处理（process）。\n",
    "\n",
    "简单来说，一个实际运行的 Kafka 分布式系统是基于 TCP/IP 网络协议的集群。\n",
    "具体又分为服务器（Servers）和客户端（Clients）两种。\n",
    "服务端可以是一个，也可以是多台，构成存储层的服务器称为代理（Broker）。\n",
    "客户端允许用户编写分布式应用或者微服务，可以对数据流进行并行的读写和处理。\n",
    "\n",
    "Kafka 支持很多种编程语言，比如 Java、Scala、Go、Python等。\n",
    "另外，Kafka 可以接入其他系统，或者将数据传输给其他系统。\n",
    "\n",
    "## 4.3 Kafka 的构成\n",
    "\n",
    "Kafka 的最小元素是事件（Event），一个事件一般有三个数据项目：\n",
    "* 事件键名（Event Key）：\"张三\"\n",
    "* 事件键值（Event Value）：\"支付话费50元\"\n",
    "* 事件时间戳（Event TimeStamp）：\"2023年12月1日12点48分\"\n",
    "当然，实际上还可以添加很多额外的数据。\n",
    "\n",
    "向整个 Kafka 系统写入事件流的，称为生产者（Producer），订阅（读取并处理）事件流的，称为消费者（Consumer）。\n",
    "有生产者写入事件，实际上就是生成流数据；也有消费者读取事件，实际上也就是使用流数据。\n",
    "\n",
    "可 Kafka 是一个分布式的并行系统，能一下子收发处理好多个数据流，那数据流之间怎么来区分呢？\n",
    "这就引入了下一个概念，就是主题（Topic）。\n",
    "主题就像是磁盘路径下的文件夹，关注同一个数据流的生产者和消费者只要指定好同样的主题，就不会和其他的数据流有混淆了。\n",
    "\n",
    "如果情况复杂一点，一个主题下有多个生产者，该怎么来保证消费者读取数据的时候不混乱呢？\n",
    "Kafka 给主题引入了分区的概念，具有相同事件关键字的事件会被写入相同的主题分区，保证特定主题分区的任何用户都能以与写入事件完全相同的顺序读取该分区的事件。\n",
    "\n",
    "![](./images/streams-and-tables-p1_p4.png)\n",
    "\n",
    "## 4.4 Kafka 容器运行\n",
    "\n",
    "本次课程选择使用 Docker 镜像方式来运行 Kafka。\n",
    "可以首先找到一个有权限写入和存储的路径，创建名为`docker-compose.yml`的文件，内容如下：\n",
    "```YML\n",
    "version: '3'\n",
    "services:\n",
    "  zookeeper:\n",
    "    image: 'bitnami/zookeeper:latest'\n",
    "    environment:\n",
    "      - ALLOW_ANONYMOUS_LOGIN=yes\n",
    "  kafka:\n",
    "    image: 'bitnami/kafka:latest'\n",
    "    ports:\n",
    "      - '9092:9092'\n",
    "    environment:\n",
    "      - KAFKA_ADVERTISED_HOST_NAME=localhost\n",
    "      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181\n",
    "      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092\n",
    "      - KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9092\n",
    "      - KAFKA_CREATE_TOPICS=test:1:1\n",
    "      - ALLOW_PLAINTEXT_LISTENER=yes\n",
    "    depends_on:\n",
    "      - zookeeper\n",
    "```\n",
    "\n",
    "如果有需要，可以使用以下命令停止并删除当前所有的 Docker 容器：\n",
    "```Bash\n",
    "docker stop $(docker ps -a -q) && docker rm $(docker ps -a -q)\n",
    "#这条命令会先停止所有正在运行的 Docker 容器，然后删除所有 Docker 容器。\n",
    "```\n",
    "如果需要删除所有 Docker 镜像，可以使用以下命令：\n",
    "```Bash\n",
    "docker rmi $(docker images -q)\n",
    "# 请注意，这些命令会删除所有 Docker 容器和镜像，包括正在运行的容器和镜像\n",
    "```\n",
    "\n",
    "将上面名为`docker-compose.yml`的文件保存好后，可以在该文件所在路径下运行下列命令启动容器：\n",
    "```Bash\n",
    "sudo docker-compose up -d\n",
    "```\n",
    "\n",
    "然后终端中可能会有如下提示：\n",
    "```Bash\n",
    "Starting pyflink-tutorial_zookeeper_1 ... done\n",
    "Starting pyflink-tutorial_kafka_1     ... done\n",
    "```\n",
    "\n",
    "如果后续运行 Python 代码推送数据到 Kafka 的时候遇到报错，可以检查容器是否正常运行，使用如下命令：\n",
    "```Bash\n",
    "sudo docker ps\n",
    "```\n",
    "\n",
    "正常情况下应该会有一个 Kafka 容器和一个 ZooKeeper 容器运行，如下所示：\n",
    "```Bash\n",
    "CONTAINER ID   IMAGE                      COMMAND                  CREATED      STATUS              PORTS                                       NAMES\n",
    "6dea7ce04807   bitnami/kafka:latest       \"/opt/bitnami/script…\"   6 days ago   Up 6 seconds        0.0.0.0:9092->9092/tcp, :::9092->9092/tcp   pyflink-tutorial_kafka_1\n",
    "88d0c914f65d   bitnami/zookeeper:latest   \"/opt/bitnami/script…\"   6 days ago   Up About a minute   2181/tcp, 2888/tcp, 3888/tcp, 8080/tcp      pyflink-tutorial_zookeeper_1\n",
    "```\n",
    "\n",
    "\n",
    "如果缺少某一个，就重新运行上面的命令来启动容器即可：\n",
    "```Bash\n",
    "sudo docker-compose up -d\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Kafka 环境生成、传输、展示文本数据流\n",
    "\n",
    "## 5.1 基于文本生成数据流\n",
    "\n",
    "接下来我们使用 Python 创建一个生产者，读取本地的一个“hamlet.txt”文本文件，发送给运行在本地容器中的 Kafka。\n",
    "\n",
    "使用以下代码将“TXT”文件转换为“Kafka Stream”。\n",
    "\n",
    "```Python\n",
    "#以下代码使用kafka-python模块将数据发送到本地Kafka集群。\n",
    "#此代码打开一个名为“hamlet.txt”的文本文件，并将其内容作为流发送到指定的 Kafka 主题“hamlet”：\n",
    "\n",
    "from kafka import KafkaProducer\n",
    "import time\n",
    "import os\n",
    "\n",
    "# 定义一个函数，用于将文件发送到Kafka，参数为文件路径、主题和Kafka服务器地址\n",
    "def send_file_to_kafka(file_path: str, topic: str, bootstrap_servers: str):\n",
    "    # 创建一个KafkaProducer实例，用于发送消息\n",
    "    producer = KafkaProducer(bootstrap_servers=bootstrap_servers)\n",
    "    # 获取文件大小\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    # 循环发送文件\n",
    "    while True:\n",
    "        # 打开文件，以二进制方式读取\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            # 循环读取文件\n",
    "            while True:\n",
    "                # 读取1024字节的数据\n",
    "                data = f.read(1024)\n",
    "                # 如果读取完毕，则跳出循环\n",
    "                if not data:\n",
    "                    break\n",
    "                # 将数据发送到Kafka，并打印发送的进度\n",
    "                producer.send(topic, data)\n",
    "                # 计算已发送的数据量\n",
    "                percent_sent = (f.tell() / file_size) * 100\n",
    "                bytes_sent = len(data)\n",
    "                print(f\"Sent {bytes_sent} bytes {topic} {percent_sent:.2f}% sent\")\n",
    "                # 每3秒打印一次发送进度\n",
    "                time.sleep(3)\n",
    "        # 询问用户是否继续发送文件\n",
    "        user_input = input(\"Press 'c' to continue sending the file or 'q' to quit: \")\n",
    "        # 如果用户输入q，则退出循环\n",
    "        if user_input == \"q\":\n",
    "            break\n",
    "\n",
    "# 调用函数，将hamlet.txt文件发送到Kafka，主题为hamlet，Kafka服务器地址为localhost:9092\n",
    "send_file_to_kafka(\"./hamlet.txt\",  \"hamlet\", \"localhost:9092\")\n",
    "\n",
    "# 在此代码中，send_file_to_kafka 函数接受三个参数：file_path、topic 和 bootstrap_servers。\n",
    "# file_path是本地文件的路径，topic是数据要发送到的Kafka主题，bootstrap_servers是Kafka集群的地址。\n",
    "# 该函数使用with语句打开文件，读取其内容，并将它们作为流数据发送到指定的Kafka主题。\n",
    "# 发送过程中，打印出发送进度，并使用time.sleep方法暂停3秒来控制发送速率。\n",
    "```\n",
    "\n",
    "运行起来后在终端中的效果大概如下所示：\n",
    "```Bash\n",
    "Sent 1024 bytes hamlet 0.57% sent\n",
    "Sent 1024 bytes hamlet 1.13% sent\n",
    "Sent 1024 bytes hamlet 1.70% sent\n",
    "Sent 1024 bytes hamlet 2.27% sent\n",
    "Sent 1024 bytes hamlet 2.83% sent\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## 5.2 使用 Kafka-Python 接收文本数据流\n",
    "\n",
    "使用以下代码将数据流中主题为hamlet的文本数据展示出来。\n",
    "\n",
    "```Python\n",
    "from kafka import KafkaConsumer\n",
    "\n",
    "# 创建一个KafkaConsumer对象，用于从Kafka主题中读取消息\n",
    "consumer = KafkaConsumer(\n",
    "    # 指定要读取的消息主题\n",
    "    \"hamlet\",\n",
    "    # 指定Kafka服务器的地址和端口\n",
    "    bootstrap_servers=[\"localhost:9092\"],\n",
    "    # 指定消费偏移量，可以是earliest、latest或指定specific offset\n",
    "    auto_offset_reset=\"earliest\",\n",
    "    # 指定是否在消费时自动提交偏移量\n",
    "    enable_auto_commit=True,\n",
    "    # 指定消费组名\n",
    "    group_id=\"my-group\",\n",
    "    # 指定消息反序列化函数，将字节转换为字符串\n",
    "    value_deserializer=lambda x: x.decode(\"utf-8\")\n",
    ")\n",
    "\n",
    "# 循环读取Kafka消息，并打印消息长度和消息内容\n",
    "for message in consumer:\n",
    "    print(f\"Received {len(message.value)} bytes from Kafka topic {message.topic}\")\n",
    "    print(f\"{message.value}\")\n",
    "\n",
    "\n",
    "# 在上面的代码中，我们使用`KafkaConsumer`类来创建一个消费者对象。\n",
    "# 我们将 `hamlet` 作为主题名称传递给构造函数。\n",
    "# 我们还传递 `localhost:9092` 作为引导服务器的地址。\n",
    "# 我们使用 `value_deserializer` 参数来解码从 Kafka 主题收到的消息。\n",
    "# 我们使用 `for` 循环来迭代消费者对象，并使用 `print` 函数来打印消息的内容。\n",
    "```\n",
    "\n",
    "\n",
    "运行起来后在终端中的效果大概如下所示：\n",
    "```Bash\n",
    "start reading data from kafka\n",
    "10>\n",
    "The Tragedy of Hamlet, Prince of Denmark\n",
    "Shakespeare homepage | Hamlet | Entire play\n",
    "ACT I\n",
    "\n",
    "SCENE I. Elsinore. A platform before the castle.\n",
    "\n",
    "FRANCISCO at his post. Enter to him BERNARDO\n",
    "BERNARDO\n",
    "Who's there?\n",
    "FRANCISCO\n",
    "Nay, answer me: stand, and unfold yourself.\n",
    "BERNARDO\n",
    "Long live the king!\n",
    "FRANCISCO\n",
    "```\n",
    "\n",
    "## 5.3 简单的词频统计\n",
    "\n",
    "使用以下代码，对刚生成的这个 hamlet 数据流的逐行词频进行统计：\n",
    "```Python\n",
    "import argparse\n",
    "import io\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from io import StringIO\n",
    "from pyflink.common import SimpleStringSchema, Time\n",
    "from pyflink.datastream.connectors.kafka import FlinkKafkaConsumer\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.table import (DataTypes, EnvironmentSettings, FormatDescriptor,\n",
    "                           Schema, StreamTableEnvironment, TableDescriptor,\n",
    "                           TableEnvironment, udf)\n",
    "from pyflink.table.expressions import col, lit\n",
    "\n",
    "# 定义一个函数，用于移除文本中的标点符号\n",
    "def remove_punctuation(text):\n",
    "    # 使用正则表达式移除文本中的标点符号\n",
    "    return re.sub(r'[^\\w\\s]','',text)\n",
    "\n",
    "# 定义一个函数，用于计算文本中字节数\n",
    "def count_bytes(text):\n",
    "    # 返回文本中字节数\n",
    "    return len(text.encode('utf-8'))\n",
    "\n",
    "# 定义一个函数，用于计算文本中单词数\n",
    "def count_words(text):\n",
    "    # 将文本拆分成单词数组\n",
    "    words = text.split()\n",
    "    # 计算单词数\n",
    "    result = dict(Counter(words))\n",
    "    # 获取出现次数最多的单词\n",
    "    max_word = max(result, key=result.get)\n",
    "    # 返回文本中字节数、单词数、出现次数最多的单词以及出现次数\n",
    "    return {'total_bytes': count_bytes(text), 'total_words': len(words), 'most_frequent_word': max_word, 'most_frequent_word_count': result[max_word]}\n",
    "\n",
    "# 定义一个函数，用于从Kafka中读取数据\n",
    "def read_from_kafka():\n",
    "    # 获取StreamExecutionEnvironment实例\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()  \n",
    "    # 添加FlinkKafkaConnector的jar包\n",
    "    env.add_jars(\"file:///home/hadoop/Desktop/PyFlink-Tutorial/flink-sql-connector-kafka-3.1-SNAPSHOT.jar\")\n",
    "    print(\"start reading data from kafka\")\n",
    "    # 创建FlinkKafkaConsumer实例\n",
    "    kafka_consumer = FlinkKafkaConsumer(\n",
    "        # 指定要读取的topic\n",
    "        topics='hamlet', \n",
    "        # 指定反序列化器\n",
    "        deserialization_schema= SimpleStringSchema('UTF-8'), \n",
    "        # 指定Kafka服务器的配置信息\n",
    "        properties={'bootstrap.servers': 'localhost:9092', 'group.id': 'my-group'} \n",
    "    )\n",
    "    \n",
    "    # 从最早的记录开始读取\n",
    "    kafka_consumer.set_start_from_earliest()\n",
    "    # 将Kafka中的数据添加到Stream中\n",
    "    stream_original_text = env.add_source(kafka_consumer)\n",
    "    # 对Stream中的数据进行处理，移除文本中的标点符号\n",
    "    stream_remove_punctuation = stream_original_text.map(lambda x: remove_punctuation(x))\n",
    "    # 对Stream中的数据进行处理，计算文本中单词数\n",
    "    stream_count_words = stream_remove_punctuation.map(lambda x: count_words(x))\n",
    "    # 将处理后的数据打印出来\n",
    "    stream_count_words.print()\n",
    "    # 执行Stream中的任务\n",
    "    env.execute()\n",
    "\n",
    "# 调用read_from_kafka函数，从Kafka中读取数据\n",
    "read_from_kafka()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "运行起来后在终端中的效果大概如下所示：\n",
    "```Bash\n",
    "start reading data from kafka\n",
    "10> {'total_bytes': 979, 'total_words': 159, 'most_frequent_word': 'FRANCISCO', 'most_frequent_word_count': 9}\n",
    "10> {'total_bytes': 984, 'total_words': 173, 'most_frequent_word': 'of', 'most_frequent_word_count': 7}\n",
    "10> {'total_bytes': 981, 'total_words': 172, 'most_frequent_word': 'it', 'most_frequent_word_count': 8}\n",
    "10> {'total_bytes': 987, 'total_words': 186, 'most_frequent_word': 'the', 'most_frequent_word_count': 12}\n",
    "10> {'total_bytes': 979, 'total_words': 182, 'most_frequent_word': 'of', 'most_frequent_word_count': 9}\n",
    "10> {'total_bytes': 993, 'total_words': 181, 'most_frequent_word': 'the', 'most_frequent_word_count': 14}\n",
    "10> {'total_bytes': 977, 'total_words': 188, 'most_frequent_word': 'to', 'most_frequent_word_count': 8}\n",
    "10> {'total_bytes': 982, 'total_words': 186, 'most_frequent_word': 'and', 'most_frequent_word_count': 6}\n",
    "10> {'total_bytes': 985, 'total_words': 170, 'most_frequent_word': 'our', 'most_frequent_word_count': 7}\n",
    "10> {'total_bytes': 987, 'total_words': 177, 'most_frequent_word': 'and', 'most_frequent_word_count': 8}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Kafka + PyFlink 生成、传输、展示表格数据流\n",
    "\n",
    "## 6.1 基于文本生成数据流\n",
    "\n",
    "接下来我们使用 Python 创建一个生产者，读取本地的一个“data.csv”文件，发送给运行在本地容器中的 Kafka。\n",
    "\n",
    "使用以下代码将“CSV”文件转换为“Kafka Stream”。\n",
    "\n",
    "```Python\n",
    "from kafka import KafkaProducer\n",
    "import time\n",
    "import os\n",
    "import chardet\n",
    "\n",
    "def send_file_to_kafka(file_path: str, topic: str, bootstrap_servers: str):\n",
    "    # 创建KafkaProducer实例，用于发送消息\n",
    "    producer = KafkaProducer(bootstrap_servers=bootstrap_servers)\n",
    "    # 获取文件大小\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    # 检测文件编码\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        result = chardet.detect(f.read())\n",
    "        encoding = result[\"encoding\"]\n",
    "\n",
    "    # 读取文件内容\n",
    "    with open(file_path, \"r\", encoding=encoding) as f:\n",
    "        lines_total = len(f.readlines())\n",
    "    lines_send = 0\n",
    "    # 循环发送文件内容\n",
    "    while True:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            while True:\n",
    "                data = f.readlines(10)\n",
    "                if not data:\n",
    "                    break\n",
    "                data_str = str(data)\n",
    "                data_bytes = data_str.encode()\n",
    "                # 发送消息\n",
    "                producer.send(topic, data_bytes)\n",
    "                lines_send += 10\n",
    "                # 计算已发送的百分比\n",
    "                percent_sent = (lines_send / lines_total) * 100                \n",
    "                bytes_sent = len(data_bytes)\n",
    "                print(f\"Sent {bytes_sent} bytes {topic} {percent_sent:.2f}% sent\")\n",
    "                # 每3秒检查一次\n",
    "                time.sleep(3)\n",
    "        # 询问是否继续发送\n",
    "        user_input = input(\"Press 'c' to continue sending the file or 'q' to quit: \")\n",
    "        if user_input == \"q\":\n",
    "            break\n",
    "# 调用函数，将文件发送到Kafka\n",
    "send_file_to_kafka(\"./data.csv\",  \"data\", \"localhost:9092\")\n",
    "```\n",
    "\n",
    "运行起来后在终端中的效果大概如下所示：\n",
    "```Bash\n",
    "Sent 76 bytes data 0.01% sent\n",
    "Sent 75 bytes data 0.02% sent\n",
    "Sent 77 bytes data 0.03% sent\n",
    "Sent 77 bytes data 0.04% sent\n",
    "Sent 75 bytes data 0.05% sent\n",
    "Sent 78 bytes data 0.06% sent\n",
    "Sent 75 bytes data 0.07% sent\n",
    "Sent 78 bytes data 0.08% sent\n",
    "Sent 75 bytes data 0.09% sent\n",
    "```\n",
    "\n",
    "## 6.2 使用 Kafka-Python 接收表格数据流\n",
    "\n",
    "使用以下代码将数据流中主题为hamlet的文本数据展示出来。\n",
    "\n",
    "```Python\n",
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyflink.common import Types\n",
    "from pyflink.common.serialization import SimpleStringSchema\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.datastream.connectors.file_system import FileSink, FileSource, OutputFileConfig, RollingPolicy\n",
    "from pyflink.datastream.connectors.kafka import FlinkKafkaConsumer, FlinkKafkaProducer\n",
    "from pyflink.datastream.formats.csv import CsvRowDeserializationSchema, CsvRowSerializationSchema\n",
    "from pyflink.datastream.state import ValueStateDescriptor\n",
    "from pyflink.table import StreamTableEnvironment\n",
    "\n",
    "\n",
    "# 定义一个函数，从Kafka读取数据\n",
    "def read_from_kafka():\n",
    "    # 获取当前的StreamExecutionEnvironment\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()    \n",
    "    # 添加Flink Kafka连接器JAR包\n",
    "    env.add_jars(\"file:///home/hadoop/Desktop/PyFlink-Tutorial/flink-sql-connector-kafka-3.1-SNAPSHOT.jar\")\n",
    "    # 打印信息\n",
    "    print(\"start reading data from kafka\")\n",
    "    # 创建一个FlinkKafkaConsumer，用于从Kafka读取数据\n",
    "    kafka_consumer = FlinkKafkaConsumer(\n",
    "        topics='data', # 流数据的主题\n",
    "        deserialization_schema= SimpleStringSchema('UTF-8'),\n",
    "        properties={'bootstrap.servers': 'localhost:9092', 'group.id': 'my-group'} # Kafka 服务器地址和端口\n",
    "    )\n",
    "\n",
    "    # 从最早的记录开始读取数据\n",
    "    kafka_consumer.set_start_from_earliest()\n",
    "    # 将Kafka消费者添加到StreamExecutionEnvironment，并打印输出\n",
    "    env.add_source(kafka_consumer).print()\n",
    "    # 执行StreamExecutionEnvironment\n",
    "    env.execute()\n",
    "\n",
    "# 调用函数\n",
    "if __name__ == '__main__':\n",
    "    read_from_kafka()\n",
    "```\n",
    "\n",
    "运行起来后在终端中的效果大概如下所示：\n",
    "```Bash\n",
    "start reading data from kafka\n",
    "1> [b'Credit Card Number,Name,ID Number,Amount,Direction,Transaction Time\\r\\n']\n",
    "1> [b'4590840829695540,Anne Jordan,86112888,6954,out,2022-12-30 00:00:01\\r\\n']\n",
    "1> [b'4438465406412071,Colin Thomas,470402943,4150,out,2022-12-30 00:00:01\\r\\n']\n",
    "1> [b'4421638609078837,Robert Wilson,680417227,6319,in,2022-12-30 00:00:02\\r\\n']\n",
    "1> [b'4639776153365867,Jill Barron,920108822,5520,in,2022-12-30 00:00:02\\r\\n']\n",
    "1> [b'4620263044977862,Connie Jimenez,820128975,5032,in,2022-12-30 00:00:04\\r\\n']\n",
    "1> [b'4734682376534996,Monica Knapp,98062296,4562,in,2022-12-30 00:00:04\\r\\n']\n",
    "1> [b'4149025028086396,Kevin Thompson,050314421,4843,in,2022-12-30 00:00:05\\r\\n']\n",
    "1> [b'4262779556561458,Linda Riley,840421391,473,out,2022-12-30 00:00:05\\r\\n']\n",
    "1> [b'4480953414532524,Andrea Ross,530718165,7224,out,2022-12-30 00:00:05\\r\\n']\n",
    "1> [b'4463121597813092,Patrick Yoder,980609177,1757,in,2022-12-30 00:00:05\\r\\n']\n",
    "1> [b'4349295739787982,Stephen Moore,731009319,7919,out,2022-12-30 00:00:05\\r\\n']\n",
    "```\n",
    "\n",
    "## 6.3 简单处理：映射（Map）和筛选（Filter）\n",
    "\n",
    "`MapFunction`: 将一个元素作为输入并将一个元素作为输出的函数。通过对每个元素应用转换，它可用于转换数据流。\n",
    "\n",
    "`FilterFunction`: 将一个元素作为输入并返回一个布尔值的函数。它可用于删除不符合特定条件的元素，从而过滤数据流。\n",
    "\n",
    "参考下面的代码对上一次课程中主题为data的csv表格数据流进行简单处理。\n",
    "```Python\n",
    "import re\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "from io import StringIO\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from pyflink.table import StreamTableEnvironment\n",
    "from pyflink.common import WatermarkStrategy, Encoder, Types\n",
    "from pyflink.datastream import StreamExecutionEnvironment, RuntimeExecutionMode\n",
    "from pyflink.datastream.connectors.file_system import FileSource, StreamFormat, FileSink, OutputFileConfig, RollingPolicy\n",
    "from pyflink.common import Types, SimpleStringSchema\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.datastream.connectors.kafka import FlinkKafkaProducer, FlinkKafkaConsumer\n",
    "\n",
    "\n",
    "# 定义开始年份和结束年份\n",
    "Year_Begin =1999\n",
    "Year_End = 2023\n",
    "\n",
    "def extract_numbers(x):\n",
    "    return ' '.join(re.findall(r'\\d+', x))\n",
    "\n",
    "def filter_years(x):\n",
    "    return any([Year_Begin <= int(i) <= Year_End for i in x.split()])\n",
    "\n",
    "def map_years(x):\n",
    "    return [i for i in x.split() if Year_Begin <= int(i) <= Year_End][0]\n",
    "\n",
    "\n",
    "def read_from_kafka():\n",
    "    # 获取流环境\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()    \n",
    "    # 添加jar包\n",
    "    env.add_jars(\"file:///home/hadoop/Desktop/PyFlink-Tutorial/flink-sql-connector-kafka-3.1-SNAPSHOT.jar\")\n",
    "    print(\"start reading data from kafka\")\n",
    "    # 创建kafka消费者\n",
    "    kafka_consumer = FlinkKafkaConsumer(\n",
    "        topics='data', \n",
    "        deserialization_schema= SimpleStringSchema('UTF-8'),\n",
    "        properties={'bootstrap.servers': 'localhost:9092', 'group.id': 'my-group'} \n",
    "    )\n",
    "    # 从最早开始读取数据\n",
    "    kafka_consumer.set_start_from_earliest()\n",
    "    # 创建输出流\n",
    "    output = StringIO()\n",
    "    sys.stdout = output\n",
    "    # 添加源，并过滤出指定年份的数据\n",
    "    ds = env.add_source(kafka_consumer)\n",
    "    ds = ds.map(extract_numbers)\n",
    "    ds = ds.filter(filter_years)\n",
    "    ds = ds.map(map_years)\n",
    "    ds.print()\n",
    "    env.execute()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    read_from_kafka()\n",
    "```\n",
    "\n",
    "\n",
    "运行起来后在终端中的效果大概如下所示：\n",
    "```Bash\n",
    "start reading data from kafka\n",
    "1> 2022\n",
    "1> 2022\n",
    "1> 2022\n",
    "1> 2022\n",
    "1> 2022\n",
    "1> 2022\n",
    "1> 2022\n",
    "1> 2022\n",
    "1> 2022\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 PyFlink 数据流\n",
    "\n",
    "## 7.1 数据流的基本操作\n",
    "\n",
    "下面的代码是读取一个 JSON 文件的基本样例，进行了哪些操作？\n",
    "\n",
    "```Python\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "from pyflink.common import Types\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "\n",
    "# 定义show函数，用于显示数据流\n",
    "def show(ds, env):\n",
    "    ds.print()\n",
    "    env.execute()\n",
    "\n",
    "# 定义update_tel函数，用于更新tel字段\n",
    "def update_tel(data):\n",
    "    json_data = json.loads(data.info)\n",
    "    json_data['tel'] += 1\n",
    "    return data.id, json.dumps(json_data)\n",
    "\n",
    "# 定义filter_by_id函数，用于过滤id字段\n",
    "def filter_by_id(data):\n",
    "    return data.id == 1\n",
    "\n",
    "# 定义map_country_tel函数，用于将国家字段和tel字段映射到元组中\n",
    "def map_country_tel(data):\n",
    "    json_data = json.loads(data.info)\n",
    "    return json_data['addr']['country'], json_data['tel']\n",
    "\n",
    "# 定义key_by_country函数，用于将元组中的国家字段作为key\n",
    "def key_by_country(data):\n",
    "    return data[0]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=\"%(message)s\")\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()\n",
    "    env.set_parallelism(1)\n",
    "    ds = env.from_collection(\n",
    "        collection=[\n",
    "            (1, '{\"name\": \"Flink\", \"tel\": 111, \"addr\": {\"country\": \"Germany\", \"city\": \"Berlin\"}}'),\n",
    "            (2, '{\"name\": \"hello\", \"tel\": 222, \"addr\": {\"country\": \"China\", \"city\": \"Shanghai\"}}'),\n",
    "            (3, '{\"name\": \"world\", \"tel\": 333, \"addr\": {\"country\": \"USA\", \"city\": \"NewYork\"}}'),\n",
    "            (4, '{\"name\": \"PyFlink\", \"tel\": 444, \"addr\": {\"country\": \"China\", \"city\": \"Hangzhou\"}}')\n",
    "        ],\n",
    "        type_info=Types.ROW_NAMED([\"id\", \"info\"], [Types.INT(), Types.STRING()])\n",
    "    )\n",
    "    print('\\nFirst we map it: \\n')\n",
    "    # 调用show函数，显示数据流\n",
    "    show(ds.map(update_tel), env)\n",
    "    \n",
    "    print('\\nThen we filter it: \\n')\n",
    "    # 调用show函数，显示筛选后的数据流\n",
    "    show(ds.filter(filter_by_id), env)\n",
    "\n",
    "    print('\\nThen we select it: \\n')\n",
    "    # 调用show函数，显示按照国家字段分组后的数据流\n",
    "    show(ds.map(map_country_tel).key_by(key_by_country), env)\n",
    "```\n",
    "\n",
    "`Map`将一个元素作为输入并将一个元素作为输出。通过对每个元素应用转换，用于转换数据流。\n",
    "`Filter`将一个元素作为输入并返回一个布尔值。用于删除不符合特定条件的元素，从而筛选过滤数据流。\n",
    "`KeyBy`将数据流分区为不同的逻辑分区,由键控制，相同键的所有元素都分配到同一个分区中。\n",
    "\n",
    "## 7.2 数据流生成\n",
    "\n",
    "```Python\n",
    "#以下代码使用kafka-python模块将数据发送到本地Kafka集群。\n",
    "#此代码打开一个名为“transaction_data_ generated.csv”的文本文件，并将其内容作为流发送到指定的 Kafka 主题“transaction”：\n",
    "\n",
    "from kafka import KafkaProducer\n",
    "import time\n",
    "import os\n",
    "import chardet\n",
    "\n",
    "# 定义一个函数，用于将文件发送到Kafka，参数为文件路径、主题和Kafka服务器地址\n",
    "def send_file_to_kafka(file_path: str, topic: str, bootstrap_servers: str):\n",
    "    # 创建一个KafkaProducer对象，用于发送消息\n",
    "    producer = KafkaProducer(bootstrap_servers=bootstrap_servers)\n",
    "    # 获取文件大小\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    # 检测文件编码\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        result = chardet.detect(f.read())\n",
    "        encoding = result[\"encoding\"]\n",
    "    # 读取文件内容\n",
    "    with open(file_path, \"r\", encoding=encoding) as f:\n",
    "        lines_total = len(f.readlines())\n",
    "    lines_send = 0\n",
    "    # 循环发送文件内容\n",
    "    while True:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            while True:\n",
    "                data = f.readlines(10)\n",
    "                if not data:\n",
    "                    break\n",
    "                data_str = str(data)\n",
    "                data_bytes = data_str.encode()\n",
    "                # 发送消息\n",
    "                producer.send(topic, data_bytes)\n",
    "                lines_send += 10\n",
    "                # 计算已发送的百分比\n",
    "                percent_sent = (lines_send / lines_total) * 100                \n",
    "                bytes_sent = len(data_bytes)\n",
    "                print(f\"Sent {bytes_sent} bytes {topic} {percent_sent:.2f}% sent\")\n",
    "                # 每3秒检查一次\n",
    "                time.sleep(3)\n",
    "        # 询问是否继续发送\n",
    "        user_input = input(\"Press 'c' to continue sending the file or 'q' to quit: \")\n",
    "        if user_input == \"q\":\n",
    "            break\n",
    "\n",
    "# 调用函数，将文件发送到Kafka，主题为transaction，服务器地址为localhost:9092\n",
    "send_file_to_kafka(\"./transaction_data_generated.csv\",  \"transaction\", \"localhost:9092\")\n",
    "\n",
    "# 在此代码中，send_file_to_kafka 函数接受三个参数：file_path、topic 和 bootstrap_servers。\n",
    "# file_path是本地文件的路径，topic是数据要发送到的Kafka主题，bootstrap_servers是Kafka集群的地址。\n",
    "# 该函数使用with语句打开文件，读取其内容，并将它们作为流数据发送到指定的Kafka主题。\n",
    "# 发送过程中，打印出发送进度，并使用time.sleep方法暂停3秒来控制发送速率。\n",
    "```\n",
    "\n",
    "## 7.3 数据流显示\n",
    "\n",
    "```Python\n",
    "import os\n",
    "import argparse\n",
    "import csv\n",
    "import re\n",
    "import io\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from pyflink.table import StreamTableEnvironment\n",
    "from pyflink.common import WatermarkStrategy, Encoder, Types\n",
    "from pyflink.datastream import StreamExecutionEnvironment, RuntimeExecutionMode\n",
    "from pyflink.datastream.connectors.file_system import FileSource, StreamFormat, FileSink, OutputFileConfig, RollingPolicy\n",
    "from pyflink.common import Types, SimpleStringSchema\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.datastream.connectors.kafka import FlinkKafkaProducer, FlinkKafkaConsumer\n",
    "\n",
    "##### 定义一个函数parse_csv_old，用于解析csv文件\n",
    "def parse_csv_old(x):\n",
    "    # 使用csv模块的reader函数读取csv文件\n",
    "    result = csv.reader(io.StringIO(x))    \n",
    "    # 返回csv文件的第一行\n",
    "    return next(result)\n",
    "\n",
    "##### 定义一个函数parse_csv，用于解析csv文件\n",
    "def parse_csv(x):\n",
    "    # 将x中的[b'替换为空字符\n",
    "    x = x.replace(\"[b'\", \"\")\n",
    "    # 将x中的\\\\n']替换为空字符\n",
    "    x = x.replace(\"\\\\n']\", \"\")\n",
    "    # 使用csv模块的reader函数读取csv文件\n",
    "    result = csv.reader(io.StringIO(x))\n",
    "    # 返回csv文件的第一行\n",
    "    return next(result)\n",
    "\n",
    "##### 定义一个函数read_from_kafka，用于从Kafka读取数据\n",
    "def read_from_kafka():\n",
    "    # 获取StreamExecutionEnvironment实例\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()    \n",
    "    # 添加flink-sql-connector-kafka-3.1-SNAPSHOT.jar包\n",
    "    env.add_jars(\"file:///home/hadoop/Desktop/PyFlink-Tutorial/flink-sql-connector-kafka-3.1-SNAPSHOT.jar\")\n",
    "    # 打印信息\n",
    "    print(\"start reading data from kafka\")\n",
    "    # 创建一个FlinkKafkaConsumer实例，用于从Kafka读取数据\n",
    "    kafka_consumer = FlinkKafkaConsumer(\n",
    "        topics='transaction', # The topic to consume messages from\n",
    "        deserialization_schema= SimpleStringSchema('UTF-8'), # The schema to deserialize messages\n",
    "        properties={'bootstrap.servers': 'localhost:9092', 'group.id': 'my-group'} # The Kafka broker address and consumer group ID\n",
    "    )\n",
    "    # 从最早的记录开始读取数据\n",
    "    kafka_consumer.set_start_from_earliest()\n",
    "    # 将kafka_consumer添加到StreamExecutionEnvironment中\n",
    "    stream = env.add_source(kafka_consumer)\n",
    "    # 将stream中的每一条数据解析为csv文件\n",
    "    parsed_stream = stream.map(parse_csv)\n",
    "    # 打印解析后的数据\n",
    "    parsed_stream.print()\n",
    "    # 执行StreamExecutionEnvironment\n",
    "    env.execute()\n",
    "\n",
    "##### 调用函数read_from_kafka\n",
    "if __name__ == '__main__':\n",
    "    read_from_kafka()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## 7.4 数据流处理\n",
    "\n",
    "```Python\n",
    "import platform\n",
    "import os\n",
    "import argparse\n",
    "import csv\n",
    "import io\n",
    "import logging\n",
    "import sys\n",
    "from typing import Iterable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from pyflink.common import Types, WatermarkStrategy, Time, Encoder\n",
    "from pyflink.common.watermark_strategy import TimestampAssigner\n",
    "from pyflink.datastream import StreamExecutionEnvironment, ProcessWindowFunction\n",
    "from pyflink.datastream.connectors.file_system import FileSink, OutputFileConfig, RollingPolicy\n",
    "from pyflink.datastream.connectors.kafka import FlinkKafkaProducer, FlinkKafkaConsumer\n",
    "from pyflink.datastream.window import SlidingEventTimeWindows, TimeWindow\n",
    "from pyflink.table import StreamTableEnvironment\n",
    "from pyflink.datastream import StreamExecutionEnvironment, RuntimeExecutionMode\n",
    "from pyflink.datastream.connectors.file_system import FileSource, StreamFormat\n",
    "from pyflink.common import SimpleStringSchema\n",
    "\n",
    "##### 定义一个beep函数，用于发出哔声，根据当前操作系统不同，使用不同的库\n",
    "def beep():\n",
    "    if platform.system() == \"Windows\":\n",
    "        import winsound\n",
    "        winsound.Beep(440, 1000)\n",
    "    elif platform.system() == \"Linux\":\n",
    "        os.system(\"beep\")\n",
    "    else:\n",
    "        print(\"Unsupported platform\")\n",
    "\n",
    "##### 定义一个parse_csv函数，用于解析csv文件，并返回解析后的结果\n",
    "def parse_csv(x):    \n",
    "    x = x.replace(\"[b'\", \"\")\n",
    "    x = x.replace(\"\\n']\", \"\")\n",
    "    x = x.replace(\"\\\\n']\", \"\")\n",
    "    result = csv.reader(io.StringIO(x))\n",
    "    parsed_result = []\n",
    "    for item in result:\n",
    "        parsed_item = []\n",
    "        for element in item:\n",
    "            try:\n",
    "                parsed_element = int(element)\n",
    "            except ValueError:\n",
    "                parsed_element = element\n",
    "            parsed_item.append(parsed_element)\n",
    "        parsed_result.append(parsed_item)\n",
    "    return parsed_result\n",
    "\n",
    "##### 定义一个count_rows函数，用于计算data中行数和类型，并打印出来\n",
    "def count_rows(data):\n",
    "    row_count = len(data)\n",
    "    type_count = type(data)\n",
    "    print(f\"Received {row_count} rows of {type_count} data.\")\n",
    "    return data \n",
    "\n",
    "##### 定义一个check_data函数，用于检查data中第一行的第四个元素是否大于5000，如果大于5000，则发出哔声，并打印出来\n",
    "def check_data(data):\n",
    "    try:\n",
    "        if int(data[0][3]) >= 5000:\n",
    "            beep()\n",
    "            print(f\"data[0][3] is {(data[0][3])}\",f\" Larger than 5000!\\n\")\n",
    "        return int(data[0][3]) >= 5000\n",
    "    except ValueError:\n",
    "        pass  \n",
    "\n",
    "##### 定义一个函数parse_tuple，用于解析元组，参数x为元组\n",
    "def parse_tuple(x):\n",
    "    try:\n",
    "        # 返回元组中的第一个元素，转换为字符串，第二个元素，转换为字符串，第三个元素，转换为整数，第四个元素，转换为整数，第五个元素，转换为字符串，第六个元素，转换为字符串\n",
    "        return (str(x[0][0]), str(x[0][1]), int(x[0][2]), int(x[0][3]), str(x[0][4]), str(x[0][5]))\n",
    "    except ValueError:\n",
    "        # 如果解析失败，打印错误信息\n",
    "        logging.error(f\"Failed to parse tuple: {x}\")\n",
    "        return None\n",
    "\n",
    "##### 定义一个函数read_from_kafka，用于从Kafka中读取数据\n",
    "def read_from_kafka():\n",
    "    # 创建一个参数解析器\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # 添加一个参数，用于指定输出文件路径\n",
    "    parser.add_argument(\n",
    "        '--output',\n",
    "        dest='output',\n",
    "        required=False,\n",
    "        help='Output file to write results to.')\n",
    "    # 获取参数列表\n",
    "    argv = sys.argv[1:]\n",
    "    # 解析参数列表\n",
    "    known_args, _ = parser.parse_known_args(argv)\n",
    "    # 获取输出文件路径\n",
    "    output_path = known_args.output\n",
    "    # 获取Flink运行环境\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()\n",
    "    # 设置并行度为1\n",
    "    env.set_parallelism(1)  \n",
    "    # 添加Kafka连接器\n",
    "    env.add_jars(\"file:///home/hadoop/Desktop/PyFlink-Tutorial/flink-sql-connector-kafka-3.1-SNAPSHOT.jar\")\n",
    "    # 打印信息\n",
    "    print(\"start reading data from kafka\")\n",
    "    # 创建一个FlinkKafkaConsumer，用于从Kafka中读取数据\n",
    "    kafka_consumer = FlinkKafkaConsumer(\n",
    "        # 指定要读取的topic\n",
    "        topics='transaction',\n",
    "        # 指定反序列化方式\n",
    "        deserialization_schema= SimpleStringSchema('UTF-8'), \n",
    "        # 指定Kafka的配置信息\n",
    "        properties={'bootstrap.servers': 'localhost:9092', 'group.id': 'my-group'} \n",
    "    )\n",
    "    # 从最早的记录开始读取\n",
    "    kafka_consumer.set_start_from_earliest()\n",
    "    # 从Kafka中读取数据\n",
    "    stream = env.add_source(kafka_consumer)\n",
    "    # 将读取的数据进行解析\n",
    "    parsed_stream = stream.map(parse_csv)\n",
    "    # 过滤掉不符合条件的数据\n",
    "    data_stream = parsed_stream.filter(check_data)\n",
    "    # 定义输出流\n",
    "    data_stream.print()\n",
    "    env.execute()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    read_from_kafka()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 PyFlink 数据表\n",
    "\n",
    "\n",
    "## 8.1 数据表与Pandas\n",
    "\n",
    "```Python\n",
    "import logging\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyflink.table import (DataTypes, TableEnvironment, EnvironmentSettings)\n",
    "\n",
    "def conversion_from_dataframe():\n",
    "    # 创建一个TableEnvironment对象，并设置为流式处理模式\n",
    "    t_env = TableEnvironment.create(EnvironmentSettings.in_streaming_mode())\n",
    "    # 设置并行度为1\n",
    "    t_env.get_config().set(\"parallelism.default\", \"1\")\n",
    "\n",
    "    # define the source with watermark definition\n",
    "    # 定义源，使用水印定义\n",
    "    pdf = pd.DataFrame(np.random.rand(1000, 2))\n",
    "    table = t_env.from_pandas(\n",
    "        pdf,\n",
    "        schema=DataTypes.ROW([DataTypes.FIELD(\"a\", DataTypes.DOUBLE()),\n",
    "                              DataTypes.FIELD(\"b\", DataTypes.DOUBLE())]))\n",
    "    # 打印出Pandas DataFrame\n",
    "    print(table.to_pandas())\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 设置日志输出格式\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=\"%(message)s\")\n",
    "    # 调用函数\n",
    "    conversion_from_dataframe()\n",
    "```\n",
    "\n",
    "## 8.2 流表混用\n",
    "\n",
    "```Python\n",
    "import logging\n",
    "import sys\n",
    "from pyflink.common import Types\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.table import (DataTypes, TableDescriptor, Schema, StreamTableEnvironment)\n",
    "from pyflink.table.expressions import col\n",
    "from pyflink.table.udf import udf\n",
    "\n",
    "##### 定义一个函数，用于混合使用数据流和表\n",
    "def mixing_use_of_datastream_and_table():\n",
    "    # use StreamTableEnvironment instead of TableEnvironment when mixing use of table & datastream\n",
    "    # 使用StreamTableEnvironment替代TableEnvironment，当混合使用表和数据流时\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()\n",
    "    t_env = StreamTableEnvironment.create(stream_execution_environment=env)\n",
    "\n",
    "    # define the source\n",
    "    # 定义源\n",
    "    t_env.create_temporary_table(\n",
    "        'source',\n",
    "        TableDescriptor.for_connector('datagen')\n",
    "                       .schema(Schema.new_builder()\n",
    "                               .column('id', DataTypes.BIGINT())\n",
    "                               .column('data', DataTypes.STRING())\n",
    "                               .build())\n",
    "                       .option(\"number-of-rows\", \"10\")\n",
    "                       .build())\n",
    "\n",
    "    # define the sink\n",
    "    # 定义汇\n",
    "    t_env.create_temporary_table(\n",
    "        'sink',\n",
    "        TableDescriptor.for_connector('print')\n",
    "                       .schema(Schema.new_builder()\n",
    "                               .column('a', DataTypes.BIGINT())\n",
    "                               .build())\n",
    "                       .build())\n",
    "\n",
    "    # define a user-defined function\n",
    "    # 定义用户自定义函数\n",
    "    @udf(result_type=DataTypes.BIGINT())\n",
    "    def length(data):\n",
    "        return len(data)\n",
    "\n",
    "    # perform table api operations\n",
    "    # 执行表API操作\n",
    "    table = t_env.from_path(\"source\")\n",
    "    table = table.select(col('id'), length(col('data')))\n",
    "\n",
    "    # convert table to datastream and perform datastream api operations\n",
    "    # 将表转换为数据流，并执行数据流API操作\n",
    "    ds = t_env.to_data_stream(table)\n",
    "    ds = ds.map(lambda i: i[0] + i[1], output_type=Types.LONG())\n",
    "\n",
    "    # convert datastream to table and perform table api operations as you want\n",
    "    # 将数据流转换为表，并执行表API操作\n",
    "    table = t_env.from_data_stream(\n",
    "        ds,\n",
    "        Schema.new_builder().column(\"f0\", DataTypes.BIGINT()).build())\n",
    "\n",
    "    # execute\n",
    "    # 执行\n",
    "    table.execute_insert('sink') \\\n",
    "         .wait()\n",
    "    # remove .wait if submitting to a remote cluster, refer to\n",
    "    # 移除.wait，如果提交到远程集群，请参考\n",
    "    # https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/python/faq/#wait-for-jobs-to-finish-when-executing-jobs-in-mini-cluster\n",
    "    # for more details\n",
    "    # 获取更多详情\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=\"%(message)s\")\n",
    "    mixing_use_of_datastream_and_table()\n",
    "```\n",
    "\n",
    "\n",
    "## 8.3 窗口\n",
    "\n",
    "PyFlink 1.18支持以下四种窗口类型:\n",
    "\n",
    "1. **Tumbling Window**: 连续的、不重叠的固定长度窗口。例如，大小为5分钟的滚动窗口将元素分组为5分钟的间隔。\n",
    "\n",
    "```Python\n",
    "import logging\n",
    "import sys\n",
    "from pyflink.common.time import Instant\n",
    "from pyflink.common import Types\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.table import (DataTypes, TableDescriptor, Schema, StreamTableEnvironment)\n",
    "from pyflink.table.expressions import lit, col\n",
    "from pyflink.table.window import Tumble\n",
    "\n",
    "def tumble_window_demo():\n",
    "    # 获取当前的StreamExecutionEnvironment\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()\n",
    "    # 设置并行度为1\n",
    "    env.set_parallelism(1)\n",
    "    # 创建StreamTableEnvironment\n",
    "    t_env = StreamTableEnvironment.create(stream_execution_environment=env)\n",
    "    # 定义源，并设置watermark\n",
    "    ds = env.from_collection(\n",
    "        collection=[\n",
    "            (Instant.of_epoch_milli(1000), 'Alice', 110.1),\n",
    "            (Instant.of_epoch_milli(4000), 'Bob', 30.2),\n",
    "            (Instant.of_epoch_milli(3000), 'Alice', 20.0),\n",
    "            (Instant.of_epoch_milli(2000), 'Bob', 53.1),\n",
    "            (Instant.of_epoch_milli(5000), 'Alice', 13.1),\n",
    "            (Instant.of_epoch_milli(3000), 'Bob', 3.1),\n",
    "            (Instant.of_epoch_milli(7000), 'Bob', 16.1),\n",
    "            (Instant.of_epoch_milli(10000), 'Alice', 20.1)\n",
    "        ],\n",
    "        type_info=Types.ROW([Types.INSTANT(), Types.STRING(), Types.FLOAT()]))\n",
    "\n",
    "    # 将源转换为Table\n",
    "    table = t_env.from_data_stream(\n",
    "        ds,\n",
    "        Schema.new_builder()\n",
    "              .column_by_expression(\"ts\", \"CAST(f0 AS TIMESTAMP(3))\")\n",
    "              .column(\"f1\", DataTypes.STRING())\n",
    "              .column(\"f2\", DataTypes.FLOAT())\n",
    "              .watermark(\"ts\", \"ts - INTERVAL '3' SECOND\")\n",
    "              .build()\n",
    "    ).alias(\"ts\", \"name\", \"price\")\n",
    "\n",
    "    # define the sink\n",
    "    # 定义sink\n",
    "    t_env.create_temporary_table(\n",
    "        'sink',\n",
    "        TableDescriptor.for_connector('print')\n",
    "                       .schema(Schema.new_builder()\n",
    "                               .column('name', DataTypes.STRING())\n",
    "                               .column('total_price', DataTypes.FLOAT())\n",
    "                               .column('w_start', DataTypes.TIMESTAMP_LTZ())\n",
    "                               .column('w_end', DataTypes.TIMESTAMP_LTZ())\n",
    "                               .build())\n",
    "                       .build())\n",
    "\n",
    "    # 定义 tumble window 操作\n",
    "    table = table.window(Tumble.over(lit(5).seconds).on(col(\"ts\")).alias(\"w\")) \\\n",
    "                 .group_by(col('name'), col('w')) \\\n",
    "                 .select(col('name'), col('price').sum, col(\"w\").start, col(\"w\").end)\n",
    "\n",
    "    # 提交执行\n",
    "    table.execute_insert('sink') \\\n",
    "         .wait()\n",
    "    # 移除.wait() 如果是提交到远程集群，参考https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/python/faq/#wait-for-jobs-to-finish-when-executing-jobs-in-mini-cluster\n",
    "    # 获取更多详情\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 设置日志输出格式\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=\"%(message)s\")\n",
    "    tumble_window_demo()\n",
    "```\n",
    "\n",
    "2. **Sliding Window**: 固定大小并按指定滑动间隔滑动的窗口。如果滑动间隔小于窗口大小，则滑动窗口会重叠。例如，大小为15分钟，滑动间隔为5分钟的滑动窗口将元素分组为15分钟，并每5分钟计算一次。每个元素包含在三个连续的窗口评估中。\n",
    "\n",
    "```Python\n",
    "import logging\n",
    "import sys\n",
    "from pyflink.common.time import Instant\n",
    "from pyflink.common import Types\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.table import (DataTypes, TableDescriptor, Schema, StreamTableEnvironment)\n",
    "from pyflink.table.expressions import lit, col\n",
    "from pyflink.table.window import Slide\n",
    "\n",
    "# 定义滑动窗口示例函数\n",
    "def sliding_window_demo():\n",
    "    # 获取流执行环境\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()\n",
    "    # 设置并行度为1\n",
    "    env.set_parallelism(1)\n",
    "    # 创建流表环境\n",
    "    t_env = StreamTableEnvironment.create(stream_execution_environment=env)\n",
    "    # 定义源，添加水位定义\n",
    "    ds = env.from_collection(\n",
    "        collection=[\n",
    "            (Instant.of_epoch_milli(1000), 'Alice', 110.1),\n",
    "            (Instant.of_epoch_milli(4000), 'Bob', 30.2),\n",
    "            (Instant.of_epoch_milli(3000), 'Alice', 20.0),\n",
    "            (Instant.of_epoch_milli(2000), 'Bob', 53.1),\n",
    "            (Instant.of_epoch_milli(5000), 'Alice', 13.1),\n",
    "            (Instant.of_epoch_milli(3000), 'Bob', 3.1),\n",
    "            (Instant.of_epoch_milli(7000), 'Bob', 16.1),\n",
    "            (Instant.of_epoch_milli(10000), 'Alice', 20.1)\n",
    "        ],\n",
    "        type_info=Types.ROW([Types.INSTANT(), Types.STRING(), Types.FLOAT()]))\n",
    "\n",
    "    # 从数据流中创建表\n",
    "    table = t_env.from_data_stream(\n",
    "        ds,\n",
    "        Schema.new_builder()\n",
    "              .column_by_expression(\"ts\", \"CAST(f0 AS TIMESTAMP(3))\")\n",
    "              .column(\"f1\", DataTypes.STRING())\n",
    "              .column(\"f2\", DataTypes.FLOAT())\n",
    "              .watermark(\"ts\", \"ts - INTERVAL '3' SECOND\")\n",
    "              .build()\n",
    "    ).alias(\"ts\", \"name\", \"price\")\n",
    "\n",
    "    # define the sink\n",
    "    # 定义输出表\n",
    "    t_env.create_temporary_table(\n",
    "        'sink',\n",
    "        TableDescriptor.for_connector('print')\n",
    "                       .schema(Schema.new_builder()\n",
    "                               .column('name', DataTypes.STRING())\n",
    "                               .column('total_price', DataTypes.FLOAT())\n",
    "                               .column('w_start', DataTypes.TIMESTAMP_LTZ())\n",
    "                               .column('w_end', DataTypes.TIMESTAMP_LTZ())\n",
    "                               .build())\n",
    "                       .build())\n",
    "\n",
    "    # define the sliding window operation\n",
    "    # 定义滑动窗口操作\n",
    "    table = table.window(Slide.over(lit(5).seconds).every(lit(2).seconds).on(col(\"ts\")).alias(\"w\"))\\\n",
    "                 .group_by(col('name'), col('w')) \\\n",
    "                 .select(col('name'), col('price').sum, col(\"w\").start, col(\"w\").end)\n",
    "    # 提交执行\n",
    "    table.execute_insert('sink') \\\n",
    "         .wait()\n",
    "    # 移除.wait()，如果提交到远程集群，参考https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/python/faq/#wait-for-jobs-to-finish-when-executing-jobs-in-mini-cluster\n",
    "    # 获取更多详情\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 设置日志输出流为标准输出流，日志级别为INFO，格式为%(message)s\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=\"%(message)s\")\n",
    "    # 调用滑动窗口示例函数\n",
    "    sliding_window_demo()\n",
    "```\n",
    "\n",
    "\n",
    "3. **Session Window**: 会话窗口的边界由不活动的时间间隔定义，即，如果在定义的间隔期内没有事件出现，则会话窗口关闭。\n",
    "\n",
    "```Python\n",
    "import logging\n",
    "import sys\n",
    "from pyflink.common.time import Instant\n",
    "from pyflink.common import Types\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.table import (DataTypes, TableDescriptor, Schema, StreamTableEnvironment)\n",
    "from pyflink.table.expressions import lit, col\n",
    "from pyflink.table.window import Session\n",
    "\n",
    "# 定义一个session_window_demo函数\n",
    "def session_window_demo():\n",
    "    # 获取当前的StreamExecutionEnvironment实例\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()\n",
    "    # 设置并行度为1\n",
    "    env.set_parallelism(1)\n",
    "    # 创建一个StreamTableEnvironment实例\n",
    "    t_env = StreamTableEnvironment.create(stream_execution_environment=env)\n",
    "    # 定义一个数据源，并设置watermark定义\n",
    "    ds = env.from_collection(\n",
    "        collection=[\n",
    "            (Instant.of_epoch_milli(1000), 'Alice', 110.1),\n",
    "            (Instant.of_epoch_milli(4000), 'Bob', 30.2),\n",
    "            (Instant.of_epoch_milli(3000), 'Alice', 20.0),\n",
    "            (Instant.of_epoch_milli(2000), 'Bob', 53.1),\n",
    "            (Instant.of_epoch_milli(8000), 'Bob', 16.1),\n",
    "            (Instant.of_epoch_milli(10000), 'Alice', 20.1)\n",
    "        ],\n",
    "        type_info=Types.ROW([Types.INSTANT(), Types.STRING(), Types.FLOAT()]))\n",
    "\n",
    "    # 将数据源转换为StreamTable\n",
    "    table = t_env.from_data_stream(\n",
    "        ds,\n",
    "        Schema.new_builder()\n",
    "              .column_by_expression(\"ts\", \"CAST(f0 AS TIMESTAMP(3))\")\n",
    "              .column(\"f1\", DataTypes.STRING())\n",
    "              .column(\"f2\", DataTypes.FLOAT())\n",
    "              .watermark(\"ts\", \"ts - INTERVAL '3' SECOND\")\n",
    "              .build()\n",
    "    ).alias(\"ts\", \"name\", \"price\")\n",
    "\n",
    "    # define the sink\n",
    "    # 定义一个临时表，用于存储结果\n",
    "    t_env.create_temporary_table(\n",
    "        'sink',\n",
    "        TableDescriptor.for_connector('print')\n",
    "                       .schema(Schema.new_builder()\n",
    "                               .column('name', DataTypes.STRING())\n",
    "                               .column('total_price', DataTypes.FLOAT())\n",
    "                               .column('w_start', DataTypes.TIMESTAMP_LTZ())\n",
    "                               .column('w_end', DataTypes.TIMESTAMP_LTZ())\n",
    "                               .build())\n",
    "                       .build())\n",
    "    # 定义一个session window操作 这里的 with_gap(lit(5).seconds) 就是控制session窗口的大小\n",
    "    table = table.window(Session.with_gap(lit(5).seconds).on(col(\"ts\")).alias(\"w\")) \\\n",
    "                 .group_by(col('name'), col('w')) \\\n",
    "                 .select(col('name'), col('price').sum, col(\"w\").start, col(\"w\").end)\n",
    "\n",
    "    # 提交执行\n",
    "    table.execute_insert('sink') \\\n",
    "         .wait()\n",
    "    # 移除.wait()，如果提交到远程集群，参考https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/python/faq/#wait-for-jobs-to-finish-when-executing-jobs-in-mini-cluster\n",
    "    # 获取更多详情\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 设置日志输出格式\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=\"%(message)s\")\n",
    "    # 调用session_window_demo函数\n",
    "    session_window_demo()\n",
    "```\n",
    "\n",
    "\n",
    "4. **Over Window**: 类似于SQL，Over窗口聚合计算相邻行的范围内的聚合。\n",
    "\n",
    "```Python\n",
    "import logging\n",
    "import sys\n",
    "from pyflink.common.time import Instant\n",
    "from pyflink.common import Types\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.table import (DataTypes, TableDescriptor, Schema, StreamTableEnvironment)\n",
    "from pyflink.table.expressions import col, row_interval, CURRENT_ROW\n",
    "from pyflink.table.window import Over\n",
    "\n",
    "# 定义一个函数tumble_window_demo，用于演示全局窗口操作\n",
    "def tumble_window_demo():\n",
    "    # 获取当前的StreamExecutionEnvironment，并设置并行度为1\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()\n",
    "    env.set_parallelism(1)\n",
    "    # 创建一个StreamTableEnvironment\n",
    "    t_env = StreamTableEnvironment.create(stream_execution_environment=env)\n",
    "    # 定义一个数据源，并设置watermark\n",
    "    ds = env.from_collection(\n",
    "        collection=[\n",
    "            (Instant.of_epoch_milli(1000), 'Alice', 110.1),\n",
    "            (Instant.of_epoch_milli(4000), 'Bob', 30.2),\n",
    "            (Instant.of_epoch_milli(3000), 'Alice', 20.0),\n",
    "            (Instant.of_epoch_milli(2000), 'Bob', 53.1),\n",
    "            (Instant.of_epoch_milli(5000), 'Alice', 13.1),\n",
    "            (Instant.of_epoch_milli(3000), 'Bob', 3.1),\n",
    "            (Instant.of_epoch_milli(7000), 'Bob', 16.1),\n",
    "            (Instant.of_epoch_milli(10000), 'Alice', 20.1)\n",
    "        ],\n",
    "        type_info=Types.ROW([Types.INSTANT(), Types.STRING(), Types.FLOAT()]))\n",
    "\n",
    "    # 将数据源转换为Table，并设置watermark\n",
    "    table = t_env.from_data_stream(\n",
    "        ds,\n",
    "        Schema.new_builder()\n",
    "              .column_by_expression(\"ts\", \"CAST(f0 AS TIMESTAMP(3))\")\n",
    "              .column(\"f1\", DataTypes.STRING())\n",
    "              .column(\"f2\", DataTypes.FLOAT())\n",
    "              .watermark(\"ts\", \"ts - INTERVAL '3' SECOND\")\n",
    "              .build()\n",
    "    ).alias(\"ts\", \"name\", \"price\")\n",
    "\n",
    "    # 定义一个sink，用于输出结果\n",
    "    t_env.create_temporary_table(\n",
    "        'sink',\n",
    "        TableDescriptor.for_connector('print')\n",
    "                       .schema(Schema.new_builder()\n",
    "                               .column('name', DataTypes.STRING())\n",
    "                               .column('total_price', DataTypes.FLOAT())\n",
    "                               .build())\n",
    "                       .build())\n",
    "\n",
    "    # 定义一个全局窗口操作，并计算每个名字的总价格\n",
    "    table = table.over_window(\n",
    "        Over.partition_by(col(\"name\"))\n",
    "            .order_by(col(\"ts\"))\n",
    "            .preceding(row_interval(2))\n",
    "            .following(CURRENT_ROW)\n",
    "            .alias('w')) \\\n",
    "        .select(col('name'), col('price').max.over(col('w')))\n",
    "\n",
    "    # 提交执行，并等待执行完成\n",
    "    table.execute_insert('sink') \\\n",
    "         .wait()\n",
    "    # 移除.wait()，如果提交到远程集群，请参考https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/python/faq/#wait-for-jobs-to-finish-when-executing-jobs-in-mini-cluster\n",
    "    # 获取更多详情\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 设置日志输出格式\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=\"%(message)s\")\n",
    "    # 调用tumble_window_demo函数\n",
    "    tumble_window_demo()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 建筑变形数据监测\n",
    "\n",
    "只做简单筛选咱们试过了。\n",
    "接下来是是稍微复杂的，对建筑变形数据监测。\n",
    "\n",
    "```Python\n",
    "#以下代码使用kafka-python模块将数据发送到本地Kafka集群。\n",
    "#此代码打开一个名为“building_data.csv”的文本文件，并将其内容作为流发送到指定的 Kafka 主题“building”：\n",
    "from kafka import KafkaProducer\n",
    "import time\n",
    "import os\n",
    "import chardet\n",
    "\n",
    "# 定义一个函数，用于将文件发送到Kafka，参数为文件路径、主题和Kafka服务器地址\n",
    "def send_file_to_kafka(file_path: str, topic: str, bootstrap_servers: str):\n",
    "    # 创建一个KafkaProducer对象，用于发送消息\n",
    "    producer = KafkaProducer(bootstrap_servers=bootstrap_servers)\n",
    "    # 获取文件大小\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    # 检测文件编码\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        result = chardet.detect(f.read())\n",
    "        encoding = result[\"encoding\"]\n",
    "    # 读取文件内容\n",
    "    with open(file_path, \"r\", encoding=encoding) as f:\n",
    "        lines_total = len(f.readlines())\n",
    "    lines_send = 0\n",
    "    # 循环发送文件内容\n",
    "    while True:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            while True:\n",
    "                data = f.readlines(10)\n",
    "                if not data:\n",
    "                    break\n",
    "                data_str = str(data)\n",
    "                data_bytes = data_str.encode()\n",
    "                # 发送消息\n",
    "                producer.send(topic, data_bytes)\n",
    "                lines_send += 10\n",
    "                # 计算已发送的百分比\n",
    "                percent_sent = (lines_send / lines_total) * 100                \n",
    "                bytes_sent = len(data_bytes)\n",
    "                print(f\"Sent {bytes_sent} bytes {topic} {percent_sent:.2f}% sent\")\n",
    "                # 每3秒检查一次\n",
    "                time.sleep(3)\n",
    "        # 询问是否继续发送\n",
    "        user_input = input(\"Press 'c' to continue sending the file or 'q' to quit: \")\n",
    "        if user_input == \"q\":\n",
    "            break\n",
    "\n",
    "# 调用函数，将文件发送到Kafka，主题为building，服务器地址为localhost:9092\n",
    "send_file_to_kafka(\"./building_data.csv\",  \"building\", \"localhost:9092\")\n",
    "# 在此代码中，send_file_to_kafka 函数接受三个参数：file_path、topic 和 bootstrap_servers。\n",
    "# file_path是本地文件的路径，topic是数据要发送到的Kafka主题，bootstrap_servers是Kafka集群的地址。\n",
    "# 该函数使用with语句打开文件，读取其内容，并将它们作为流数据发送到指定的Kafka主题。\n",
    "# 发送过程中，打印出发送进度，并使用time.sleep方法暂停0.1秒来控制发送速率。\n",
    "```\n",
    "\n",
    "```Python\n",
    "import platform\n",
    "import os\n",
    "import argparse\n",
    "import csv\n",
    "import io\n",
    "import logging\n",
    "import sys\n",
    "from typing import Iterable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from pyflink.common import Types, WatermarkStrategy, Time, Encoder\n",
    "from pyflink.common.watermark_strategy import TimestampAssigner\n",
    "from pyflink.datastream import StreamExecutionEnvironment, ProcessWindowFunction\n",
    "from pyflink.datastream.connectors.file_system import FileSink, OutputFileConfig, RollingPolicy\n",
    "from pyflink.datastream.connectors.kafka import FlinkKafkaProducer, FlinkKafkaConsumer\n",
    "from pyflink.datastream.window import SlidingEventTimeWindows, TimeWindow\n",
    "from pyflink.table import StreamTableEnvironment\n",
    "from pyflink.datastream import StreamExecutionEnvironment, RuntimeExecutionMode\n",
    "from pyflink.datastream.connectors.file_system import FileSource, StreamFormat\n",
    "from pyflink.common import SimpleStringSchema\n",
    "\n",
    "# 定义一个函数parse_csv_old，用于解析csv文件\n",
    "def parse_csv_old(x):\n",
    "    # 使用csv模块的reader函数读取csv文件\n",
    "    result = csv.reader(io.StringIO(x))    \n",
    "    # 返回csv文件的第一行\n",
    "    return next(result)\n",
    "\n",
    "# 定义一个函数parse_csv，用于解析csv文件\n",
    "def parse_csv(x):\n",
    "    # 将x中的[b'替换为空字符\n",
    "    x = x.replace(\"[b'\", \"\")\n",
    "    # 将x中的\\\\n']替换为空字符\n",
    "    x = x.replace(\"\\\\n']\", \"\")\n",
    "    # 使用csv模块的reader函数读取csv文件\n",
    "    result = csv.reader(io.StringIO(x))\n",
    "    # 返回csv文件的第一行\n",
    "    return next(result)\n",
    "\n",
    "# 定义一个函数，用于计算传入数据的行数\n",
    "def count_rows(data):\n",
    "    # 计算传入数据的行数\n",
    "    row_count = len(data)\n",
    "    # 计算传入数据的类型\n",
    "    type_count = type(data)\n",
    "    # 打印出传入数据的行数和类型\n",
    "    print(f\"Received {row_count} rows of {type_count} data.\")\n",
    "    # 返回传入数据\n",
    "    return data \n",
    "\n",
    "# 定义一个函数，用于解析元组\n",
    "def parse_tuple(x):\n",
    "    \n",
    "    # 打印出传入数据的第一个元素的类型、第二个元素的类型和第一个元素的长度\n",
    "    print(f\"x[0] type is {type(x[0])}\",f\"x[0][1] type is {type(x[0][1])}\",f\"x[0] len is {len(x[0])}\")\n",
    "    try:\n",
    "        # 尝试使用datetime.strptime函数将传入数据的第一个元素转换为时间戳，并将其第二个元素转换为float类型\n",
    "        return (datetime.strptime(str(x[0][0]), \"%Y-%m-%d %H:%M:%S\").timestamp(), float(x[0][1]))\n",
    "    except ValueError:\n",
    "        # 如果转换失败，则打印出传入数据的值，并返回None\n",
    "        logging.error(f\"Failed to parse tuple: {x}\")\n",
    "        return None\n",
    "\n",
    "# 定义一个函数read_from_kafka，用于从Kafka读取数据\n",
    "def read_from_kafka():\n",
    "    # 获取StreamExecutionEnvironment实例\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()    \n",
    "    # 添加flink-sql-connector-kafka-3.1-SNAPSHOT.jar包\n",
    "    env.add_jars(\"file:///home/hadoop/Desktop/PyFlink-Tutorial/flink-sql-connector-kafka-3.1-SNAPSHOT.jar\")\n",
    "    # 打印信息\n",
    "    print(\"start reading data from kafka\")\n",
    "    # 创建一个FlinkKafkaConsumer实例，用于从Kafka读取数据\n",
    "    kafka_consumer = FlinkKafkaConsumer(\n",
    "        topics='building', # The topic to consume messages from\n",
    "        deserialization_schema= SimpleStringSchema('UTF-8'), # The schema to deserialize messages\n",
    "        properties={'bootstrap.servers': 'localhost:9092', 'group.id': 'my-group'} # The Kafka broker address and consumer group ID\n",
    "    )\n",
    "    # 从最早的记录开始读取数据\n",
    "    kafka_consumer.set_start_from_earliest()\n",
    "    # 将kafka_consumer添加到StreamExecutionEnvironment中\n",
    "    stream = env.add_source(kafka_consumer)\n",
    "    # 将stream中的每一条数据解析为csv文件\n",
    "    parsed_stream = stream.map(parse_csv)\n",
    "    # 打印解析后的数据\n",
    "    parsed_stream.print()\n",
    "    # 执行StreamExecutionEnvironment\n",
    "    env.execute()\n",
    "\n",
    "# 调用函数read_from_kafka\n",
    "if __name__ == '__main__':\n",
    "    read_from_kafka()\n",
    "```\n",
    "\n",
    "\n",
    "```Python\n",
    "import platform\n",
    "import os\n",
    "import argparse\n",
    "import csv\n",
    "import io\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Iterable\n",
    "from datetime import datetime\n",
    "from pyflink.common import Types, WatermarkStrategy, Time, Encoder\n",
    "from pyflink.common.watermark_strategy import TimestampAssigner\n",
    "from pyflink.datastream import StreamExecutionEnvironment, ProcessWindowFunction\n",
    "from pyflink.datastream.connectors.file_system import FileSink, OutputFileConfig, RollingPolicy\n",
    "from pyflink.datastream.connectors.kafka import FlinkKafkaProducer, FlinkKafkaConsumer\n",
    "from pyflink.datastream.window import SlidingEventTimeWindows, TimeWindow\n",
    "from pyflink.table import StreamTableEnvironment\n",
    "from pyflink.datastream import StreamExecutionEnvironment, RuntimeExecutionMode\n",
    "from pyflink.datastream.connectors.file_system import FileSource, StreamFormat\n",
    "from pyflink.common import SimpleStringSchema\n",
    "\n",
    "# 定义一个beep函数，根据不同的操作系统，播放不同的声音\n",
    "def beep():\n",
    "    # 如果是Windows系统\n",
    "    if platform.system() == \"Windows\":\n",
    "        # 导入winsound模块\n",
    "        import winsound\n",
    "        # 播放440Hz的音调，持续1000ms\n",
    "        winsound.Beep(440, 1000)\n",
    "    # 如果是Linux系统\n",
    "    elif platform.system() == \"Linux\":\n",
    "        # 播放beep命令\n",
    "        os.system(\"beep\")\n",
    "    # 如果是其他系统\n",
    "    else:\n",
    "        # 打印不支持的平台\n",
    "        print(\"Unsupported platform\")\n",
    "\n",
    "# 定义一个parse_csv函数，用于解析csv文件\n",
    "def parse_csv(x):    \n",
    "    # 将x中的[b'替换为空\n",
    "    x = x.replace(\"[b'\", \"\")\n",
    "    # 将x中的\\n']替换为空\n",
    "    x = x.replace(\"\\n']\", \"\")\n",
    "    # 将x中的\\\\n']替换为空\n",
    "    x = x.replace(\"\\\\n']\", \"\")\n",
    "    # 将x中的\\r']替换为空\n",
    "    x = x.replace(\"\\r\", \"\")\n",
    "    # 将x中的\\\\r']替换为空\n",
    "    x = x.replace(\"\\\\r\", \"\")\n",
    "    # 将x转换为csv格式\n",
    "    result = csv.reader(io.StringIO(x))\n",
    "    # 创建一个空列表，用于存放解析后的结果\n",
    "    parsed_result = []\n",
    "    # 遍历result中的每一项\n",
    "    for item in result:\n",
    "        # 创建一个空列表，用于存放解析后的每一项\n",
    "        parsed_item = []\n",
    "        # 遍历item中的每一项\n",
    "        for element in item:\n",
    "            # 尝试将element转换为整数\n",
    "            try:\n",
    "                parsed_element = int(element)\n",
    "            # 如果转换失败，则将element的值赋给parsed_element\n",
    "            except ValueError:\n",
    "                parsed_element = element\n",
    "            # 将parsed_element添加到parsed_item中\n",
    "            parsed_item.append(parsed_element)\n",
    "        # 将parsed_item添加到parsed_result中\n",
    "        parsed_result.append(parsed_item)\n",
    "    # 返回解析后的结果\n",
    "    return parsed_result\n",
    "\n",
    "# 定义一个count_rows函数，用于计算data中行数\n",
    "def count_rows(data):\n",
    "    # 计算data中行数\n",
    "    row_count = len(data)\n",
    "    # 获取data的类型\n",
    "    type_count = type(data)\n",
    "    # 打印data中行数和类型\n",
    "    print(f\"Received {row_count} rows of {type_count} data.\")\n",
    "    # 返回data\n",
    "    return data \n",
    "\n",
    "# 定义一个check_data函数，用于检查data中每一行的数据\n",
    "def check_data(data):\n",
    "    # 检查data中第一行的数据是否大于0.5\n",
    "    if abs(float(data[0][1])) >= 0.5:\n",
    "        # 如果大于0.5，则播放beep函数\n",
    "        beep()\n",
    "        # 打印data中第一行的数据和ABS值\n",
    "        # print(f\"data at {data[0][0]} is {(data[0][1])}\",f\" ABS Larger than 0.5!\\n\")\n",
    "    # 返回data\n",
    "    return abs(float(data[0][1])) >= 0.5\n",
    "\n",
    "# 定义一个read_from_kafka函数，用于从kafka中读取数据\n",
    "def read_from_kafka():\n",
    "    # 获取StreamExecutionEnvironment实例\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()\n",
    "    # 添加jars\n",
    "    env.add_jars(\"file:///home/hadoop/Desktop/PyFlink-Tutorial/flink-sql-connector-kafka-3.1-SNAPSHOT.jar\")\n",
    "    # 打印提示信息\n",
    "    print(\"start reading data from kafka\")\n",
    "    # 创建一个FlinkKafkaConsumer实例，用于从kafka中读取数据\n",
    "    kafka_consumer = FlinkKafkaConsumer(\n",
    "        topics='building',\n",
    "        deserialization_schema= SimpleStringSchema('UTF-8'), \n",
    "        properties={'bootstrap.servers': 'localhost:9092', 'group.id': 'my-group'} \n",
    "    )\n",
    "        \n",
    "    # 将kafka_consumer添加到StreamExecutionEnvironment中\n",
    "    stream = env.add_source(kafka_consumer)\n",
    "    # 将stream中的每一行数据转换为csv格式\n",
    "    parsed_stream = stream.map(parse_csv)\n",
    "\n",
    "    # 将parsed_stream中的每一行数据传入check_data函数，检查数据是否符合要求\n",
    "    data_stream = parsed_stream.filter(check_data)\n",
    "\n",
    "    # 将data_stream中的数据打印到标准输出中\n",
    "    print(\"Printing result to stdout.\")\n",
    "    data_stream.print()\n",
    "\n",
    "    # 执行StreamExecutionEnvironment\n",
    "    env.execute()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 调用read_from_kafka函数，从kafka中读取数据\n",
    "    read_from_kafka()\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
