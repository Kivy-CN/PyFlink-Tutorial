{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 基于Python的流数据处理技术简介\n",
    "\n",
    "## 1.1 课程定位和设计思路\n",
    "\n",
    "这学期是我们这门课的第一次开课。\n",
    "最初，我们想通过一个传统的编程语言Scala来实现流数据处理框架，并带领大家完成几个样例。\n",
    "但后来我们发现这不太妥当。古人说得好，授人以鱼不如授人以渔。\n",
    "如果我们只是把这个框架的使用方法告诉你，那么这样的教学意义不大。\n",
    "因此，我们决定向大家展示这种框架的来龙去脉，包括流数据处理的概念、目的等等。\n",
    "\n",
    "此外，我们不再引入其他编程语言。一方面，这会干扰基础思想的理解，而我们注重的是思想而非技术。\n",
    "另一方面，这些框架的发展速度很快。前几个版本可能支持Scala，但后几个版本可能就不支持了。\n",
    "有些函数和方法在某些版本上是实现的，但在后续版本中可能会被修改。\n",
    "在这种情况下，教大家记忆某个编程语言中的某个东西是否可修改，另一个是否不可修改，是没有意义的。\n",
    "\n",
    "例如，有一段代码最初使用for循环来实现，但速度很慢。后来改成列表推导式，速度提升了。\n",
    "但是，随着编程语言的发展和编译环境的改进，for循环有时可能比列表推导式更快。\n",
    "因此，如果考试要求判断哪个更快，就像背古诗词一样，这样的考试就没有意义，也不好玩。\n",
    "\n",
    "在我们这门课中，我们强调的是一个框架性的认知。\n",
    "就是关于这个流数据的整个过程，从他的基本概念思路，再到产生发送接收和处理。\n",
    "这个过程会有很多环节，需要大家动手去探索。有可能你们之前学过Linux操作系统，也学过虚拟机，也学过Python等等。\n",
    "之前学的那些基础知识，在我们这一面这种层次的课程就要开始应用了。\n",
    "\n",
    "本来，我已经为大家准备好了一个几乎完整的开发环境。虚拟机里面装好了所有的运行必备的组件以及编译器等等。\n",
    "当然，我会提供这个虚拟机给大家。但还是希望能够带着大家从头开始独立构建。\n",
    "这样能有机会了解每一步的操作，以便在以后的工作中能够独立完成。\n",
    "而不只是通过考试，却没有真正掌握这些知识。\n",
    "\n",
    "如果你以后要从事这方面的相关工作，听了这一门课就够了吗？\n",
    "那绝对不可能。因为任何一门大学的课程，一般都是一个概述。\n",
    "通过这种课程，我们的目的是让你了解这个领域的基础概念、基础思维和基础思路。\n",
    "然后，你需要进一步深入学习，可能去读研究生，专门从事学术领域的相关研究。\n",
    "或者在具体的生产环境中去探索这方面的更深入的开发工作。\n",
    "\n",
    "## 1.2 编程语言选择与代码难度\n",
    "\n",
    "\n",
    "我们选择了Python编程语言，而没有选择Java和Scala。\n",
    "首先是考虑到大家也都学过Python，上手的难度会比较低，对大家来说可能会比较友好。\n",
    "另外是考虑依赖包的安装问题，如果选择Java，有的时候需要用maven去下载一些包，有时候会有网络问题；\n",
    "然后还考虑到开发环境的搭建问题，Java的开发环境的配置相比Python也要复杂一点，工程的内容含量更高一些。\n",
    "本来我们也考虑过引入Scala，后来教研室几位老师反复商量，没有这么做。\n",
    "一方面是觉得引入一门全新的编程语言，对于大家来说还是有挑战。\n",
    "另外一方面是，Scala本身虽然性能更好，但有很大可能要被一些开源项目所放弃。\n",
    "因为针对它的专门维护可能需要一些额外的工作经历，但现在很多开源团队未必能有这么多的额外精力去继续维护它。\n",
    "综上所述，我们选择了对大家都友好，又好上手好配置，不用太担心依赖包安装问题的Python。\n",
    "\n",
    "\n",
    "| **编程语言** | **优势** | **劣势** |\n",
    "|------------|------------|------------|\n",
    "| Java       | - 跨平台- 可扩展、向后兼容、稳定、生产就绪- 支持各种经过试验和测试的库 | - 冗长- 不支持\"Read-Evaluate-Print-Loop（REPL）\" 中文翻译是 **读取-求值-输出-循环** |\n",
    "| Scala      | - 面向对象和函数式编程特性- 高度可扩展 | - 学习曲线较陡峭 |\n",
    "| Python     | - 易于学习、易于使用、易于阅读的语法- 有许多强大的库和框架 | - 解释性语言- 动态类型的语言 |\n",
    "\n",
    "以上是编程语言的选择。\n",
    "对于代码这部分，大家不要担心。\n",
    "\n",
    "首先在代码难度上，这堂课上涉及到的代码还是很友好的。\n",
    "一方面，基本上所有的代码都有做注释，大家自己阅读的时候也能比较容易地理解。\n",
    "另外一方面，授课的过程中，我们会拆解开来，一边演示一边讲解。\n",
    "代码经验不是很丰富的同学，相信是能够跟得上这种详细的讲解的。\n",
    "如果有学有余力的同学，也完全可以自己在深度上更进一步探索。\n",
    "\n",
    "另外在代码规模上，本课程的代码一定不会很长很累人。\n",
    "很多内容的复杂也就是思路和架构上的复杂性看上去吓人。\n",
    "实际上基本概念落实到代码中可能就有几百行代码顶多了。\n",
    "有的甚至就几十行代码就能完成。\n",
    "\n",
    "\n",
    "\n",
    "## 1.3 数据的基础认知\n",
    "\n",
    "### 1.3.1 描述而非定义\n",
    "\n",
    "首先，我们需要明确数据的定义。\n",
    "在这里，我们所说的数据并不是指计算机处理器里面的指令和数据那种狭义场景下的数据。\n",
    "而是一个更广泛意义上的数据。数据是可以被观测的，并且能够用数值记录下来的。\n",
    "这是一个描述，而不是一个严格的定义。\n",
    "虽然对数据的定义可能会有很多的争论，但我们做出的描述总是符合一定的经验的。\n",
    "\n",
    "### 1.3.2 数据描述了人类个体、人类社会、外在世界\n",
    "\n",
    "人的个体成长，作为一个自然过程，就伴随着各项数据的不断变化。身高、体重、体温等等。\n",
    "小朋友，刚出生的时候可能只有几公斤，等长到了几岁了，可能就有十几公斤甚至几十公斤。\n",
    "再长一些之后就有可能有上百公斤，比如我曾经就有100多公斤啊。\n",
    "随着时间发展，人的宏观上身高、体重和各种生理指标可能都会发生变化。\n",
    "而更细节上的体温、血相等等数据的异常波动可能预示了身体健康状况的问题。\n",
    "有的人可能要由于身体健康的原因做长期的数据观测，检测血压、血糖、心率等等。\n",
    "\n",
    "![](./images/head.gif)\n",
    "\n",
    "人的社会群体关系，也是一个充满数据的过程，可以用数据去描述。\n",
    "亲属关系、身份编号、手机号码、就读学校、所学专业等等，这些信息也都在有意或者无意中被数据记录下来。\n",
    "人类的复杂社会行为，比如商品供需关系、股票交易价格、货币汇率等等，也都得量化成数据。\n",
    "\n",
    "![](./images/stocks.gif)\n",
    "\n",
    "人去看待外在的自然世界，也要借助数据。\n",
    "外面天气的冷暖、风速的变化、降雨量的多少、地震的强度、建筑沉降的幅度、湖泊的面积、河水的流量、海水的含盐度等等。\n",
    "人最早对外界事物的系统的描述，大概就是度量衡，在日常生活中用于计量物体长短、容积、轻重。\n",
    "现在咱们也说衡量、度量、平衡等等，就这么来的。\n",
    "甚至在微观领域上，对一个分子或者原子的描述，也要用到数据。\n",
    "\n",
    "![](./images/stocks.gif)\n",
    "\n",
    "\n",
    "\n",
    "## 1.4 数据的形态划分这段话可以这样重构和润色：\n",
    "\n",
    "所有的数据都在不断变化，而传统的观察方式只能截取到一个截面或者累积若干个截面，难以应对不断变化的现实世界。\n",
    "\n",
    "成规模的数据有两种形态。一种是静态的数据，观测了一堆，不管这堆有多大，它是有固定的规模和有限的个数的，不会再进行改变。\n",
    "另一种情况比较复杂，需要一直关注和更新。\n",
    "这种情况有两种可能：\n",
    "一种是这东西是无限的，一直在被生成出来，随时在变化，需要一直观测；\n",
    "另一种可能是这东西规模太大，人力无法短时间内穷尽，即使是有限的，也需要持续观测和处理。这就是原生的流数据。\n",
    "\n",
    "人的观测能力是有限的，因为人作为动物，我们的感官本身是受限的。\n",
    "而且人记录的载体本身也存在物理上的上限。\n",
    "因此，面对一些场景，人力无法全部认识整体，只能一点一点来，这个过程中也会导致数据持续产生。\n",
    "\n",
    "静态的数据和流动的数据未必总是那么好区分。比如窗台边这有的是花岗岩，这些岩石形成的过程涉及到温度、压力和粘度等方面的变化。\n",
    "这些变化有的是物理变化，有的是化学变化，这个过程有的可能要花费大概一百万年。\n",
    "人现在认识这东西，都是测试平均的这么个成分上的，变成了一个静态的数据，但其实它的形成过程是一个动态的过程。\n",
    "\n",
    "具体的个体，因为它自身状态可能会随着时间发生不断的变化，可能会产生流动的数据。\n",
    "有限的群体，因为他们彼此之间的关系可能会发生不断的变化，也可能会产生流动的数据。\n",
    "如果是一段有限的数据集合，但它在时间或者空间上可以有不同的排布和次序，也能变成流动的数据。\n",
    "\n",
    "\n",
    "\n",
    "## 1.5 流数据的具体特征\n",
    "\n",
    "专门提出来这个流数据的分析和处理，就是因为有一些场景下需要这类思路解决对应的问题。\n",
    "而用传统的那种静态数据的处理思路，可能有一些解决不了的问题。\n",
    "流数据处理技术是基于现实需求而产生。\n",
    "\n",
    "事物是时刻不停的发展变化的，万事万物都非静态，运动是无处不在的。\n",
    "自然界中咱们所观测到的一切数据实际上都是原生的流数据，因为这些东西往往存在着各种的变化。\n",
    "只是变化的幅度可能会有差异，但这种变化是不停的。\n",
    "\n",
    "流数据具有如下特征：\n",
    "* 数据快速持续到达，潜在大小未知\n",
    "* 数据来源众多，格式复杂，类型多样\n",
    "* 数据量大，但不关注存储，经过处理后丢弃或归档存储\n",
    "* 注重数据的整体价值，不过分关注个别数据\n",
    "* 数据次序错乱，可能顺序颠倒或不完整，系统无法预测将处理的新到达数据元素的顺序\n",
    "\n",
    "#### 思考题 1 什么样的数据是静态的数据，什么样的数据是流数据？\n",
    "用自己的话通顺的表达出来，并且举一些例子。\n",
    "\n",
    "\n",
    "## 1.6 批计算与流计算\n",
    "\n",
    "简单来说，对静态数据就凑够一批来进行批计算，对于流数据就随着数据流进行流计算。\n",
    "这说的计算可以简单理解成就是加减乘除，甚至就是加法，当然还有逻辑运算。\n",
    "再说复杂点呢，就有映射、筛选、键值选择等等，这些后面再细说。\n",
    "\n",
    "批计算就是凑够一批，然后再算。\n",
    "流计算就是一边流着，一边计算。\n",
    "很直观的命名对不对？\n",
    "\n",
    "传统的批计算能不能解决上面的一些场景呢？其实也能。\n",
    "比如spark实际上就是用批计算，把流数据切成一小段一小段，然后每一段来进行处理。\n",
    "反过来流计算能去应对传统的批计算场景吗？其实也能。\n",
    "比如Flink也能做批计算，就是把批数据一条一条的发送过来，然后当成流数据来处理。\n",
    "但这里边有一个关键的问题，就是用批计算去模拟云计算，一定要涉及到最小的批规模的问题。\n",
    "要凑够一个最小的规模形成微批，这个最小可能也要几百毫秒。\n",
    "而如果直接用流计算，可能响应时间就能缩到几毫秒。\n",
    "在一些特定的商业场景，响应时间可能就很重要。\n",
    "比如电子商务、电子交易，实时外卖派单等等。\n",
    "当然也有的场景对于响应时间没有那么强的要求，反倒是对数据规模有要求。\n",
    "比如机器学习，大语言模型的训练等等场景。\n",
    "但综合来看，用流计算去模拟批计算是相对简单的。\n",
    "而反过来用批计算去模拟流计算总是要面对着响应时间的障碍。\n",
    "\n",
    "# 1.7 流计算的需求场景\n",
    "\n",
    "流计算秉承一个基本理念，即数据的价值随着时间的流逝而降低，如用户点击、商品浏览、天气数据等等。\n",
    "相比昨天的天气，更注重今天的天气。\n",
    "当事件出现时就应该立即进行处理，而不是缓存起来进行批量处理。\n",
    "\n",
    "自然环境中，数据的产生原本就是流式的。\n",
    "无论是来自 Web 服务器的事件数据，证券交易所的交易数据，还是来自工厂车间机器上的传感器数据，其数据都是流式的。\n",
    "当分析数据时，可以围绕 有界流（bounded）或 无界流（unbounded）两种模型来组织处理数据。\n",
    "选择不同的模型，程序的执行和处理方式也都会不同。\n",
    "\n",
    "![](./images/bounded-unbounded.png)\n",
    "\n",
    "批处理是有界数据流处理的范例。在这种模式下，你可以选择在计算结果输出之前输入整个数据集，这也就意味着你可以对整个数据集的数据进行排序、统计或汇总计算后再输出结果。\n",
    "\n",
    "流处理正相反，其涉及无界数据流。至少理论上来说，它的数据输入永远不会结束，因此程序必须持续不断地对到达的数据进行处理。\n",
    "\n",
    "为了及时处理流数据，就需要一个低延迟、可扩展、高可靠的处理引擎\n",
    "对于一个流计算系统来说，它应达到如下需求：\n",
    "* 高性能：处理大数据的基本要求，如每秒处理几十万条数据\n",
    "* 海量式：支持TB级甚至是PB级的数据规模\n",
    "* 实时性：保证较低的延迟时间，达到秒级别，甚至是毫秒级别\n",
    "* 分布式：支持大数据的基本架构，必须能够平滑扩展\n",
    "* 易用性：能够快速进行开发和部署\n",
    "* 可靠性：能可靠地处理流数据\n",
    "\n",
    "\n",
    "#### 思考题 2 对静态数据和流数据的处理，在计算模式上有什么不同？\t\n",
    "\n",
    "#### 思考题 3 使用批计算框架来处理流数据的思路是什么？\n",
    "\n",
    "#### 思考题 4 使用流计算框架来处理批数据的思路是什么？\n",
    "\n",
    "#### 思考题 5 适合流计算的场景可能是什么样的？请举例说明。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 流计算的框架选择\n",
    "\n",
    "\n",
    "## 2.1 传统架构的困境\n",
    "\n",
    "传统的大数据系统一般使用所谓的Lambda架构，如下图所示。\n",
    "其基本思路就是将数据进行分流：\n",
    "* 一部分作为实时数据，使用流处理框架，比如Storm或者Spark Streaming来处理；\n",
    "* 另一部分存入全量数据，使用MapReduce或者Spark来处理，当然现在主要是Spark。\n",
    "\n",
    "![](./images/lambda.svg)\n",
    "\n",
    "上面这种传统的Lambda架构有啥问题呢？\n",
    "首先就是架构复杂，技术栈多样，风险比较高。\n",
    "一旦某一个环节出问题，就可能影响到很多方面。\n",
    "由于整体架构复杂，又难以快速排查。\n",
    "另外是运营和维护成本高。\n",
    "每一条数据流程都需要有对应的单独的技术栈，可能就都需要对应的计算环境，就有可能带来资源浪费。\n",
    "另外每一个单独的技术栈也需要单独的技术团队来维护，人力成本和管理成本都上升了。\n",
    "\n",
    "\n",
    "## 2.2 流计算框架对比\n",
    "\n",
    "针对上面的Lambda架构的复杂度问题，解决思路自然是统一用一个技术栈。\n",
    "\n",
    "Spark的解决方案是使用Spark本体进行批计算，使用Spark Streaming进行流计算。\n",
    "但实际上Spark Streaming是将流数据分成非常小的微批来处理，因此难以达到毫秒级的响应。\n",
    "对Spark而言，它会使用一系列连续的微小批处理来模拟流处理，也就是说，它会在特定的时间间隔内发起一次计算，而不是每条数据都触发计算，这就相当于把无界数据集切分为多个小量的有界数据集。\n",
    "\n",
    "Flink的解决方案是全面流计算，将批数据也都当作有限流来处理。\n",
    "这样一来确实很好地降低了整体架构的复杂度。但是在机器学习等场景下还不如Spark好用。\n",
    "对Flink而言，它把有界数据集看成无界数据集的一个子集，因此，将批处理与流处理混合到同一套引擎当中，用户使用Flink引擎能够同时实现批处理与流处理任务。\n",
    "\n",
    "#### 思考题 6 Spark 和 Flink 设计思路有何差异？\n",
    "\n",
    "除此以外，关于流计算，还有传统的老牌实力派框架Storm。\n",
    "Storm的性能也可以，能做到毫秒级延迟，但问题是吞吐量不够高，而且数据一致性上有点问题。\n",
    "\n",
    "## 2.3 数据一致性\n",
    "\n",
    "数据一致性是个什么概念呢？\n",
    "\n",
    "当在分布式系统中引入状态时，自然也引入了一致性问题。根据正确性级别的不同，一致性可以分为如下三种形式：\n",
    "* 最多一次（at-most-once）可能会少：尽可能正确，但不保证一定正确。也就是说，当故障发生时，什么都不做，既不恢复丢失状态，也不重播丢失的数据。这就意味着，在系统发生故障以后，聚合结果可能会出错。\n",
    "* 至少一次（at-least-once）可能会多：在系统发生故障以后，聚合计算不会漏掉故障恢复之前窗口内的事件，但可能会重复计算某些事件，这通常用于实时性较高但准确性要求不高的场合。该模式意味着系统将以一种更加简单的方式来对算子的状态进行快照处理，系统崩溃后恢复时，算子的状态中有一些记录可能会被重放多次。例如，失败后恢复时，统计值将等于或者大于流中元素的真实值。\n",
    "* 精确一次（exactly-once）个数精准：在系统发生故障后，聚合结果与假定没有发生故障情况时一致。该模式意味着系统在进行恢复时，每条记录将在算子状态中只被重播一次。例如在一段数据流中，不管该系统崩溃或者重启了多少次，该统计结果将总是跟流中的元素的真实个数一致。这种语义加大了高吞吐和低延迟的实现难度。与“至少一次”模式相比，“精确一次”模式整体的处理速度会相对比较慢，因为在开启“精确一次”模式后，为了保证一致性，就会开启数据对齐，从而会影响系统的一些性能。\n",
    "\n",
    "| **数据一致性** | **基本含义** |**出错应对** |\n",
    "|------------|------------|------------|\n",
    "| 最多一次 at most once| 一条数据在数据流中最多出现一次| 出错即抛弃 |\n",
    "| 至少一次 at least once| 一条数据在数据流中最少出现一次| 一直补上去，多了也不管 |\n",
    "| 精确一次 exactly once| 一条数据在数据流中只出现一次| 出错了也要补上去，且保证只有一个|\n",
    "\n",
    "简单来说，数据一致性可以分为如下三种形式：\n",
    "* 最多一次（at-most-once）可能会少，当故障发生时，什么都不做，既不恢复丢失状态，也不重播丢失的数据。\n",
    "* 至少一次（at-least-once）可能会多，不会漏掉故障恢复之前窗口内的事件，但可能会重复计算某些事件，适合实时性较高但准确性要求不高的场合。\n",
    "* 精确一次（exactly-once）个数精准，每个事件用且仅用一次，适合对数据准确性要求较高的场合。\n",
    "\n",
    "#### 思考题 7 分布式计算框架里数据一致性一般有哪三种？各自适合什么场景？\n",
    "\n",
    "\n",
    "单独来看看流处理框架，曾经的主流是Storm，但随着技术发展，现在的舞台上主角已经是Spark和Flink了。\n",
    "Spark Streaming由于微批（Mini Batch）设计，难以实现毫秒级的响应，在对实时性要求比较高的场景下不太适用。\n",
    "不过由于Spark有多年积累，机器学习领域的应用很广泛。\n",
    "\n",
    "| **框架** | **数据一致性** | **吞吐量** | **延迟** | **适应场景** |\n",
    "|------------|------------|------------|------------|------------|\n",
    "| Storm      | - 至少一次 | - 低 | - 毫秒 | - 稍低吞吐量场景 |\n",
    "| Spark Streaming   | - 精确一次 | - 高 | - 100毫秒 |- 响应时间不敏感场景，如机器学习场景 |\n",
    "| Flink      | - 精确一次 | - 高 | - 毫秒 |- 响应时间敏感场景 |\n",
    "\n",
    "如果要真正实现流批一体处理，Flink更适合高吞吐量的快速响应场景。\n",
    "\n",
    "## 2.4 Flink 的发展历史\n",
    "\n",
    "Flink是Apache软件基金会的一个顶级项目，是为分布式、高性能、随时可用以及准确的流处理应用程序打造的开源流处理框架，并且可以同时支持实时计算和批量计算。\n",
    "\n",
    "Flink起源于Stratosphere 项目，该项目是在2010年到2014年间由柏林工业大学Technical University of Berlin、柏林洪堡大学 Humboldt University of Berlin和哈索普拉特纳研究所Hasso Plattner Institute联合开展的。\n",
    "\n",
    "Flink具有十分强大的功能，可以支持不同类型的应用程序。Flink的主要特性包括：批流一体化、精密的状态管理、事件时间支持以及精确一次的状态一致性保障等。\n",
    "Flink 不仅可以运行在包括 YARN、Kubernetes等在内的多种资源管理框架上，还支持在裸机集群上独立部署。\n",
    "在启用高可用选项的情况下，Flink不存在单点失效问题。\n",
    "Flink 已经可以扩展到数千核心，其状态可以达到 TB 级别，且仍能保持高吞吐、低延迟的特性。\n",
    "世界各地有很多要求严苛的流处理应用都运行在 Flink 之上。\n",
    "\n",
    "在国外，优步、网飞、微软和亚马逊等已经开始使用Flink。\n",
    "在国内，包括阿里巴巴、美团、滴滴等在内的知名互联网企业，都已经开始大规模使用Flink作为企业的分布式大数据处理引擎。\n",
    "\n",
    "在阿里巴巴，基于Flink搭建的平台于2016年正式上线，并从阿里巴巴的搜索和推荐这两大场景开始实现。\n",
    "阿里巴巴很多业务都采用了基于Flink搭建的实时计算平台，内部积累起来的状态数据，已经达到PB级别规模。\n",
    "每天在平台上处理的数据量已经超过万亿条，在峰值期间可以承担每秒超过4.72亿次的访问，最典型的应用场景是双11。\n",
    "\n",
    "2014年4月，Stratosphere代码被贡献给Apache软件基金会，成为Apache软件基金会孵化器项目。\n",
    "2014年12月，Flink项目成为Apache软件基金会顶级项目。\n",
    "\n",
    "## 2.5 Flink 的基本概念\n",
    "\n",
    "\n",
    "\n",
    "### 2.5.1 Flink 的抽象层次\n",
    "\n",
    "Flink 设计了四个抽象层次，从最基础到最高层依次是状态数据流、数据流和数据集核心接口、数据表接口、高级交互式查询语言接口。\n",
    "\n",
    "![](./images/levels_of_abstraction.svg)\n",
    "\n",
    "Flink API 最底层的抽象为有状态实时流处理。\n",
    "\n",
    "Flink API 第二层抽象是核心接口（Core APIs），也是大家一般要用到的。\n",
    "其中对咱们课程来说，最重要的就是数据流接口（DataStream API） 应用于有界/无界数据流场景。\n",
    "这一层 API 中处理的数据类型在每种编程语言中都有其对应的类。\n",
    "目前 PyFlink 官方样例没有看到提及 DataSet API（数据集接口）。\n",
    "\n",
    "Flink API 第三层抽象是数据表接口（Table API）。\n",
    "数据表接口（Table API） 是以表（Table）为中心的。\n",
    "流式数据场景下，可以表示一张正在动态改变的表。\n",
    "数据表（Table）拥有模式（schema），发音 “skee-muh” 或者“skee-mah”。\n",
    "类似于关系型数据库中的 schema，定义了组织和结构，包含了表、列、数据类型、视图、存储过程、关系等。\n",
    "数据表接口（Table API）也提供了类似于关系模型中的操作，比如 select、project、join、group-by 和 aggregate 等。\n",
    "\n",
    "在Flink中，数据表（Table）和数据流（DataStream）可切换，Flink 允许用户在编写应用程序时混合使用。\n",
    "\n",
    "Flink API 最顶层抽象是 SQL，其程序实现都是 SQL 查询表达式。\n",
    "SQL 查询语句可以在 Table API 中定义的表上执行。\n",
    "\n",
    "### 2.5.2 Flink 的架构设计\n",
    "\n",
    "Flink 可以运行在本地，也可以运行于集群之中。\n",
    "在集群上，Flink 有两类进程角色：至少一个工作管理节点（JobManager）和多个工作节点（Worker）或者也叫任务管理节点（TaskManager）。\n",
    "\n",
    "工作管理节点（JobManager）负责调度和资源管理，内部有三个不同组件：\n",
    "* 资源管理器（ResourceManager）\n",
    "* 作业分发器（Dispatcher）\n",
    "* 任务管理员（JobMaster）\n",
    "\n",
    "![](./images/processes.svg)\n",
    "\n",
    "增加工作管理节点，可以提高Flink集群系统的可用性。\n",
    "但多个工作管理节点之中，只有一个一直是首要节点（leader），其他的是待命状态（standby）\n",
    "下图展示的是一个三个工作管理节点的集群，其中首要节点发生故障后，任务进行了移交。\n",
    "\n",
    "![](./images/jobmanager_ha_overview.png)\n",
    "\n",
    "工作节点（Worker）/任务管理节点（TaskManager）执行作业流的人物（task），缓存和交换数据流。\n",
    "* 一个工作节点中资源调度的最小单位是任务槽（task slot）；\n",
    "* 一个工作节点并发处理任务的数量就是任务槽的总量；\n",
    "* 每一个任务槽可以执行多个算子。\n",
    "\n",
    "![](./images/tasks_chains.svg)\n",
    "\n",
    "客户端可以是单独的Java/Python程序，去访问已有的Flink集群。\n",
    "也可以在Flink环境下以命令方式运行。\n",
    "\n",
    "客户端访问Flink 集群有两种模式：\n",
    "* 分离模式下，客户端可以与集群断开连接；\n",
    "* 附加模式下，客户端可以与集群保持连接接收进程报告。\n",
    "\n",
    "\n",
    "\n",
    "### 2.5.3 Flink 的数据流\n",
    "\n",
    "在 Flink 中，应用程序由用户自定义算子转换而来的数据流（dataflows） 所组成。\n",
    "这些数据流形成了有向图，以一个或多个源（source）开始，并以一个或多个汇（sink）结束。\n",
    "在这个过程中，程序会对数据流进行各种变换（transformation），这些变换往往是由一个或者多个算子（operator）组成。\n",
    "\n",
    "![](./images/program_dataflow.svg)\n",
    "\n",
    "数据流的来源可以是文件、数据库、消息队列或者实时数据，比如来自Kafka，也可以是有界的历史数据。\n",
    "数据流被Flink进行变换操作后得到的结果可以是数据汇，也可以继续是新的数据流。\n",
    "新的数据流就可以发送到其他的程序过程中。\n",
    "![](./images/flink-application-sources-sinks.png)\n",
    "\n",
    "Flink 程序本质上是分布式并行程序。\n",
    "Flink 程序执行期间，一个流有一个或多个流分区（Stream Partition）；\n",
    "每个算子有一个或多个算子子任务（Operator Subtask）；\n",
    "每个子任务彼此独立，并在不同的线程、在不同的计算机或容器中运行。\n",
    "算子的子任务的个数就是其对应算子的并行度。\n",
    "在同一程序中，不同算子也可能具有不同的并行度。\n",
    "\n",
    "![](./images/parallel_dataflow.svg)\n",
    "\n",
    "\n",
    "Flink 可以将任何可序列化的对象转化为流，自带的序列化器有：\n",
    "* 基本类型，String、Long、Integer、Boolean、Array\n",
    "* 复合类型：Tuples、POJOs、Scala case classes\n",
    "\n",
    "将上面的各种数据转换成数据流之后，还要构建执行环境。\n",
    "每个 Flink 应用都需要有执行环境，在大多数示例代码中为 env。\n",
    "流式应用需要用到 StreamExecutionEnvironment。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 2.5.4 Flink 的数据表\n",
    "\n",
    "Flink提供了两种关系型接口，一种是数据表接口（Table API），另一种是Flink SQL。前者是用于Java/Python的查询接口，可以直观地对数据进行选取过滤等操作运算，后者是一种标准SQL接口。\n",
    "\n",
    "要注意，Flink 的数据表（Table）并不是传统意义的表，而是可以以不断变化修改的形态统一处理流和批的通用数据接口。\n",
    "\n",
    "数据表的来源可以是数据流，也可以是具体的文件，比如JSON文件或者CSV文件等等。\n",
    "在后续的代码环节，这些场景我们都会遇到。\n",
    "\n",
    "下面是 Flink 中数据表内容所支持的一些变量类型\n",
    "\n",
    "| **变量类型** | **简单解释** | \n",
    "|------------|------------|\n",
    "|CHAR\t|             |\n",
    "|VARCHAR|\t             |\n",
    "|STRING\t|             |\n",
    "|BOOLEAN|\t             |\n",
    "|BINARY\t|             |\n",
    "|VARBINARY|\t             |\n",
    "|BYTES\t|             |\n",
    "|DECIMAL|\tSupports fixed precision and scale.             |\n",
    "|TINYINT|\t             |\n",
    "|SMALLINT|\t             |\n",
    "|INTEGER|\t             |\n",
    "|BIGINT\t|             |\n",
    "|FLOAT\t|             |\n",
    "|DOUBLE\t|             |\n",
    "|DATE\t|             |\n",
    "|TIME\t|Supports only a precision of 0.             |\n",
    "|TIMESTAMP\t|             |\n",
    "|TIMESTAMP_LTZ|\t             |\n",
    "|INTERVAL|\tSupports only interval of MONTH and SECOND(3).             |\n",
    "|ARRAY\t|             |\n",
    "|MULTISET|\t             |\n",
    "|MAP\t|             |\n",
    "|ROW\t|             |\n",
    "|RAW\t|             |\n",
    "|Structured types|\tOnly exposed in user-defined functions yet.|  \n",
    "\n",
    "\n",
    "下面是这些变量类型的翻译，有的数据类型比如RAW我都没在计算机里面遇到过：\n",
    "\n",
    "| **变量类型** | **翻译** |\n",
    "|------------|--------|\n",
    "| CHAR       | 字符串   |\n",
    "| VARCHAR    | 可变长度字符串 |\n",
    "| STRING     | 字符串   |\n",
    "| BOOLEAN    | 布尔值   |\n",
    "| BINARY     | 二进制数据 |\n",
    "| VARBINARY  | 可变长度二进制数据 |\n",
    "| BYTES      | 字节    |\n",
    "| DECIMAL    | 十进制定点数 |\n",
    "| TINYINT    | 微整型   |\n",
    "| SMALLINT   | 小整型   |\n",
    "| INTEGER    | 整型    |\n",
    "| BIGINT     | 大整型   |\n",
    "| FLOAT      | 浮点数   |\n",
    "| DOUBLE     | 双精度浮点数 |\n",
    "| DATE       | 日期    |\n",
    "| TIME       | 无时区意义的时间 |\n",
    "| TIMESTAMP  | 时间戳   |\n",
    "| TIMESTAMP_LTZ | 本地时间戳，这个LTZ就是 local time zone |\n",
    "| INTERVAL   | 仅支持月和秒的间隔 |\n",
    "| ARRAY      | 数组    |\n",
    "| MULTISET   | 多集合  |\n",
    "| MAP        | 映射    |\n",
    "| ROW        | 行     |\n",
    "| RAW        | 原始数据  |\n",
    "|Structured types| 结构化类型, 仅在用户定义的函数中公开。 |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 环境搭建\n",
    "\n",
    "工欲善其事必先利其器，工具的选择是第一步。\n",
    "\n",
    "## 3.1 操作系统安装\n",
    "\n",
    "生产环境中，大规模集群普遍使用的是GNU/Linux操作系统，当然也有一些追求稳定性的场景可能会选用FreeBSD或者OpenBSD之类的BSD系统。\n",
    "对于初入相关领域的同学，可以选择从Debian系的发行版入手，如果以后对运维方面感兴趣，可以再自行探索RedHat系的系统发行版。\n",
    "\n",
    "本次课程选用的是 Ubuntu 22.04.3 LTS AMD64 版本操作系统，可以从[清华大学TUNA的下载地址](https://mirrors.tuna.tsinghua.edu.cn/ubuntu-releases/jammy/ubuntu-22.04.3-desktop-amd64.iso)来获取ISO文件。\n",
    "\n",
    "操作系统的安装有两种方式，物理机安装，或者虚拟机安装。\n",
    "\n",
    "物理机安装就是将操作系统安装到真实的物理实体的一天机器上。\n",
    "这样的好处是性能会充分发挥，与真实环境非常接近。\n",
    "适合有单独的一台机器可以用于安装的场景。\n",
    "但单独使用Linux系统可能要面对一些软件生态的挑战，因此不适合对Windows或者macOS软件生态有重度依赖的场景。\n",
    "\n",
    "虚拟机安装有两种方式。首先，对于Windows10/11的用户来说，可以试试用 WSL 或者 WSL2。\n",
    "WSL 是 Windows Subsystem for Linux 的缩写，意思为适用于 Linux 的 Windows 子系统。\n",
    "WSL2 是 WSL1 的升级版本，有更方便的文件访问权限。\n",
    "使用 WSL1/2 实际上是基于 Windows 10/11 内置的 Hyper-V 虚拟机，优势在于性能较好，且方便与宿主系统交互。\n",
    "缺点在于网络配置无法单独实现，通常只能使用宿主系统的网络地址转发。\n",
    "因此适合注重性能但不需要复杂集群网络结构的单节点场景。\n",
    "\n",
    "最常见的虚拟机安装方式，是基于 VMware Player 或者 VirtualBox 之类的桌面虚拟化软件来安装。\n",
    "这些桌面虚拟化软件安装虚拟机后，虚拟机的打开和运行就像是宿主机上的一个应用程序一样，非常方便。\n",
    "这种场景可能最适合大家初学时期的日常使用。\n",
    "VMware Player 的性能似乎更好些，但似乎仅在 Linux 宿主的版本中提供了进阶的网络配置功能，Windows上没有提供。\n",
    "VirtualBox 的性能表现稍差，但有非常全面的配置选项，尤其是网络自定义功能很方便，适合虚拟组网。\n",
    "\n",
    "还有一种虚拟机安装方式，是先安装专门用于虚拟化的平台系统，比如PVE（Proxmox Virtual Environment）或者 VMware ESXi 等。\n",
    "然后在安装好的虚拟化平台上来安装多个虚拟机系统。\n",
    "这种安装方式被用于很多企业的开发和生产环境中，与容器和资源管理调度等相结合，可以实现服务的冗余备份和无感知迁移。\n",
    "\n",
    "还有一种更为复杂一点的玩法，是使用嵌套虚拟化（nested virtualization），这需要你的处理器等硬件平台支持。\n",
    "对于Intel处理器，需要VT-x支持；对于AMD处理器，需要AMD-V支持。当然，最近几年的主流处理器可能都支持了。\n",
    "在硬件支持的前提下，可以用桌面虚拟化的方式借助 VMware Player 或者 VirtualBox，在其中安装 PVE 或者 ESXi 之类的虚拟化平台，然后再在平台上安装需要的虚拟机。\n",
    "当然，嵌套虚拟化可能要面对网络设置和存储资源分配等多方面较为复杂的配置问题。\n",
    "\n",
    "综合考虑，本次课程选择使用 [VirtualBox](https://www.virtualbox.org/wiki/Downloads) 这一虚拟机软件，在其中运行 [Ubuntu 22.04.3 操作系统](https://mirrors.tuna.tsinghua.edu.cn/ubuntu-releases/jammy/ubuntu-22.04.3-desktop-amd64.iso)。\n",
    "\n",
    "在安装的过程中，大家都尽量使用默认配置即可，另外需要注意的是，请尽量将用户名和密码暂且都设置为 hadoop，以保持与分布式课程的设置一致性。\n",
    "\n",
    "大家可以选择自行安装虚拟机系统，也可以直接使用本课程提供的虚拟镜像。\n",
    "虚拟机镜像中本课程项目的存储路径为`/home/hadoop/Desktop/PyFlink-Tutorial`。\n",
    "\n",
    "![](./images/VirtualMachine.png)\n",
    "\n",
    "\n",
    "## 3.2 依赖组件安装\n",
    "\n",
    "安装好操作系统之后，还需要安装一些基础组件。\n",
    "编程语言方面，本课程使用的是Python编程语言。\n",
    "其他组件主要有两个，一个适用于流数据传输与接收的 Apache Kafka，另一个是用于流数据的处理与计算的 Apache Flink。\n",
    "\n",
    "考虑到授课时长和复杂度等方面的因素，本次课程不再单独讲授 Apache Kafka 的安装，也不使用单独运行的 Kafka，而是使用 docker 镜像来运行一个本地实例。\n",
    "在 Python 下使用的 PyFlink 需要本地安装有 Apache Flink，只需要采取单节点模式即可。\n",
    "\n",
    "### 3.2.1 Anaconda3 安装\n",
    "\n",
    "考虑到版本管理和环境控制等方面的便利性，选择安装 Anaconda3。\n",
    "首先从TUNA下载Anaconda3安装包。\n",
    "这里使用的是特定的版本，目的是为了保证后面组件的兼容性。\n",
    "\n",
    "```Bash\n",
    "wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-2023.09-0-Linux-x86_64.sh\n",
    "sh Anaconda3-2023.09-0-Linux-x86_64.sh\n",
    "```\n",
    "安装过程中，请使用默认设置。\n",
    "应该安装在`~/anaconda3`。\n",
    "\n",
    "注意不要下载成其他操作系统或者其他处理器架构的版本。\n",
    "\n",
    "### 3.2.2 Python 3.9 安装\n",
    "\n",
    "Anaconda3 最新版默认安装的可能已经是 Python 3.11 甚至 Python 3.12。\n",
    "而我们所用的 PyFlink 还是适合运行在 Python 3.9 这个版本上。\n",
    "\n",
    "这里要强调的是，生产环境中，对于很多组件不能够盲目追求最新，而环境的稳定性才是最重要的。\n",
    "\n",
    "通过 conda 安装 Python 3.9 将变得简单可靠。\n",
    "\n",
    "```Bash\n",
    "conda create -n pyflink_39 python=3.9\n",
    "conda activate pyflink_39\n",
    "```\n",
    "\n",
    "\n",
    "### 3.2.3 Apache-Flink 安装\n",
    "\n",
    "先去[Apache 官网](https://dlcdn.apache.org/flink/)下载安装 flink，这里以 1.18.0 为例：\n",
    "\n",
    "```Bash\n",
    "wget https://dlcdn.apache.org/flink/flink-1.18.0/flink-1.18.0-bin-scala_2.12.tgz\n",
    "sudo tar -zxvf flink-1.18.0-bin-scala_2.12.tgz  -C /usr/local   \n",
    "```\n",
    "\n",
    "修改目录名称，并设置权限，命令如下：\n",
    "```Bash\n",
    "cd /usr/local\n",
    "sudo mv / flink-1.18.0 ./flink #这里是因为我这里下的是这个版本，读者需要酌情调整\n",
    "sudo chown -R hadoop:hadoop ./flink ##这里是因为我这里虚拟机的用户名是这个，读者需要酌情调整\n",
    "```\n",
    "\n",
    "Flink解压缩并且设置好权限后，直接就可以在本地模式运行，不需要修改任何配置。\n",
    "如果要做调整，可以编辑`“/usr/local/flink/conf/flink-conf.yam`这个文件。\n",
    "比如其中的`env.java.home`参就可以设置为本地Java的绝对路径\n",
    "不过一般不需要手动修改什么配置。\n",
    "\n",
    "不过，需要注意的是，Flink现在需要的是Java11，所以需要用下列命令手动安装一下：\n",
    "```Bash\n",
    "sudo apt install openjdk-11-jdk -y\n",
    "```\n",
    "\n",
    "接下来还需要修接下来还需要修改配置文件，添加环境变量：\n",
    "\n",
    "```Bash\n",
    "nano ~/.bashrc\n",
    "```\n",
    "\n",
    "文件中添加如下内容：\n",
    "```\n",
    "export FLINK_HOME=/usr/local/flink\n",
    "export PATH=$FLINK_HOME/bin:$PATH\n",
    "```\n",
    "\n",
    "保存并退出.bashrc文件，然后执行如下命令让配置文件生效：\n",
    "```Bash\n",
    "source ~/.bashrc\n",
    "```\n",
    "\n",
    "### 3.2.4 PyFlink 和 Kafka-Python 以及依赖包\n",
    "\n",
    "然后使用 pip 安装 apache-flink 包， 以及 Kafka-python 等等依赖包。\n",
    "\n",
    "```Bash\n",
    "pip install apache-flink \n",
    "pip install kafka-python chardet pandas numpy scipy simpy \n",
    "pip install matplotlib cython sympy xlrd pyopengl BeautifulSoup4 pyqt6 scikit-learn requests tensorflow torch keras tqdm gym DRL\n",
    "```\n",
    "\n",
    "## 3.3 代码说明\n",
    "\n",
    "本文项目示例代码修改自官方[文档版本1.18](https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/dev/python/datastream_tutorial/)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 流数据的生成与传输\n",
    "\n",
    "## 4.1 流数据的来源\n",
    "\n",
    "网站数据采集，用户行为产生，购物网站、社交网站。\n",
    "传感器采集，物联网传输，科研观测、探测器。\n",
    "人类在互联网上的几乎一切活动，都可以看作是流数据的生成过程。\n",
    "\n",
    "生成了流数据之后，如何使之流动，就成了问题。\n",
    "具体来说，要对其进行传输、存储、接收。\n",
    "\n",
    "## 4.2 Apache Kafka 基础\n",
    "\n",
    "本次课程我们使用的是 [Apache Kafka](https://kafka.apache.org/intro)。\n",
    "Kafka 是一个流数据传输平台，腾讯、字节跳动、Cisco、Oracle、Paypal、Spotify等等厂商都在使用。\n",
    "Kafka 主要具有三个主要的功能：\n",
    "* 写入（write）和读取（read）数据流；\n",
    "* 根据需求设计存储（store）数据流；\n",
    "* 对数据流进行处理（process）。\n",
    "\n",
    "简单来说，一个实际运行的 Kafka 分布式系统是基于 TCP/IP 网络协议的集群。\n",
    "具体又分为服务器（Servers）和客户端（Clients）两种。\n",
    "服务端可以是一个，也可以是多台，构成存储层的服务器称为代理（Broker）。\n",
    "客户端允许用户编写分布式应用或者微服务，可以对数据流进行并行的读写和处理。\n",
    "\n",
    "Kafka 支持很多种编程语言，比如 Java、Scala、Go、Python等。\n",
    "另外，Kafka 可以接入其他系统，或者将数据传输给其他系统。\n",
    "\n",
    "## 4.3 Kafka 的构成\n",
    "\n",
    "Kafka 的最小元素是事件（Event），一个事件一般有三个数据项目：\n",
    "* 事件键名（Event Key）：\"张三\"\n",
    "* 事件键值（Event Value）：\"支付话费50元\"\n",
    "* 事件时间戳（Event TimeStamp）：\"2023年12月1日12点48分\"\n",
    "当然，实际上还可以添加很多额外的数据。\n",
    "\n",
    "向整个 Kafka 系统写入事件流的，称为生产者（Producer），订阅（读取并处理）事件流的，称为消费者（Consumer）。\n",
    "有生产者写入事件，实际上就是生成流数据；也有消费者读取事件，实际上也就是使用流数据。\n",
    "\n",
    "可 Kafka 是一个分布式的并行系统，能一下子收发处理好多个数据流，那数据流之间怎么来区分呢？\n",
    "这就引入了下一个概念，就是主题（Topic）。\n",
    "主题就像是磁盘路径下的文件夹，关注同一个数据流的生产者和消费者只要指定好同样的主题，就不会和其他的数据流有混淆了。\n",
    "\n",
    "如果情况复杂一点，一个主题下有多个生产者，该怎么来保证消费者读取数据的时候不混乱呢？\n",
    "Kafka 给主题引入了分区的概念，具有相同事件关键字的事件会被写入相同的主题分区，保证特定主题分区的任何用户都能以与写入事件完全相同的顺序读取该分区的事件。\n",
    "\n",
    "![](./images/streams-and-tables-p1_p4.png)\n",
    "\n",
    "## 4.4 搭建 Kafka 环境\n",
    "\n",
    "Kafka 可以有多种安装和运行方式，比如可以和 Zookeeper 搭配，安装在物理集群上。\n",
    "另外也可以运行于 Docker 的容器之中。\n",
    "\n",
    "为了便于大家入门，本次课程选用容器的方式，使用 Docker 来运行一个本地的 Kafka 环境。\n",
    "然后利用 Python 代码将本地文件读取并发送给Kafka，接着再用 Python 代码将生成的对应数据流从 Kafka 中读取出来。\n",
    "\n",
    "\n",
    "1. 安装 Docker 和 Docker Compose:\n",
    "```Bash\n",
    "sudo apt install Docker Docker-compose\n",
    "```\n",
    "2. 创建本地 `docker-compose.yml` 文件，其中包含以下内容：\n",
    "\n",
    "```yaml\n",
    "version: '3'\n",
    "services:\n",
    "  zookeeper:\n",
    "    image: 'bitnami/zookeeper:latest'\n",
    "    environment:\n",
    "      - ALLOW_ANONYMOUS_LOGIN=yes\n",
    "  kafka:\n",
    "    image: 'bitnami/kafka:latest'\n",
    "    ports:\n",
    "      - '9092:9092'\n",
    "    environment:\n",
    "      - KAFKA_ADVERTISED_HOST_NAME=localhost\n",
    "      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181\n",
    "      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092\n",
    "      - KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9092\n",
    "      - KAFKA_CREATE_TOPICS=test:1:1\n",
    "      - ALLOW_PLAINTEXT_LISTENER=yes\n",
    "    depends_on:\n",
    "      - zookeeper\n",
    "```\n",
    "\n",
    "3. 找到“docker-compose.yml”所在目录并运行以下命令：\n",
    "\n",
    "````Bash\n",
    "sudo docker-compose up -d\n",
    "````\n",
    "\n",
    "这将运行一个包含 Zookeeper 实例和 Kafka 实例的本地 Kafka 集群，该集群将在本地主机的端口 9092 上运行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从文本到数据\n",
    "\n",
    "\n",
    "假设我们得到一个“data.csv”文件，其中包含很多。\n",
    "我们首先使用以下代码将“CSV”文件转换为“Kafka Stream”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 输出CSV数据\n",
    "\n",
    "Data_CSV_Stream_Shower.py 是一个使用 DataStream 处理 CSV 文件的 Python 脚本。实际上，下面的代码使用 re 函数。\n",
    "但这不重要，只是对从 CSV 文件生成的 DataStream 再进行输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MapFunction`: 将一个元素作为输入并将一个元素作为输出的函数。通过对每个元素应用转换，它可用于转换数据流。\n",
    "`FilterFunction`: 将一个元素作为输入并返回一个布尔值的函数。它可用于删除不符合特定条件的元素，从而过滤数据流。\n",
    "`KeySelector`: 从元素中提取键的函数。它可用于按键对数据流中的元素进行分组。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建筑数据的检测\n",
    "\n",
    "只做简单筛选咱们试过了。\n",
    "接下来是是稍微复杂的，对建筑沉降数据进行检测。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
