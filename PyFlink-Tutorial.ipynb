{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于Python的流数据处理技术简介\n",
    "\n",
    "## 1. 课程定位和设计思路\n",
    "\n",
    "咱们这门课是这学期第1次开。还是想着呢，就把那个传统的这个 Scala编程语言，嗯，放了一个流数据处理框架，带着大家跑几个样例，但后来发现这不妥。\n",
    "\n",
    "古人说，授人以鱼，不如授人以渔。如果只拿这一个框架把这东西嗯就怼给你，让你说这个就这么用这个就这么用，告诉完一遍相当于念说明书，这个可能意义不大。\n",
    "\n",
    "那就不如把这种东西的来龙去脉都给大家呈现一下，包括这流数据是个啥，为什么要弄这，等等咱们到时候都给说说。\n",
    "\n",
    "另外就是呢，咱们这课程不再给大家引入额外的一种编程语言了。一方面呢是这个会导致对基础思想理解的干扰。咱们本身在于道不在于术。而且这些框架的发展也很快，有可能前几个版本是支持这Scala的，但可能后几个版本就把他的支持给去掉了。有的时候一些函数和方法啊，在一些版本上是实现的，也可能在后面的几个版本就给改了。那这些情况下教大家背什么，嗯，某一个编程语言里边这某个东西是可修改的，另一个是不可修改的，这没有意义。\n",
    "\n",
    "比如说当初有一段的代码用那for循环来做，最开始是很慢的，后来改成列表推导式，唉，速度提升了，但后来编程语言发展的一点变化，唉编译环境有了一点改进，那就发现那for循环又可能比列表推导式快。所以考试要是说考到底哪个快，就像是被古诗词，那这就没意思，这东西不好玩。\n",
    "\n",
    "这东西是什么呢？咱们这个是先把这概念这思路，这最核心的这些东西说个大概不会说的，特别细，因为咱们课时有限，另外就是这方面的技术也在发展，但是主要的东西会给咱们讲到。\n",
    "\n",
    "咱们强调的是一个框架性的认知。就是关于这个流数据的整个这个过程，从他的基本概念思路，唉，不是被记的那个概念啊，主要是思维。再到这个他的。产生发送接收和处理，咱们都讲一讲。\n",
    "\n",
    "那这个过程会有很多环节，需要大家动手去探索。有可能你们之前学过Linux操作系统，也学过虚拟机，也学过Python啊，等等这些东西或者容器啊等等你们都学过吧？\n",
    "\n",
    "之前学的那些基础知识，在咱们这一面这种层次的课程就要开始应用了。\n",
    "\n",
    "本来呢，我是已经准备好了一个几乎完整的开发环境了。那虚拟机里面装好了所有的运行必备的组件以及编译器等等。这个我当然也会提供给大家，但是咱们这门课还是希望能够带着大家从头来把这东西弄起来，让你以后起码有个机会，如果你真用得上的话，你自个能把每一步都知道怎么做。不至于说我只是过了一门课的考试，然后就没了。\n",
    "\n",
    "如果你以后要从事这方面的相关工作，是不是听了咱这一门课就够了？那绝对不可能。因为这任何的一门大学的课程，一般都是一个概述。通过这种课程是让你了解这个领域的基础概念，基础思维，基础思路，然后呢，你需要再去深化和学习，你可能去读研专门从事学术领域的相关研究，你也可能去工作，在具体的生产环境去探索这方面的更深入的开发工作。\n",
    "\n",
    "## 2. 编程语言选择与代码难度\n",
    "\n",
    "咱们这门课选择的是Python编程语言，没有选择Java和Scala。这主要就是因为Python，大家好像也都学过这个，上手的难度会比较低，对大家来说可能会比较友好。如果选择Java呢，有的时候需要用maven去下载一些包，这些包的安装比Python的那个包的安装要麻烦一些，而且Java的这个开发环境的配置什么的呢，相比Python也要复杂一点，它的工程的内容含量更高一些。本来是也考虑过引入Scala的。后来我们教研室这几位老师反复商量了，就觉得引入一门全新的编程语言，对于大家来说还是有挑战。另外就是这个Scala本身可能要被一些开源项目所放弃，因为针对他的专门维护可能需要一些额外的工作经历，但现在很多开源团队，他未必能有这么多的额外精力去继续维护他。\n",
    "\n",
    "所以就选择对大家都友好，又好上手好配置，不用太担心网络问题的Python。\n",
    "\n",
    "所以大家不要怕，咱们这堂课上涉及到的代码一定不难，我基本上会把每一句代码都给大家做好注释，然后讲的时候，可能有的同学会嫌慢一点，但没办法，咱们要照顾大多数同学学有余力的同学，那你学得快，那你可以自己在自行探索更深入，但对于大多数同学来说，我会详细的讲每一部分的具体实现。\n",
    "\n",
    "当然这一定不会很难很复杂的啊，这种东西就是思路和架构上的复杂性。它是看上去吓人，但实际上在代码工作中呢可能就有几百行代码，顶多了，一般有的甚至就几十行代码就能把咱们。某一些部分的这个要点表述。\n",
    "\n",
    "\n",
    "## 3. 数据的特征概述\n",
    "\n",
    "首先得说数据这数据是什么东西呢？这块咱们讲的数据并不是计算机处理器里面的指令和数据那种狭义场景下的。而是一个更广泛意义上的数据。\n",
    "\n",
    "可以观测的，并被人类。之后能够用数值记录下来的。是一个定义，这是一个描述，描述和定义是不一样的。严格的定义可能会有很多的争论，但我做出这个描述它总是符合一定的经验的。\n",
    "\n",
    "总体来看，人的成长是一个自然过程，这个过程中人的各种指标数据可能会发生变化。如果不看成长的尺度，就看平时稳定的状态，如果感染疾病有病变，人的一些生理指标也会有变化。\n",
    "\n",
    "对于人自己来说，数据的测量很重要。而且有的一些人可能要由于身体健康的原因做长期的数据观测，检测血压、血糖等等。\n",
    "我家小朋友当初生病了，发烧晕厥，就被送到了重症监护室，那时候他的心率，血压，血氧都要持续记录数据。\n",
    "\n",
    "从人的个体到人的社会群体关系，这过程中都是有有数据的。或者更贴切的说是可以用数据去描述。衍生出来的手机号码，亲属关系，家庭成员，兴趣爱好，学历学位所学专业等等，这些信息也都是数据。\n",
    "\n",
    "所以可见关于一个人的所有的特征的描述都可以成为数据。而人是会成长变化的小朋友，刚出生的时候可能只有几公斤，等长到了几岁了，可能就有十几公斤甚至几十公斤。再长一些之后，比如像我这比较能吃的人，我曾经就有100多公斤啊。\n",
    "\n",
    "随着时间发展，人的身高体重和各种生理指标可能都要发生变化。\n",
    "\n",
    "那对于自然角度来说，就是人类去看待这世界，也要借助数据。外面天气的冷暖风速的变化，降雨量的多少，地震的强度，建筑沉降的幅度。湖泊的面积，河水的流量，海水的含盐度。这些也都是数据。\n",
    "\n",
    "## 4. 数据的形态划分\n",
    "\n",
    "那这种数据就可以有两种形态，一种是观测了一堆，不管这堆有多大，反正它是有固定的规模是有限的个数的它放到这儿了，也不对他进行改变了，这个就叫做静态的数据。另外一种情况有点复杂，就是你得一直关注你得一直更新，但为什么要一直贯彻一直更新呢？有两种可能，一种可能是这东西是无限的，它一直在被生成出来，随时在变化，那就得一直观测。另一种可能是这东西规模太大，你没办法短时间内穷尽。就算是有限，但是人力无法短时间内穷尽。\n",
    "\n",
    "前面这种很好理解啊，比如天气温度这些东西咱们得一直在观测，一直在记录，因为它随时在变化昨天的天气预报对于今天来说意义不大，前天的可能依旧更不大了，那没办法，咱们要知道以后的天气一定得是基于现在状态然后做的推算，那所以这种情况就是必须要持续观测和持续处理的，这个大家也好理解，这就是原生的流数据。\n",
    "\n",
    "另外一种就稍微绕了那么一点点。就是人的观测能力是有限的，这是因为人作为动物，我们的感官本身是受限的，然后人作为这种文明，我们记录文明的载体本身，它也存在物理上的上限，所以就注定了面对一些场景下人力无法全部的整体的把它全都认识，那只能一点一点来，那这个过程中也会导致数据是持续产生的。\n",
    "\n",
    "当然对于这个数据的形态得辩证的来看，静态的数据和流动的数据其实未必总是那么好区分。\n",
    "\n",
    "比如窗台边这有的是花岗岩，这些岩石形成的过程涉及到温度和压力和粘度等等方面的变化。这些变化有的是物理变化，有的是化学变化，这个过程有的可能要花费大概一百万年。人现在认识这东西，都是测试平均的这么个成分上的，变成了一个静态的数据，但其实它的形成过程是一个动态的过程。\n",
    "\n",
    "具体的个体，因为它自身状态可能会随着时间发生不断的变化，可能会产生流动的数据。\n",
    "有限的群体，因为他们彼此之间的关系可能会发生不断的变化。也可能会产生流动的数据。\n",
    "如果是一段有限的数据集合，但它在时间或者空间上可以有不同的排布和次序，也能变成流动的数据。\n",
    "\n",
    "#### 思考题 1 \n",
    "大家思考一下什么样的数据是静态的数据，什么样的数据是流数据？\n",
    "用自己的话通顺的表达出来，并且举一些例子，\n",
    "那你看特地强调了这个题，考试时候是不是会考呢？\n",
    "\n",
    "## 5. 避免思维受到知识诅咒\n",
    "\n",
    "知识诅咒其实有两个方向，一个方向是拥有了某些知识之后想再去思考，就是都容易带着这种知识的底色，很难再给这种外行的人讲明白了。会觉得有某些东西是潜移默化中觉得理所应当能知道的，而其实外行人不知道，这就带来沟通上的障碍。另一个方向就是已有知识对自己的阻碍和限制。\n",
    "\n",
    "学工科的人就需要去准备避免被自己所学的东西给限制住。因为工科的教育在本科阶段通识层面上，它其实可能会有所缺憾。缺憾在于两方面，一方面是自然科学知识可能就不再学了，另一方面是人文社科相关的知识可能也不是很重视，大家更重视的是什么呢？嗯，可能是计算机网络操作系统编译原理，数据结构，算法。这些当然很重要，但这些内容呢，是基于人所设计出来的体系的，是一般意义上来说内部自洽的体系的。\n",
    "\n",
    "而面对自然科学或者是人文社科方面的内容的时候，咱们。工科的同学就一定要避免被自己学科所塑造的那种思维所影响，把一些问题，给简单化的认识。当然了，把复杂的问题简单化，从中抽取出规律，这是很重要的，但另一方面就是很多复杂的事物，它并不一定都能够简单化。对于开发者或者是从事与开发相关工作的人员来说，一定要保持对自然科学和人文社科的敬畏，要摒弃掉自己所学的这个工科领域，具有万能效用的认知。\n",
    "\n",
    "有一些人的思维有一个特点，按照戚继光在绩效新书里面说的，叫“心有外物而不化，自恃旧习以为佳”。就是把一切东西都按照他所学过内容的那种模式去理解。\n",
    "\n",
    "## 6. 流数据场景下的现实问题\n",
    "\n",
    "专门提出来这个流数据的分析和处理，就是因为有一些场景下需要这类思路解决对应的问题。\n",
    "而用传统的那种静态数据的处理思路，可能有一些解决不了的问题。\n",
    "流数据处理技术是基于现实需求而产生。\n",
    "\n",
    "事物是时刻不停的发展变化的，万事万物都非静态，运动是无处不在的。\n",
    "自然界中咱们所观测到的一切数据实际上都是原生的流数据，因为这些东西往往存在着各种的变化，只是变化的幅度可能会有差异，但这种变化是不停的。\n",
    "\n",
    "在以前就有一些工科的人面对着自然界原生的流数据，尤其在人类的健康数据监测这个场景上翻车了。\n",
    "\n",
    "谷歌在若干年前曾经扬言要攻克癌症。前几年有一个人叫伊丽莎白霍尔姆斯办了一个后来被大家发现是诈骗的那么个公司说要通过验一滴血，就能监控人的身体卫生的健康状况。当时是有很多人都对这类的东西叫趋之若鹜。上当的人很多，也不乏一些所谓成功人士。\n",
    "\n",
    "但他们最终都失败了，谷歌并没有攻克癌症，而那伊丽莎白霍尔姆斯呢，也被判决入狱了。\n",
    "\n",
    "这里的各种细节咱们暂且不表。针对人这个个体的测量所得到的数据，身高体重这些当然是了，但更多的更常见的也是广泛被关注的，可能就是血液成分数据。血压，血细胞的比例，血脂，血糖等等这些都是数据。\n",
    "\n",
    "那人的这些个生理状况，健康指标它是稳定的吗？肯定不是啊，人有睡眠作息，人有吃五谷杂粮，还有可能要生病受伤等等都会导致生理状况，各种健康指标的变化，所以关于人的个体的数据就原本是变化的。\n",
    "\n",
    "大家生病的时候去医院有时候要做一检查就是血常规，血常规呢就有白细胞的比例，红细胞的比例，白细胞当中不同种类细胞的含量，比如说单核粒细胞有多少，淋巴细胞有多少等等。为什么这些数值它会有变化呢？因为人的生理状况发生了变化，有可能是生病了，这会有免疫反应的白细胞就增多炎症反应。\n",
    "\n",
    "另外就是做心电图，我小的时候这心肌炎有心脏病就去做心电图，但很不巧，我每次去做那会儿心疼难受，这劲儿就过去了，不再发这个时刻，能那一个时间段过去了之后再做心电图就做不出来。这种情况怎么办呢？后来有这新的技术叫24小时，心电就是更长的时间，窗口上持续的对我的心脏测量心电，带了一些东西，还得避免干扰等等，那这样得到的时间窗口更大了，就可能把这个异常值发现出来。\n",
    "\n",
    "## 7. 批计算与流计算\n",
    "\n",
    "简单来说，对静态数据就凑够一批来进行批计算，对于流数据就随着数据流进行流计算。\n",
    "这说的计算可以简单理解成就是加减乘除，甚至就是加法，当然还有逻辑运算。\n",
    "再说复杂点呢，就有映射、筛选、键值选择等等，这些后面再细说。\n",
    "\n",
    "批计算就是凑够一批，然后再算。\n",
    "流计算就是一边流着，一边计算。\n",
    "很直观的命名对不对？\n",
    "\n",
    "传统的批计算能不能解决上面的一些场景呢？其实也能。\n",
    "比如spark实际上就是用批计算，把流数据切成一小段一小段，然后每一段来进行处理。\n",
    "反过来流计算能去应对传统的批计算场景吗？其实也能。\n",
    "比如Flink也能做批计算，就是把批数据一条一条的发送过来，然后当成流数据来处理。\n",
    "但这里边有一个关键的问题，就是用批计算去模拟云计算，一定要涉及到最小的批规模的问题。\n",
    "要凑够一个最小的规模形成微批，这个最小可能也要几百毫秒。\n",
    "而如果直接用流计算，可能响应时间就能缩到几毫秒。\n",
    "在一些特定的商业场景，响应时间可能就很重要。\n",
    "比如电子商务、电子交易，实时外卖派单等等。\n",
    "当然也有的场景对于响应时间没有那么强的要求，反倒是对数据规模有要求。\n",
    "比如机器学习，大语言模型的训练等等场景。\n",
    "但综合来看，用流计算去模拟批计算是相对简单的。\n",
    "而反过来用批计算去模拟流计算总是要面对着响应时间的障碍。\n",
    "\n",
    "#### 思考题 2\n",
    "对静态数据和流数据的处理，在计算模式上有什么不同？\t\n",
    "\n",
    "#### 思考题 3 \n",
    "使用批计算框架来处理流数据的思路是什么？\n",
    "\n",
    "#### 思考题 4\n",
    "使用流计算框架来处理批数据的思路是什么？\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flink 简介\n",
    "\n",
    "Flink是Apache软件基金会的一个顶级项目，是为分布式、高性能、随时可用以及准确的流处理应用程序打造的开源流处理框架，并且可以同时支持实时计算和批量计算。\n",
    "\n",
    "Flink起源于Stratosphere 项目，该项目是在2010年到2014年间由柏林工业大学Technical University of Berlin、柏林洪堡大学 Humboldt University of Berlin和哈索普拉特纳研究所Hasso Plattner Institute联合开展的。\n",
    "\n",
    "2014年4月，Stratosphere代码被贡献给Apache软件基金会，成为Apache软件基金会孵化器项目。\n",
    "2014年12月，Flink项目成为Apache软件基金会顶级项目。\n",
    "\n",
    "目前，Flink是Apache软件基金会的5个最大的大数据项目之一。\n",
    "在全球范围内拥有350多位开发人员，并在越来越多的企业中得到了应用。\n",
    "在国外，优步、网飞、微软和亚马逊等已经开始使用Flink。\n",
    "在国内，包括阿里巴巴、美团、滴滴等在内的知名互联网企业，都已经开始大规模使用Flink作为企业的分布式大数据处理引擎。\n",
    "\n",
    "在阿里巴巴，基于Flink搭建的平台于2016年正式上线，并从阿里巴巴的搜索和推荐这两大场景开始实现。\n",
    "阿里巴巴很多业务都采用了基于Flink搭建的实时计算平台，内部积累起来的状态数据，已经达到PB级别规模。\n",
    "每天在平台上处理的数据量已经超过万亿条，在峰值期间可以承担每秒超过4.72亿次的访问，最典型的应用场景是双11。\n",
    "\n",
    "Flink具有十分强大的功能，可以支持不同类型的应用程序。Flink的主要特性包括：批流一体化、精密的状态管理、事件时间支持以及精确一次的状态一致性保障等。\n",
    "Flink 不仅可以运行在包括 YARN、 Mesos、Kubernetes等在内的多种资源管理框架上，还支持在裸机集群上独立部署。在启用高可用选项的情况下，它不存在单点失效问题。\n",
    "事实证明，Flink 已经可以扩展到数千核心，其状态可以达到 TB 级别，且仍能保持高吞吐、低延迟的特性。世界各地有很多要求严苛的流处理应用都运行在 Flink 之上。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyFlink 环境搭建\n",
    "\n",
    "## Anaconda3 安装\n",
    "\n",
    "首先从TUNA下载Anaconda3安装包。\n",
    "\n",
    "这里使用的是特定的版本，目的是为了保证后面组件的兼容性。\n",
    "\n",
    "```Bash\n",
    "wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-2023.09-0-Linux-x86_64.sh\n",
    "sh Anaconda3-2023.09-0-Linux-x86_64.sh\n",
    "```\n",
    "安装过程中，请使用默认设置。\n",
    "应该安装在`~/anaconda3`。\n",
    "\n",
    "## Python 3.9 安装\n",
    "\n",
    "通过 conda 安装 Python 3.9 将变得简单可靠。\n",
    "\n",
    "```Bash\n",
    "conda create -n pyflink_39 python=3.9\n",
    "conda activate pyflink_39\n",
    "```\n",
    "\n",
    "\n",
    "## Apache-Flink 安装\n",
    "\n",
    "先去[Apache 官网](https://dlcdn.apache.org/flink/)下载安装 flink，这里以 1.18.0 为例：\n",
    "\n",
    "```Bash\n",
    "wget https://dlcdn.apache.org/flink/flink-1.18.0/flink-1.18.0-bin-scala_2.12.tgz\n",
    "sudo tar -zxvf flink-1.18.0-bin-scala_2.12.tgz  -C /usr/local   \n",
    "```\n",
    "\n",
    "修改目录名称，并设置权限，命令如下：\n",
    "```Bash\n",
    "cd /usr/local\n",
    "sudo mv / flink-1.18.0 ./flink #这里是因为我这里下的是这个版本，读者需要酌情调整\n",
    "sudo chown -R hadoop:hadoop ./flink ##这里是因为我这里虚拟机的用户名是这个，读者需要酌情调整\n",
    "```\n",
    "\n",
    "Flink解压缩并且设置好权限后，直接就可以在本地模式运行，不需要修改任何配置。\n",
    "如果要做调整，可以编辑`“/usr/local/flink/conf/flink-conf.yam`这个文件。\n",
    "比如其中的`env.java.home`参就可以设置为本地Java的绝对路径\n",
    "不过一般不需要手动修改什么配置。\n",
    "\n",
    "不过，需要注意的是，Flink现在需要的是Java11，所以需要用下列命令手动安装一下：\n",
    "```Bash\n",
    "sudo apt install openjdk-11-jdk -y\n",
    "```\n",
    "\n",
    "接下来还需要修接下来还需要修改配置文件，添加环境变量：\n",
    "\n",
    "```Bash\n",
    "nano ~/.bashrc\n",
    "```\n",
    "\n",
    "文件中添加如下内容：\n",
    "```\n",
    "export FLINK_HOME=/usr/local/flink\n",
    "export PATH=$FLINK_HOME/bin:$PATH\n",
    "```\n",
    "\n",
    "保存并退出.bashrc文件，然后执行如下命令让配置文件生效：\n",
    "```Bash\n",
    "source ~/.bashrc\n",
    "```\n",
    "\n",
    "## 安装 Python 依赖包\n",
    "\n",
    "然后使用 pip 安装 apache-flink 包， 以及 Kafka-python 等等依赖包\n",
    "\n",
    "```Bash\n",
    "pip install apache-flink \n",
    "pip install kafka-python chardet pandas numpy scipy simpy \n",
    "pip install matplotlib cython sympy xlrd pyopengl BeautifulSoup4 pyqt6 scikit-learn requests tensorflow torch keras tqdm gym DRL\n",
    "```\n",
    "\n",
    "## 代码说明\n",
    "\n",
    "本文代码修改自官方[文档版本1.18](https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/dev/python/datastream_tutorial/)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Any for unsupported type: typing.Sequence[~T]\n",
      "No module named google.cloud.bigquery_storage_v1. As a result, the ReadFromBigQuery transform *CANNOT* be used with `method=DIRECT_READ`.\n",
      "\n",
      "First we map it: \n",
      "\n",
      "(1, '{\"name\": \"Flink\", \"tel\": 112, \"addr\": {\"country\": \"Germany\", \"city\": \"Berlin\"}}')\n",
      "(2, '{\"name\": \"hello\", \"tel\": 223, \"addr\": {\"country\": \"China\", \"city\": \"Shanghai\"}}')\n",
      "(3, '{\"name\": \"world\", \"tel\": 334, \"addr\": {\"country\": \"USA\", \"city\": \"NewYork\"}}')\n",
      "(4, '{\"name\": \"PyFlink\", \"tel\": 445, \"addr\": {\"country\": \"China\", \"city\": \"Hangzhou\"}}')\n",
      "\n",
      "Then we filter it: \n",
      "\n",
      "+I[1,{\"name\": \"Flink\", \"tel\": 111, \"addr\": {\"country\": \"Germany\", \"city\": \"Berlin\"}}]\n",
      "\n",
      "Then we select it: \n",
      "\n",
      "('Germany', 111)\n",
      "('China', 222)\n",
      "('USA', 333)\n",
      "('China', 444)\n"
     ]
    }
   ],
   "source": [
    "# 基本操作：Map, Filter, Keyby\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "from pyflink.common import Types\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "\n",
    "# 定义show函数，用于显示数据流\n",
    "def show(ds, env):\n",
    "    ds.print()\n",
    "    env.execute()\n",
    "\n",
    "# 定义update_tel函数，用于更新tel字段\n",
    "def update_tel(data):\n",
    "    json_data = json.loads(data.info)\n",
    "    json_data['tel'] += 1\n",
    "    return data.id, json.dumps(json_data)\n",
    "\n",
    "# 定义filter_by_id函数，用于过滤id字段\n",
    "def filter_by_id(data):\n",
    "    return data.id == 1\n",
    "\n",
    "# 定义map_country_tel函数，用于将国家字段和tel字段映射到元组中\n",
    "def map_country_tel(data):\n",
    "    json_data = json.loads(data.info)\n",
    "    return json_data['addr']['country'], json_data['tel']\n",
    "\n",
    "# 定义key_by_country函数，用于将元组中的国家字段作为key\n",
    "def key_by_country(data):\n",
    "    return data[0]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=\"%(message)s\")\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()\n",
    "    env.set_parallelism(1)\n",
    "    ds = env.from_collection(\n",
    "        collection=[\n",
    "            (1, '{\"name\": \"Flink\", \"tel\": 111, \"addr\": {\"country\": \"Germany\", \"city\": \"Berlin\"}}'),\n",
    "            (2, '{\"name\": \"hello\", \"tel\": 222, \"addr\": {\"country\": \"China\", \"city\": \"Shanghai\"}}'),\n",
    "            (3, '{\"name\": \"world\", \"tel\": 333, \"addr\": {\"country\": \"USA\", \"city\": \"NewYork\"}}'),\n",
    "            (4, '{\"name\": \"PyFlink\", \"tel\": 444, \"addr\": {\"country\": \"China\", \"city\": \"Hangzhou\"}}')\n",
    "        ],\n",
    "        type_info=Types.ROW_NAMED([\"id\", \"info\"], [Types.INT(), Types.STRING()])\n",
    "    )\n",
    "    print('\\nFirst we map it: \\n')\n",
    "    # 调用show函数，显示数据流\n",
    "    show(ds.map(update_tel), env)\n",
    "    \n",
    "    print('\\nThen we filter it: \\n')\n",
    "    # 调用show函数，显示过滤后的数据流\n",
    "    show(ds.filter(filter_by_id), env)\n",
    "\n",
    "    print('\\nThen we select it: \\n')\n",
    "    # 调用show函数，显示按照国家字段分组后的数据流\n",
    "    show(ds.map(map_country_tel).key_by(key_by_country), env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16> (2, {'name': 'hello', 'tel': 223, 'addr': {'country': 'China', 'city': 'Shanghai'}})\n",
      "2> (4, {'name': 'PyFlink', 'tel': 445, 'addr': {'country': 'China', 'city': 'Hangzhou'}})\n"
     ]
    }
   ],
   "source": [
    "# 处理 Json 数据\n",
    "\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "\n",
    "\n",
    "def process_json_data():\n",
    "    # 获取执行环境\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()\n",
    "\n",
    "    # define the source\n",
    "    # 定义源数据\n",
    "    ds = env.from_collection(\n",
    "        collection=[\n",
    "            (1, '{\"name\": \"Flink\", \"tel\": 111, \"addr\": {\"country\": \"Germany\", \"city\": \"Berlin\"}}'),\n",
    "            (2, '{\"name\": \"hello\", \"tel\": 222, \"addr\": {\"country\": \"China\", \"city\": \"Shanghai\"}}'),\n",
    "            (3, '{\"name\": \"world\", \"tel\": 333, \"addr\": {\"country\": \"USA\", \"city\": \"NewYork\"}}'),\n",
    "            (4, '{\"name\": \"PyFlink\", \"tel\": 444, \"addr\": {\"country\": \"China\", \"city\": \"Hangzhou\"}}')]\n",
    "    )\n",
    "\n",
    "    # 定义更新电话号码的函数\n",
    "    def update_tel(data):\n",
    "        # parse the json\n",
    "        # 解析json数据\n",
    "        json_data = json.loads(data[1])\n",
    "        # 更新电话号码\n",
    "        json_data['tel'] += 1\n",
    "        # 返回更新后的数据\n",
    "        return data[0], json_data\n",
    "\n",
    "    # 定义过滤函数，过滤掉国家不是中国的数据\n",
    "    def filter_by_country(data):\n",
    "        # the json data could be accessed directly, there is no need to parse it again using\n",
    "        # json.loads\n",
    "        # 直接访问json数据，不需要使用json.loads\n",
    "        return \"China\" in data[1]['addr']['country']\n",
    "\n",
    "    # 调用map函数，更新电话号码，并过滤掉国家不是中国的数据\n",
    "    ds.map(update_tel).filter(filter_by_country).print()\n",
    "\n",
    "    # submit for execution\n",
    "    # 提交执行\n",
    "    env.execute()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 设置日志输出格式\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=\"%(message)s\")\n",
    "\n",
    "    # 调用process_json_data函数\n",
    "    process_json_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7> ('Alice', 110.0999984741211)\n",
      "7> ('Alice', 130.0999984741211)\n",
      "7> ('Alice', 143.19999885559082)\n",
      "7> ('Alice', 163.29999923706055)\n",
      "2> ('Bob', 30.200000762939453)\n",
      "2> ('Bob', 83.29999923706055)\n",
      "2> ('Bob', 86.39999914169312)\n",
      "2> ('Bob', 102.49999952316284)\n"
     ]
    }
   ],
   "source": [
    "# 状态读取\n",
    "\n",
    "from pyflink.common import Time\n",
    "from pyflink.common.typeinfo import Types\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.datastream.functions import KeyedProcessFunction, RuntimeContext\n",
    "from pyflink.datastream.state import ValueStateDescriptor, StateTtlConfig\n",
    "\n",
    "\n",
    "# 定义一个Sum类，继承自KeyedProcessFunction\n",
    "class Sum(KeyedProcessFunction):\n",
    "\n",
    "    # 初始化函数\n",
    "    def __init__(self):\n",
    "        self.state = None\n",
    "\n",
    "    # 打开函数，获取运行时上下文\n",
    "    def open(self, runtime_context: RuntimeContext):\n",
    "        # 创建一个状态描述符，类型为float\n",
    "        state_descriptor = ValueStateDescriptor(\"state\", Types.FLOAT())\n",
    "        # 创建一个状态TTL配置，设置TTL时间为1秒，更新类型为OnReadAndWrite，禁用后台清理\n",
    "        state_ttl_config = StateTtlConfig \\\n",
    "            .new_builder(Time.seconds(1)) \\\n",
    "            .set_update_type(StateTtlConfig.UpdateType.OnReadAndWrite) \\\n",
    "            .disable_cleanup_in_background() \\\n",
    "            .build()\n",
    "        # 启用TTL，并传入TTL配置\n",
    "        state_descriptor.enable_time_to_live(state_ttl_config)\n",
    "        # 获取状态\n",
    "        self.state = runtime_context.get_state(state_descriptor)\n",
    "\n",
    "    # 处理元素函数\n",
    "    def process_element(self, value, ctx: 'KeyedProcessFunction.Context'):\n",
    "        # retrieve the current count\n",
    "        # 获取当前状态\n",
    "        current = self.state.value()\n",
    "        # 如果当前状态为空，则设置为0\n",
    "        if current is None:\n",
    "            current = 0\n",
    "\n",
    "        # update the state's count\n",
    "        # 更新状态的计数\n",
    "        current += value[1]\n",
    "        # 更新状态\n",
    "        self.state.update(current)\n",
    "\n",
    "        # 返回元组\n",
    "        yield value[0], current\n",
    "\n",
    "\n",
    "# 定义一个state_access_demo函数，用于演示状态访问\n",
    "def state_access_demo():\n",
    "    # 获取运行时环境\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()\n",
    "\n",
    "    # 从集合中创建一个流\n",
    "    ds = env.from_collection(\n",
    "        collection=[\n",
    "            ('Alice', 110.1),\n",
    "            ('Bob', 30.2),\n",
    "            ('Alice', 20.0),\n",
    "            ('Bob', 53.1),\n",
    "            ('Alice', 13.1),\n",
    "            ('Bob', 3.1),\n",
    "            ('Bob', 16.1),\n",
    "            ('Alice', 20.1)\n",
    "        ],\n",
    "        type_info=Types.TUPLE([Types.STRING(), Types.FLOAT()]))\n",
    "\n",
    "    # apply the process function onto a keyed stream\n",
    "    # 应用处理函数，对流中的每一个元素进行处理\n",
    "    ds.key_by(lambda value: value[0]) \\\n",
    "      .process(Sum()) \\\n",
    "      .print()\n",
    "\n",
    "    # submit for execution\n",
    "    # 提交执行\n",
    "    env.execute()\n",
    "\n",
    "\n",
    "# 调用state_access_demo函数\n",
    "if __name__ == '__main__':\n",
    "    state_access_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread read_grpc_client_inputs:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/fred/anaconda3/envs/pyflink_39/lib/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
      "Exception in thread read_grpc_client_inputs:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/fred/anaconda3/envs/pyflink_39/lib/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/fred/anaconda3/envs/pyflink_39/lib/python3.9/threading.py\", line 917, in run\n",
      "    self.run()\n",
      "  File \"/home/fred/anaconda3/envs/pyflink_39/lib/python3.9/threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/fred/anaconda3/envs/pyflink_39/lib/python3.9/site-packages/apache_beam/runners/worker/data_plane.py\", line 669, in <lambda>\n",
      "Exception in thread read_grpc_client_inputs:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/fred/anaconda3/envs/pyflink_39/lib/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
      "    target=lambda: self._read_inputs(elements_iterator),\n",
      "  File \"/home/fred/anaconda3/envs/pyflink_39/lib/python3.9/site-packages/apache_beam/runners/worker/data_plane.py\", line 652, in _read_inputs\n",
      "Exception in thread read_grpc_client_inputs:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/fred/anaconda3/envs/pyflink_39/lib/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/fred/anaconda3/envs/pyflink_39/lib/python3.9/site-packages/apache_beam/runners/worker/data_plane.py\", line 669, in <lambda>\n",
      "    self.run()\n",
      "  File \"/home/fred/anaconda3/envs/pyflink_39/lib/python3.9/threading.py\", line 917, in run\n",
      "    for elements in elements_iterator:\n",
      "  File \"/home/fred/anaconda3/envs/pyflink_39/lib/python3.9/site-packages/grpc/_channel.py\", line 541, in __next__\n",
      "    target=lambda: self._read_inputs(elements_iterator),\n",
      "  File \"/home/fred/anaconda3/envs/pyflink_39/lib/python3.9/site-packages/apache_beam/runners/worker/data_plane.py\", line 652, in _read_inputs\n",
      "    return self._next()\n",
      "  File \"/home/fred/anaconda3/envs/pyflink_39/lib/python3.9/site-packages/grpc/_channel.py\", line 967, in _next\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/fred/anaconda3/envs/pyflink_39/lib/python3.9/site-packages/apache_beam/runners/worker/data_plane.py\", line 669, in <lambda>\n",
      "    self.run()\n",
      "  File \"/home/fred/anaconda3/envs/pyflink_39/lib/python3.9/threading.py\", line 917, in run\n",
      "Exception in thread read_grpc_client_inputs:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/fred/anaconda3/envs/pyflink_39/lib/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/fred/anaconda3/envs/pyflink_39/lib/python3.9/threading.py\", line 917, in run\n",
      "    raise self\n",
      "grpc._channel._MultiThreadedRendezvous: <_MultiThreadedRendezvous of RPC that terminated with:\n",
      "\tstatus = StatusCode.CANCELLED\n",
      "\tdetails = \"Multiplexer hanging up\"\n",
      "\tdebug_error_string = \"UNKNOWN:Error received from peer ipv6:%5B::1%5D:37203 {grpc_message:\"Multiplexer hanging up\", grpc_status:1, created_time:\"2023-11-22T13:43:02.488956279+08:00\"}\"\n",
      ">\n",
      "    target=lambda: self._read_inputs(elements_iterator),\n",
      "  File \"/home/fred/anaconda3/envs/pyflink_39/lib/python3.9/site-packages/apache_beam/runners/worker/data_plane.py\", line 652, in _read_inputs\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/fred/anaconda3/envs/pyflink_39/lib/python3.9/site-packages/apache_beam/runners/worker/data_plane.py\", line 669, in <lambda>\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/fred/anaconda3/envs/pyflink_39/lib/python3.9/site-packages/apache_beam/runners/worker/data_plane.py\", line 669, in <lambda>\n",
      "    for elements in elements_iterator:\n",
      "  File \"/home/fred/anaconda3/envs/pyflink_39/lib/python3.9/site-packages/grpc/_channel.py\", line 541, in __next__\n",
      "    return self._next()\n",
      "  File \"/home/fred/anaconda3/envs/pyflink_39/lib/python3.9/site-packages/grpc/_channel.py\", line 967, in _next\n",
      "    for elements in elements_iterator:\n",
      "  File \"/home/fred/anaconda3/envs/pyflink_39/lib/python3.9/site-packages/grpc/_channel.py\", line 541, in __next__\n",
      "    target=lambda: self._read_inputs(elements_iterator),\n",
      "  File \"/home/fred/anaconda3/envs/pyflink_39/lib/python3.9/site-packages/apache_beam/runners/worker/data_plane.py\", line 652, in _read_inputs\n",
      "    target=lambda: self._read_inputs(elements_iterator),\n",
      "  File \"/home/fred/anaconda3/envs/pyflink_39/lib/python3.9/site-packages/apache_beam/runners/worker/data_plane.py\", line 652, in _read_inputs\n",
      "    for elements in elements_iterator:\n",
      "  File \"/home/fred/anaconda3/envs/pyflink_39/lib/python3.9/site-packages/grpc/_channel.py\", line 541, in __next__\n",
      "    return self._next()\n",
      "  File \"/home/fred/anaconda3/envs/pyflink_39/lib/python3.9/site-packages/grpc/_channel.py\", line 967, in _next\n",
      "    raise self\n",
      "grpc._channel._MultiThreadedRendezvous: <_MultiThreadedRendezvous of RPC that terminated with:\n",
      "\tstatus = StatusCode.CANCELLED\n",
      "\tdetails = \"Multiplexer hanging up\"\n",
      "\tdebug_error_string = \"UNKNOWN:Error received from peer ipv6:%5B::1%5D:45281 {created_time:\"2023-11-22T13:43:02.480390919+08:00\", grpc_status:1, grpc_message:\"Multiplexer hanging up\"}\"\n",
      ">\n",
      "    return self._next()\n",
      "  File \"/home/fred/anaconda3/envs/pyflink_39/lib/python3.9/site-packages/grpc/_channel.py\", line 967, in _next\n",
      "    for elements in elements_iterator:\n",
      "  File \"/home/fred/anaconda3/envs/pyflink_39/lib/python3.9/site-packages/grpc/_channel.py\", line 541, in __next__\n",
      "    raise self\n",
      "grpc._channel._MultiThreadedRendezvous: <_MultiThreadedRendezvous of RPC that terminated with:\n",
      "\tstatus = StatusCode.CANCELLED\n",
      "\tdetails = \"Multiplexer hanging up\"\n",
      "\tdebug_error_string = \"UNKNOWN:Error received from peer ipv6:%5B::1%5D:45389 {grpc_message:\"Multiplexer hanging up\", grpc_status:1, created_time:\"2023-11-22T13:43:02.485850177+08:00\"}\"\n",
      ">\n",
      "    return self._next()\n",
      "  File \"/home/fred/anaconda3/envs/pyflink_39/lib/python3.9/site-packages/grpc/_channel.py\", line 967, in _next\n",
      "    raise self\n",
      "grpc._channel._MultiThreadedRendezvous: <_MultiThreadedRendezvous of RPC that terminated with:\n",
      "\tstatus = StatusCode.CANCELLED\n",
      "\tdetails = \"Multiplexer hanging up\"\n",
      "\tdebug_error_string = \"UNKNOWN:Error received from peer ipv6:%5B::1%5D:41377 {created_time:\"2023-11-22T13:43:02.484030946+08:00\", grpc_status:1, grpc_message:\"Multiplexer hanging up\"}\"\n",
      ">\n",
      "    raise self\n",
      "grpc._channel._MultiThreadedRendezvous: <_MultiThreadedRendezvous of RPC that terminated with:\n",
      "\tstatus = StatusCode.CANCELLED\n",
      "\tdetails = \"Multiplexer hanging up\"\n",
      "\tdebug_error_string = \"UNKNOWN:Error received from peer ipv6:%5B::1%5D:37205 {created_time:\"2023-11-22T13:43:02.485778188+08:00\", grpc_status:1, grpc_message:\"Multiplexer hanging up\"}\"\n",
      ">\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7> ('Alice', 163.29999923706055)\n",
      "7> ('Alice', 163.29999923706055)\n",
      "7> ('Alice', 163.29999923706055)\n",
      "7> ('Alice', 163.29999923706055)\n",
      "2> ('Bob', 102.49999952316284)\n",
      "2> ('Bob', 102.49999952316284)\n",
      "2> ('Bob', 102.49999952316284)\n",
      "2> ('Bob', 102.49999952316284)\n"
     ]
    }
   ],
   "source": [
    "# 事件时间\n",
    "\n",
    "\n",
    "from pyflink.common import Time, WatermarkStrategy, Duration\n",
    "from pyflink.common.typeinfo import Types\n",
    "from pyflink.common.watermark_strategy import TimestampAssigner\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.datastream.functions import KeyedProcessFunction, RuntimeContext\n",
    "from pyflink.datastream.state import ValueStateDescriptor, StateTtlConfig\n",
    "\n",
    "\n",
    "# 定义Sum类，继承自KeyedProcessFunction\n",
    "class Sum(KeyedProcessFunction):\n",
    "\n",
    "    # 初始化函数\n",
    "    def __init__(self):\n",
    "        self.state = None\n",
    "\n",
    "    # 打开函数，获取状态描述符，设置状态TTL配置，并设置状态描述符的TTL配置\n",
    "    def open(self, runtime_context: RuntimeContext):\n",
    "        state_descriptor = ValueStateDescriptor(\"state\", Types.FLOAT())\n",
    "        state_ttl_config = StateTtlConfig \\\n",
    "            .new_builder(Time.seconds(1)) \\\n",
    "            .set_update_type(StateTtlConfig.UpdateType.OnReadAndWrite) \\\n",
    "            .disable_cleanup_in_background() \\\n",
    "            .build()\n",
    "        state_descriptor.enable_time_to_live(state_ttl_config)\n",
    "        self.state = runtime_context.get_state(state_descriptor)\n",
    "\n",
    "    # 处理元素函数，获取当前状态，更新状态，并注册一个2秒后的定时器\n",
    "    def process_element(self, value, ctx: 'KeyedProcessFunction.Context'):\n",
    "        # retrieve the current count\n",
    "        current = self.state.value()\n",
    "        if current is None:\n",
    "            current = 0\n",
    "\n",
    "        # update the state's count\n",
    "        current += value[2]\n",
    "        self.state.update(current)\n",
    "\n",
    "        # register an event time timer 2 seconds later\n",
    "        ctx.timer_service().register_event_time_timer(ctx.timestamp() + 2000)\n",
    "\n",
    "    # 定时器函数，获取当前状态，并输出\n",
    "    def on_timer(self, timestamp: int, ctx: 'KeyedProcessFunction.OnTimerContext'):\n",
    "        yield ctx.get_current_key(), self.state.value()\n",
    "\n",
    "\n",
    "# 定义MyTimestampAssigner类，继承自TimestampAssigner\n",
    "class MyTimestampAssigner(TimestampAssigner):\n",
    "\n",
    "    # 提取时间戳函数，根据value和record_timestamp获取时间戳\n",
    "    def extract_timestamp(self, value, record_timestamp: int) -> int:\n",
    "        return int(value[0])\n",
    "\n",
    "\n",
    "# 定义event_timer_timer_demo函数，获取执行环境，从集合中获取数据，设置时间戳和水位策略，并应用处理函数，提交执行\n",
    "def event_timer_timer_demo():\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()\n",
    "\n",
    "    ds = env.from_collection(\n",
    "        collection=[\n",
    "            (1000, 'Alice', 110.1),\n",
    "            (4000, 'Bob', 30.2),\n",
    "            (3000, 'Alice', 20.0),\n",
    "            (2000, 'Bob', 53.1),\n",
    "            (5000, 'Alice', 13.1),\n",
    "            (3000, 'Bob', 3.1),\n",
    "            (7000, 'Bob', 16.1),\n",
    "            (10000, 'Alice', 20.1)\n",
    "        ],\n",
    "        type_info=Types.TUPLE([Types.LONG(), Types.STRING(), Types.FLOAT()]))\n",
    "\n",
    "    ds = ds.assign_timestamps_and_watermarks(\n",
    "        WatermarkStrategy.for_bounded_out_of_orderness(Duration.of_seconds(2))\n",
    "                         .with_timestamp_assigner(MyTimestampAssigner()))\n",
    "\n",
    "    # apply the process function onto a keyed stream\n",
    "    ds.key_by(lambda value: value[1]) \\\n",
    "      .process(Sum()) \\\n",
    "      .print()\n",
    "\n",
    "    # submit for execution\n",
    "    env.execute()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    event_timer_timer_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 上手流数据\n",
    "\n",
    "## 流数据的来源\n",
    "\n",
    "网站数据采集，用户行为产生，购物网站、社交网站。\n",
    "传感器采集，物联网传输，科研观测、探测器。\n",
    "\n",
    "## 从本地数据生成流数据\n",
    "\n",
    "将本地文件读取，发送给Kafka，然后再从Kafka来读取。\n",
    "\n",
    "## 使用Docker搭建本地Kafka集群\n",
    "\n",
    "操作系统选择 Ubuntu 22.04.3   \n",
    "\n",
    "1. 安装 Docker 和 Docker Compose:\n",
    "```Bash\n",
    "sudo apt install Docker Docker-compose\n",
    "```\n",
    "2. 创建本地 `docker-compose.yml` 文件，其中包含以下内容：\n",
    "\n",
    "```yaml\n",
    "version: '3'\n",
    "services:\n",
    "  zookeeper:\n",
    "    image: 'bitnami/zookeeper:latest'\n",
    "    environment:\n",
    "      - ALLOW_ANONYMOUS_LOGIN=yes\n",
    "  kafka:\n",
    "    image: 'bitnami/kafka:latest'\n",
    "    ports:\n",
    "      - '9092:9092'\n",
    "    environment:\n",
    "      - KAFKA_ADVERTISED_HOST_NAME=localhost\n",
    "      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181\n",
    "      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092\n",
    "      - KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9092\n",
    "      - KAFKA_CREATE_TOPICS=test:1:1\n",
    "      - ALLOW_PLAINTEXT_LISTENER=yes\n",
    "    depends_on:\n",
    "      - zookeeper\n",
    "```\n",
    "\n",
    "3. 找到“docker-compose.yml”所在目录并运行以下命令：\n",
    "\n",
    "````Bash\n",
    "docker-compose up -d\n",
    "````\n",
    "\n",
    "这将运行一个包含 Zookeeper 实例和 Kafka 实例的本地 Kafka 集群，该集群将在本地主机的端口 9092 上运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent 1024 bytes to Kafka topic hamlet\n",
      "0.57% of the file sent\n",
      "Sent 1024 bytes to Kafka topic hamlet\n",
      "1.13% of the file sent\n",
      "Sent 1024 bytes to Kafka topic hamlet\n",
      "1.70% of the file sent\n",
      "Sent 1024 bytes to Kafka topic hamlet\n",
      "2.27% of the file sent\n",
      "Sent 1024 bytes to Kafka topic hamlet\n",
      "2.83% of the file sent\n",
      "Sent 1024 bytes to Kafka topic hamlet\n",
      "3.40% of the file sent\n",
      "Sent 1024 bytes to Kafka topic hamlet\n",
      "3.97% of the file sent\n",
      "Sent 1024 bytes to Kafka topic hamlet\n",
      "4.53% of the file sent\n",
      "Sent 1024 bytes to Kafka topic hamlet\n",
      "5.10% of the file sent\n",
      "Sent 1024 bytes to Kafka topic hamlet\n",
      "5.66% of the file sent\n",
      "Sent 1024 bytes to Kafka topic hamlet\n",
      "6.23% of the file sent\n",
      "Sent 1024 bytes to Kafka topic hamlet\n",
      "6.80% of the file sent\n",
      "Sent 1024 bytes to Kafka topic hamlet\n",
      "7.36% of the file sent\n",
      "Sent 1024 bytes to Kafka topic hamlet\n",
      "7.93% of the file sent\n",
      "Sent 1024 bytes to Kafka topic hamlet\n",
      "8.50% of the file sent\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/HP/Documents/GitHub/PyFlink-Tutorial/PyFlink简略探索.ipynb Cell 7\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/c/Users/HP/Documents/GitHub/PyFlink-Tutorial/PyFlink%E7%AE%80%E7%95%A5%E6%8E%A2%E7%B4%A2.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/c/Users/HP/Documents/GitHub/PyFlink-Tutorial/PyFlink%E7%AE%80%E7%95%A5%E6%8E%A2%E7%B4%A2.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39m# 调用函数，将hamlet.txt文件发送到Kafka的hamlet主题\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/c/Users/HP/Documents/GitHub/PyFlink-Tutorial/PyFlink%E7%AE%80%E7%95%A5%E6%8E%A2%E7%B4%A2.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m send_file_to_kafka(\u001b[39m\"\u001b[39;49m\u001b[39m./hamlet.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m,  \u001b[39m\"\u001b[39;49m\u001b[39mhamlet\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mlocalhost:9092\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/c/Users/HP/Documents/GitHub/PyFlink-Tutorial/PyFlink%E7%AE%80%E7%95%A5%E6%8E%A2%E7%B4%A2.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39m# 在此代码中，send_file_to_kafka 函数接受三个参数：file_path、topic 和 bootstrap_servers。\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/c/Users/HP/Documents/GitHub/PyFlink-Tutorial/PyFlink%E7%AE%80%E7%95%A5%E6%8E%A2%E7%B4%A2.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39m# file_path是本地文件的路径，topic是数据要发送到的Kafka主题，bootstrap_servers是Kafka集群的地址。\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/c/Users/HP/Documents/GitHub/PyFlink-Tutorial/PyFlink%E7%AE%80%E7%95%A5%E6%8E%A2%E7%B4%A2.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39m# 该函数使用with语句打开文件，读取其内容，并将它们作为流数据发送到指定的Kafka主题。\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/c/Users/HP/Documents/GitHub/PyFlink-Tutorial/PyFlink%E7%AE%80%E7%95%A5%E6%8E%A2%E7%B4%A2.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m# 发送过程中，打印出发送进度，并使用time.sleep方法暂停0.1秒来控制发送速率。\u001b[39;00m\n",
      "\u001b[1;32m/mnt/c/Users/HP/Documents/GitHub/PyFlink-Tutorial/PyFlink简略探索.ipynb Cell 7\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/c/Users/HP/Documents/GitHub/PyFlink-Tutorial/PyFlink%E7%AE%80%E7%95%A5%E6%8E%A2%E7%B4%A2.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpercent_sent\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m% of the file sent\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/c/Users/HP/Documents/GitHub/PyFlink-Tutorial/PyFlink%E7%AE%80%E7%95%A5%E6%8E%A2%E7%B4%A2.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m         \u001b[39m# 等待3秒\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/c/Users/HP/Documents/GitHub/PyFlink-Tutorial/PyFlink%E7%AE%80%E7%95%A5%E6%8E%A2%E7%B4%A2.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m         time\u001b[39m.\u001b[39;49msleep(\u001b[39m3\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/c/Users/HP/Documents/GitHub/PyFlink-Tutorial/PyFlink%E7%AE%80%E7%95%A5%E6%8E%A2%E7%B4%A2.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m# 获取用户输入\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/c/Users/HP/Documents/GitHub/PyFlink-Tutorial/PyFlink%E7%AE%80%E7%95%A5%E6%8E%A2%E7%B4%A2.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m user_input \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPress \u001b[39m\u001b[39m'\u001b[39m\u001b[39mc\u001b[39m\u001b[39m'\u001b[39m\u001b[39m to continue sending the file or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mq\u001b[39m\u001b[39m'\u001b[39m\u001b[39m to quit: \u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 使用 kafka-python 生成流的简单方法\n",
    "\n",
    "# 以下代码使用kafka-python模块将数据发送到本地Kafka集群。\n",
    "# 此代码打开一个名为 `hamlet.txt` 的文本文件，并将其内容作为流发送到指定的 Kafka 主题 `hamlet`：\n",
    "\n",
    "# 导入KafkaProducer模块\n",
    "from kafka import KafkaProducer\n",
    "# 导入time模块\n",
    "import time\n",
    "# 导入os模块\n",
    "import os\n",
    "\n",
    "# 定义一个函数，用于将文件发送到Kafka\n",
    "def send_file_to_kafka(file_path: str, topic: str, bootstrap_servers: str):\n",
    "    # 创建一个KafkaProducer实例，用于发送消息\n",
    "    producer = KafkaProducer(bootstrap_servers=bootstrap_servers)\n",
    "    # 获取文件大小\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    # 循环发送文件\n",
    "    while True:\n",
    "        # 打开文件\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            # 循环读取文件\n",
    "            while True:\n",
    "                # 读取文件内容\n",
    "                data = f.read(1024)\n",
    "                # 如果没有内容，则跳出循环\n",
    "                if not data:\n",
    "                    break\n",
    "                # 将文件内容发送到Kafka\n",
    "                producer.send(topic, data)\n",
    "                # 计算发送的字节数\n",
    "                bytes_sent = len(data)\n",
    "                # 打印发送的字节数\n",
    "                print(f\"Sent {bytes_sent} bytes to Kafka topic {topic}\")\n",
    "                # 计算发送的百分比\n",
    "                percent_sent = (f.tell() / file_size) * 100\n",
    "                # 打印发送的百分比\n",
    "                print(f\"{percent_sent:.2f}% of the file sent\")\n",
    "                # 等待3秒\n",
    "                time.sleep(3)\n",
    "        # 获取用户输入\n",
    "        user_input = input(\"Press 'c' to continue sending the file or 'q' to quit: \")\n",
    "        # 如果用户输入q，则退出循环\n",
    "        if user_input == \"q\":\n",
    "            break\n",
    "# 调用函数，将hamlet.txt文件发送到Kafka的hamlet主题\n",
    "send_file_to_kafka(\"./hamlet.txt\",  \"hamlet\", \"localhost:9092\")\n",
    "# 在此代码中，send_file_to_kafka 函数接受三个参数：file_path、topic 和 bootstrap_servers。\n",
    "# file_path是本地文件的路径，topic是数据要发送到的Kafka主题，bootstrap_servers是Kafka集群的地址。\n",
    "# 该函数使用with语句打开文件，读取其内容，并将它们作为流数据发送到指定的Kafka主题。\n",
    "# 发送过程中，打印出发送进度，并使用time.sleep方法暂停0.1秒来控制发送速率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/HP/Documents/GitHub/PyFlink-Tutorial/PyFlink简略探索.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/c/Users/HP/Documents/GitHub/PyFlink-Tutorial/PyFlink%E7%AE%80%E7%95%A5%E6%8E%A2%E7%B4%A2.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m consumer \u001b[39m=\u001b[39m KafkaConsumer(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/c/Users/HP/Documents/GitHub/PyFlink-Tutorial/PyFlink%E7%AE%80%E7%95%A5%E6%8E%A2%E7%B4%A2.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m# 指定要读取的消息主题\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/c/Users/HP/Documents/GitHub/PyFlink-Tutorial/PyFlink%E7%AE%80%E7%95%A5%E6%8E%A2%E7%B4%A2.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mhamlet\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/c/Users/HP/Documents/GitHub/PyFlink-Tutorial/PyFlink%E7%AE%80%E7%95%A5%E6%8E%A2%E7%B4%A2.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     value_deserializer\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/c/Users/HP/Documents/GitHub/PyFlink-Tutorial/PyFlink%E7%AE%80%E7%95%A5%E6%8E%A2%E7%B4%A2.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/c/Users/HP/Documents/GitHub/PyFlink-Tutorial/PyFlink%E7%AE%80%E7%95%A5%E6%8E%A2%E7%B4%A2.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# 循环读取Kafka主题中的消息，并打印消息长度和消息内容\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/c/Users/HP/Documents/GitHub/PyFlink-Tutorial/PyFlink%E7%AE%80%E7%95%A5%E6%8E%A2%E7%B4%A2.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mfor\u001b[39;00m message \u001b[39min\u001b[39;00m consumer:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/c/Users/HP/Documents/GitHub/PyFlink-Tutorial/PyFlink%E7%AE%80%E7%95%A5%E6%8E%A2%E7%B4%A2.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReceived \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(message\u001b[39m.\u001b[39mvalue)\u001b[39m}\u001b[39;00m\u001b[39m bytes from Kafka topic \u001b[39m\u001b[39m{\u001b[39;00mmessage\u001b[39m.\u001b[39mtopic\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/c/Users/HP/Documents/GitHub/PyFlink-Tutorial/PyFlink%E7%AE%80%E7%95%A5%E6%8E%A2%E7%B4%A2.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmessage\u001b[39m.\u001b[39mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pyflink_39/lib/python3.9/site-packages/kafka/consumer/group.py:1193\u001b[0m, in \u001b[0;36mKafkaConsumer.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1191\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext_v1()\n\u001b[1;32m   1192\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1193\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext_v2()\n",
      "File \u001b[0;32m~/anaconda3/envs/pyflink_39/lib/python3.9/site-packages/kafka/consumer/group.py:1201\u001b[0m, in \u001b[0;36mKafkaConsumer.next_v2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_message_generator_v2()\n\u001b[1;32m   1200\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1201\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator)\n\u001b[1;32m   1202\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m   1203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pyflink_39/lib/python3.9/site-packages/kafka/consumer/group.py:1116\u001b[0m, in \u001b[0;36mKafkaConsumer._message_generator_v2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_message_generator_v2\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1115\u001b[0m     timeout_ms \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m \u001b[39m*\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_consumer_timeout \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mtime())\n\u001b[0;32m-> 1116\u001b[0m     record_map \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpoll(timeout_ms\u001b[39m=\u001b[39;49mtimeout_ms, update_offsets\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m   1117\u001b[0m     \u001b[39mfor\u001b[39;00m tp, records \u001b[39min\u001b[39;00m six\u001b[39m.\u001b[39miteritems(record_map):\n\u001b[1;32m   1118\u001b[0m         \u001b[39m# Generators are stateful, and it is possible that the tp / records\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m         \u001b[39m# here may become stale during iteration -- i.e., we seek to a\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m         \u001b[39m# different offset, pause consumption, or lose assignment.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m         \u001b[39mfor\u001b[39;00m record \u001b[39min\u001b[39;00m records:\n\u001b[1;32m   1122\u001b[0m             \u001b[39m# is_fetchable(tp) should handle assignment changes and offset\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m             \u001b[39m# resets; for all other changes (e.g., seeks) we'll rely on the\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m             \u001b[39m# outer function destroying the existing iterator/generator\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m             \u001b[39m# via self._iterator = None\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pyflink_39/lib/python3.9/site-packages/kafka/consumer/group.py:655\u001b[0m, in \u001b[0;36mKafkaConsumer.poll\u001b[0;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    653\u001b[0m remaining \u001b[39m=\u001b[39m timeout_ms\n\u001b[1;32m    654\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 655\u001b[0m     records \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll_once(remaining, max_records, update_offsets\u001b[39m=\u001b[39;49mupdate_offsets)\n\u001b[1;32m    656\u001b[0m     \u001b[39mif\u001b[39;00m records:\n\u001b[1;32m    657\u001b[0m         \u001b[39mreturn\u001b[39;00m records\n",
      "File \u001b[0;32m~/anaconda3/envs/pyflink_39/lib/python3.9/site-packages/kafka/consumer/group.py:675\u001b[0m, in \u001b[0;36mKafkaConsumer._poll_once\u001b[0;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_poll_once\u001b[39m(\u001b[39mself\u001b[39m, timeout_ms, max_records, update_offsets\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    666\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Do one round of polling. In addition to checking for new data, this does\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \u001b[39m    any needed heart-beating, auto-commits, and offset updates.\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[39m        dict: Map of topic to list of records (may be empty).\u001b[39;00m\n\u001b[1;32m    674\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_coordinator\u001b[39m.\u001b[39;49mpoll()\n\u001b[1;32m    677\u001b[0m     \u001b[39m# Fetch positions if we have partitions we're subscribed to that we\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39m# don't know the offset for\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_subscription\u001b[39m.\u001b[39mhas_all_fetch_positions():\n",
      "File \u001b[0;32m~/anaconda3/envs/pyflink_39/lib/python3.9/site-packages/kafka/coordinator/consumer.py:289\u001b[0m, in \u001b[0;36mConsumerCoordinator.poll\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    286\u001b[0m             metadata_update \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39mcluster\u001b[39m.\u001b[39mrequest_update()\n\u001b[1;32m    287\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39mpoll(future\u001b[39m=\u001b[39mmetadata_update)\n\u001b[0;32m--> 289\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mensure_active_group()\n\u001b[1;32m    291\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpoll_heartbeat()\n\u001b[1;32m    293\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_auto_commit_offsets_async()\n",
      "File \u001b[0;32m~/anaconda3/envs/pyflink_39/lib/python3.9/site-packages/kafka/coordinator/base.py:407\u001b[0m, in \u001b[0;36mBaseCoordinator.ensure_active_group\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m     future \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjoin_future\n\u001b[0;32m--> 407\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49mpoll(future\u001b[39m=\u001b[39;49mfuture)\n\u001b[1;32m    409\u001b[0m \u001b[39mif\u001b[39;00m future\u001b[39m.\u001b[39msucceeded():\n\u001b[1;32m    410\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_on_join_complete(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generation\u001b[39m.\u001b[39mgeneration_id,\n\u001b[1;32m    411\u001b[0m                            \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generation\u001b[39m.\u001b[39mmember_id,\n\u001b[1;32m    412\u001b[0m                            \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generation\u001b[39m.\u001b[39mprotocol,\n\u001b[1;32m    413\u001b[0m                            future\u001b[39m.\u001b[39mvalue)\n",
      "File \u001b[0;32m~/anaconda3/envs/pyflink_39/lib/python3.9/site-packages/kafka/client_async.py:602\u001b[0m, in \u001b[0;36mKafkaClient.poll\u001b[0;34m(self, timeout_ms, future)\u001b[0m\n\u001b[1;32m    599\u001b[0m             timeout \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(timeout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mretry_backoff_ms\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    600\u001b[0m         timeout \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39m0\u001b[39m, timeout)  \u001b[39m# avoid negative timeouts\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout \u001b[39m/\u001b[39;49m \u001b[39m1000\u001b[39;49m)\n\u001b[1;32m    604\u001b[0m \u001b[39m# called without the lock to avoid deadlock potential\u001b[39;00m\n\u001b[1;32m    605\u001b[0m \u001b[39m# if handlers need to acquire locks\u001b[39;00m\n\u001b[1;32m    606\u001b[0m responses\u001b[39m.\u001b[39mextend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fire_pending_completed_requests())\n",
      "File \u001b[0;32m~/anaconda3/envs/pyflink_39/lib/python3.9/site-packages/kafka/client_async.py:634\u001b[0m, in \u001b[0;36mKafkaClient._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_register_send_sockets()\n\u001b[1;32m    633\u001b[0m start_select \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 634\u001b[0m ready \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    635\u001b[0m end_select \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sensors:\n",
      "File \u001b[0;32m~/anaconda3/envs/pyflink_39/lib/python3.9/selectors.py:469\u001b[0m, in \u001b[0;36mEpollSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    467\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    468\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 469\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout, max_ev)\n\u001b[1;32m    470\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    471\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 使用 kafka-python 展现流数据的简单方法\n",
    "\n",
    "from kafka import KafkaConsumer\n",
    "\n",
    "# 创建一个KafkaConsumer实例，用于从Kafka主题中读取消息\n",
    "consumer = KafkaConsumer(\n",
    "    # 指定要读取的消息主题\n",
    "    \"hamlet\",\n",
    "    # 指定Kafka服务器的地址和端口\n",
    "    bootstrap_servers=[\"localhost:9092\"],\n",
    "    # 指定当消费者重新启动时，它应该从哪个偏移量开始读取消息\n",
    "    auto_offset_reset=\"earliest\",\n",
    "    # 指定是否在消费者处理消息时，应该提交偏移量\n",
    "    enable_auto_commit=True,\n",
    "    # 指定消费者组，用于提交偏移量\n",
    "    group_id=\"my-group\",\n",
    "    # 指定消息的解码方式\n",
    "    value_deserializer=lambda x: x.decode(\"utf-8\")\n",
    ")\n",
    "\n",
    "# 循环读取Kafka主题中的消息，并打印消息长度和消息内容\n",
    "for message in consumer:\n",
    "    print(f\"Received {len(message.value)} bytes from Kafka topic {message.topic}\")\n",
    "    print(f\"{message.value}\")\n",
    "\n",
    "# 在上面的代码中，我们使用`KafkaConsumer`类来创建一个消费者对象。\n",
    "# 我们将 `hamlet` 作为主题名称传递给构造函数。\n",
    "# 我们还传递 `localhost:9092` 作为引导服务器的地址。\n",
    "# 我们使用 `value_deserializer` 参数来解码从 Kafka 主题收到的消息。\n",
    "# 我们使用 `for` 循环来迭代消费者对象，并使用 `print` 函数来打印消息的内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start reading data from kafka\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o0.execute.\n: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.\n\tat org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)\n\tat org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)\n\tat java.base/java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:646)\n\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)\n\tat java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2147)\n\tat org.apache.flink.runtime.rpc.pekko.PekkoInvocationHandler.lambda$invokeRpc$1(PekkoInvocationHandler.java:268)\n\tat java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:863)\n\tat java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:841)\n\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)\n\tat java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2147)\n\tat org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1267)\n\tat org.apache.flink.runtime.concurrent.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)\n\tat org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)\n\tat org.apache.flink.runtime.concurrent.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)\n\tat java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:863)\n\tat java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:841)\n\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)\n\tat java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2147)\n\tat org.apache.flink.runtime.concurrent.pekko.ScalaFutureUtils$1.onComplete(ScalaFutureUtils.java:47)\n\tat org.apache.pekko.dispatch.OnComplete.internal(Future.scala:310)\n\tat org.apache.pekko.dispatch.OnComplete.internal(Future.scala:307)\n\tat org.apache.pekko.dispatch.japi$CallbackBridge.apply(Future.scala:234)\n\tat org.apache.pekko.dispatch.japi$CallbackBridge.apply(Future.scala:231)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat org.apache.flink.runtime.concurrent.pekko.ScalaFutureUtils$DirectExecutionContext.execute(ScalaFutureUtils.java:65)\n\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n\tat org.apache.pekko.pattern.PromiseActorRef.$bang(AskSupport.scala:629)\n\tat org.apache.pekko.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:34)\n\tat org.apache.pekko.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:33)\n\tat scala.concurrent.Future.$anonfun$andThen$1(Future.scala:536)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat org.apache.pekko.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:73)\n\tat org.apache.pekko.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:110)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n\tat org.apache.pekko.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:110)\n\tat org.apache.pekko.dispatch.TaskInvocation.run(AbstractDispatcher.scala:59)\n\tat org.apache.pekko.dispatch.ForkJoinExecutorConfigurator$PekkoForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:57)\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)\nCaused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy\n\tat org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:176)\n\tat org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:107)\n\tat org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:285)\n\tat org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:276)\n\tat org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:269)\n\tat org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:764)\n\tat org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:741)\n\tat org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:83)\n\tat org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:488)\n\tat jdk.internal.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.lambda$handleRpcInvocation$1(PekkoRpcActor.java:309)\n\tat org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)\n\tat org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcInvocation(PekkoRpcActor.java:307)\n\tat org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcMessage(PekkoRpcActor.java:222)\n\tat org.apache.flink.runtime.rpc.pekko.FencedPekkoRpcActor.handleRpcMessage(FencedPekkoRpcActor.java:85)\n\tat org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleMessage(PekkoRpcActor.java:168)\n\tat org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:33)\n\tat org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:29)\n\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\tat org.apache.pekko.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:29)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)\n\tat org.apache.pekko.actor.Actor.aroundReceive(Actor.scala:547)\n\tat org.apache.pekko.actor.Actor.aroundReceive$(Actor.scala:545)\n\tat org.apache.pekko.actor.AbstractActor.aroundReceive(AbstractActor.scala:229)\n\tat org.apache.pekko.actor.ActorCell.receiveMessage(ActorCell.scala:590)\n\tat org.apache.pekko.actor.ActorCell.invoke(ActorCell.scala:557)\n\tat org.apache.pekko.dispatch.Mailbox.processMailbox(Mailbox.scala:280)\n\tat org.apache.pekko.dispatch.Mailbox.run(Mailbox.scala:241)\n\tat org.apache.pekko.dispatch.Mailbox.exec(Mailbox.scala:253)\n\t... 5 more\nCaused by: org.apache.flink.kafka.shaded.org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m     env\u001b[38;5;241m.\u001b[39mexecute()\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 42\u001b[0m     \u001b[43mread_from_kafka\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 39\u001b[0m, in \u001b[0;36mread_from_kafka\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m env\u001b[38;5;241m.\u001b[39madd_source(kafka_consumer)\u001b[38;5;241m.\u001b[39mprint()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# 执行执行环境\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pyflink_39/lib/python3.9/site-packages/pyflink/datastream/stream_execution_environment.py:773\u001b[0m, in \u001b[0;36mStreamExecutionEnvironment.execute\u001b[0;34m(self, job_name)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;124;03mTriggers the program execution. The environment will execute all parts of\u001b[39;00m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;124;03mthe program that have resulted in a \"sink\" operation. Sink operations are\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;124;03m:return: The result of the job execution, containing elapsed time and accumulators.\u001b[39;00m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    772\u001b[0m j_stream_graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_stream_graph(clear_transformations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, job_name\u001b[38;5;241m=\u001b[39mjob_name)\n\u001b[0;32m--> 773\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m JobExecutionResult(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_j_stream_execution_environment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mj_stream_graph\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pyflink_39/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/pyflink_39/lib/python3.9/site-packages/pyflink/util/exceptions.py:146\u001b[0m, in \u001b[0;36mcapture_java_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyflink\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjava_gateway\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_gateway\n",
      "File \u001b[0;32m~/anaconda3/envs/pyflink_39/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o0.execute.\n: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.\n\tat org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)\n\tat org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)\n\tat java.base/java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:646)\n\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)\n\tat java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2147)\n\tat org.apache.flink.runtime.rpc.pekko.PekkoInvocationHandler.lambda$invokeRpc$1(PekkoInvocationHandler.java:268)\n\tat java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:863)\n\tat java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:841)\n\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)\n\tat java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2147)\n\tat org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1267)\n\tat org.apache.flink.runtime.concurrent.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)\n\tat org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)\n\tat org.apache.flink.runtime.concurrent.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)\n\tat java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:863)\n\tat java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:841)\n\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)\n\tat java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2147)\n\tat org.apache.flink.runtime.concurrent.pekko.ScalaFutureUtils$1.onComplete(ScalaFutureUtils.java:47)\n\tat org.apache.pekko.dispatch.OnComplete.internal(Future.scala:310)\n\tat org.apache.pekko.dispatch.OnComplete.internal(Future.scala:307)\n\tat org.apache.pekko.dispatch.japi$CallbackBridge.apply(Future.scala:234)\n\tat org.apache.pekko.dispatch.japi$CallbackBridge.apply(Future.scala:231)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat org.apache.flink.runtime.concurrent.pekko.ScalaFutureUtils$DirectExecutionContext.execute(ScalaFutureUtils.java:65)\n\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n\tat org.apache.pekko.pattern.PromiseActorRef.$bang(AskSupport.scala:629)\n\tat org.apache.pekko.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:34)\n\tat org.apache.pekko.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:33)\n\tat scala.concurrent.Future.$anonfun$andThen$1(Future.scala:536)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat org.apache.pekko.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:73)\n\tat org.apache.pekko.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:110)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n\tat org.apache.pekko.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:110)\n\tat org.apache.pekko.dispatch.TaskInvocation.run(AbstractDispatcher.scala:59)\n\tat org.apache.pekko.dispatch.ForkJoinExecutorConfigurator$PekkoForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:57)\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)\nCaused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy\n\tat org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:176)\n\tat org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:107)\n\tat org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:285)\n\tat org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:276)\n\tat org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:269)\n\tat org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:764)\n\tat org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:741)\n\tat org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:83)\n\tat org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:488)\n\tat jdk.internal.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.lambda$handleRpcInvocation$1(PekkoRpcActor.java:309)\n\tat org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)\n\tat org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcInvocation(PekkoRpcActor.java:307)\n\tat org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcMessage(PekkoRpcActor.java:222)\n\tat org.apache.flink.runtime.rpc.pekko.FencedPekkoRpcActor.handleRpcMessage(FencedPekkoRpcActor.java:85)\n\tat org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleMessage(PekkoRpcActor.java:168)\n\tat org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:33)\n\tat org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:29)\n\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\tat org.apache.pekko.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:29)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)\n\tat org.apache.pekko.actor.Actor.aroundReceive(Actor.scala:547)\n\tat org.apache.pekko.actor.Actor.aroundReceive$(Actor.scala:545)\n\tat org.apache.pekko.actor.AbstractActor.aroundReceive(AbstractActor.scala:229)\n\tat org.apache.pekko.actor.ActorCell.receiveMessage(ActorCell.scala:590)\n\tat org.apache.pekko.actor.ActorCell.invoke(ActorCell.scala:557)\n\tat org.apache.pekko.dispatch.Mailbox.processMailbox(Mailbox.scala:280)\n\tat org.apache.pekko.dispatch.Mailbox.run(Mailbox.scala:241)\n\tat org.apache.pekko.dispatch.Mailbox.exec(Mailbox.scala:253)\n\t... 5 more\nCaused by: org.apache.flink.kafka.shaded.org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from pyflink.table import StreamTableEnvironment\n",
    "from pyflink.common import WatermarkStrategy, Encoder, Types\n",
    "from pyflink.datastream import StreamExecutionEnvironment, RuntimeExecutionMode\n",
    "from pyflink.datastream.connectors.file_system import FileSource, StreamFormat, FileSink, OutputFileConfig, RollingPolicy\n",
    "from pyflink.common import Types, SimpleStringSchema\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.datastream.connectors.kafka import FlinkKafkaProducer, FlinkKafkaConsumer\n",
    "\n",
    "# 定义split函数，用于将字符串按照空格分割\n",
    "def split(line):\n",
    "    # 使用yield from语法，将line按照空格分割，并返回分割后的字符串\n",
    "    yield from line.split()\n",
    "\n",
    "# 定义read_from_kafka函数，用于从Kafka中读取数据\n",
    "def read_from_kafka():\n",
    "    # 获取StreamExecutionEnvironment实例\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()    \n",
    "    # 添加FlinkKafkaConsumer的jar包\n",
    "    env.add_jars(\"file:///home/hadoop/Desktop/PyFlink-Tutorial/flink-sql-connector-kafka-3.1-SNAPSHOT.jar\")\n",
    "    # 打印信息\n",
    "    print(\"start reading data from kafka\")\n",
    "    # 创建FlinkKafkaConsumer实例，用于从Kafka中读取数据\n",
    "    kafka_consumer = FlinkKafkaConsumer(\n",
    "        topics='hamlet', # The topic to consume messages from\n",
    "        deserialization_schema= SimpleStringSchema('UTF-8'), # The schema to deserialize messages\n",
    "        properties={'bootstrap.servers': 'localhost:9092', 'group.id': 'my-group'} # The Kafka broker address and consumer group ID\n",
    "    )\n",
    "    # 从最早的记录开始读取数据\n",
    "    kafka_consumer.set_start_from_earliest()\n",
    "    # 将FlinkKafkaConsumer实例添加到StreamExecutionEnvironment实例中，并打印读取到的数据\n",
    "    env.add_source(kafka_consumer).print()\n",
    "    # 执行StreamExecutionEnvironment实例\n",
    "    env.execute()\n",
    "\n",
    "# 调用read_from_kafka函数\n",
    "if __name__ == '__main__':\n",
    "    # 设置日志级别\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=\"%(message)s\")\n",
    "    # 调用read_from_kafka函数\n",
    "    read_from_kafka()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于 PyFlink 的简单词频统计\n",
    "\n",
    "# 导入os模块\n",
    "import os\n",
    "# 导入re模块\n",
    "import re\n",
    "# 导入Counter模块\n",
    "from collections import Counter\n",
    "# 导入StreamTableEnvironment模块\n",
    "from pyflink.table import StreamTableEnvironment\n",
    "# 导入StreamExecutionEnvironment模块\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "# 导入FlinkKafkaConsumer模块\n",
    "from pyflink.datastream.connectors.kafka import FlinkKafkaConsumer\n",
    "# 导入SimpleStringSchema模块\n",
    "from pyflink.common import SimpleStringSchema\n",
    "\n",
    "# 定义去除标点符号的函数\n",
    "def remove_punctuation(text):\n",
    "    # 使用正则表达式去除标点符号\n",
    "    return re.sub(r'[^\\w\\s]','',text)\n",
    "\n",
    "# 定义统计单词的函数\n",
    "def count_words(text):\n",
    "    # 将文本按空格分割成单词列表\n",
    "    words = text.split()\n",
    "    # 使用Counter模块统计单词出现次数\n",
    "    return Counter(words)\n",
    "\n",
    "# 定义从Kafka读取数据的函数\n",
    "def read_from_kafka():\n",
    "    # 获取StreamExecutionEnvironment实例\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()    \n",
    "    # 添加flink-sql-connector-kafka-3.1-SNAPSHOT.jar包\n",
    "    env.add_jars(\"file:///home/hadoop/Desktop/PyFlink-Tutorial/flink-sql-connector-kafka-3.1-SNAPSHOT.jar\")\n",
    "    # 打印从Kafka读取数据的信息\n",
    "    print(\"start reading data from kafka\")\n",
    "    # 创建FlinkKafkaConsumer实例\n",
    "    kafka_consumer = FlinkKafkaConsumer(\n",
    "        topics='hamlet', # The topic to consume messages from\n",
    "        deserialization_schema= SimpleStringSchema('UTF-8'), # The schema to deserialize messages\n",
    "        properties={'bootstrap.servers': 'localhost:9092', 'group.id': 'my-group'} # The Kafka broker address and consumer group ID\n",
    "    )\n",
    "    # 从最早的记录开始读取数据\n",
    "    kafka_consumer.set_start_from_earliest()\n",
    "    # 将FlinkKafkaConsumer实例添加到StreamExecutionEnvironment实例中\n",
    "    stream = env.add_source(kafka_consumer)\n",
    "    # 将StreamExecutionEnvironment实例中的数据映射为去除标点符号的文本\n",
    "    stream_remove_punctuation = stream.map(lambda x: remove_punctuation(x))\n",
    "    # 将去除标点符号的文本映射为统计单词的文本\n",
    "    stream_count_words = stream_remove_punctuation.map(lambda x: count_words(x))\n",
    "    # 打印统计单词的文本\n",
    "    stream_count_words.print()\n",
    "    # 执行StreamExecutionEnvironment实例\n",
    "    env.execute()\n",
    "\n",
    "# 调用read_from_kafka函数\n",
    "read_from_kafka()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提供更详细信息的词频统计\n",
    "\n",
    "# 导入 argparse、io、json、logging、os、pandas、re、Counter、StringIO、FlinkKafkaConsumer、StreamExecutionEnvironment、DataTypes、EnvironmentSettings、FormatDescriptor、Schema、StreamTableEnvironment、TableEnvironment、udf 模块\n",
    "import argparse\n",
    "import io\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from io import StringIO\n",
    "from pyflink.common import SimpleStringSchema, Time\n",
    "from pyflink.datastream.connectors.kafka import FlinkKafkaConsumer\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.table import (DataTypes, EnvironmentSettings, FormatDescriptor,\n",
    "                           Schema, StreamTableEnvironment, TableDescriptor,\n",
    "                           TableEnvironment, udf)\n",
    "from pyflink.table.expressions import col, lit\n",
    "\n",
    "# 定义去除标点符号的函数\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]','',text)\n",
    "\n",
    "# 定义计算字节数的函数\n",
    "def count_bytes(text):\n",
    "    return len(text.encode('utf-8'))\n",
    "\n",
    "# 定义计算单词数量的函数\n",
    "def count_words(text):\n",
    "    words = text.split()\n",
    "    result = dict(Counter(words))\n",
    "    max_word = max(result, key=result.get)\n",
    "    return {'total_bytes': count_bytes(text), 'total_words': len(words), 'most_frequent_word': max_word, 'most_frequent_word_count': result[max_word]}\n",
    "\n",
    "# 定义从Kafka读取数据的函数\n",
    "def read_from_kafka():\n",
    "    # 获取StreamExecutionEnvironment实例\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()  \n",
    "    # 添加flink-sql-connector-kafka-3.1-SNAPSHOT.jar包\n",
    "    env.add_jars(\"file:///home/hadoop/Desktop/PyFlink-Tutorial/flink-sql-connector-kafka-3.1-SNAPSHOT.jar\")\n",
    "    print(\"start reading data from kafka\")\n",
    "    # 创建FlinkKafkaConsumer实例，指定主题、反序列化函数、配置参数\n",
    "    kafka_consumer = FlinkKafkaConsumer(\n",
    "        topics='hamlet', \n",
    "        deserialization_schema= SimpleStringSchema('UTF-8'), \n",
    "        properties={'bootstrap.servers': 'localhost:9092', 'group.id': 'my-group'} \n",
    "    )\n",
    "    # 从最早的日志开始读取\n",
    "    kafka_consumer.set_start_from_earliest()\n",
    "    # 将Kafka日志流转换为流表\n",
    "    stream_original_text = env.add_source(kafka_consumer)\n",
    "    # 对流表中的每一行进行去除标点符号操作\n",
    "    stream_remove_punctuation = stream_original_text.map(lambda x: remove_punctuation(x))\n",
    "    # 对流表中的每一行进行计算单词数量的操作\n",
    "    stream_count_words = stream_remove_punctuation.map(lambda x: count_words(x))\n",
    "    # 将流表中的每一行打印出来\n",
    "    stream_count_words.print()\n",
    "    # 执行流计算\n",
    "    env.execute()\n",
    "read_from_kafka()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从文本到数据，上传CSV\n",
    "\n",
    "假设我们得到一个“data.csv”文件，其中包含很多。\n",
    "我们首先使用以下代码将“CSV”文件转换为“Kafka Stream”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一个简单的 CSV 流生成器\n",
    "# 以下代码使用kafka-python模块将数据发送到本地Kafka集群。\n",
    "\n",
    "from kafka import KafkaProducer\n",
    "import time\n",
    "import os\n",
    "import chardet\n",
    "\n",
    "def send_file_to_kafka(file_path: str, topic: str, bootstrap_servers: str):\n",
    "    # 创建KafkaProducer实例，用于发送消息\n",
    "    producer = KafkaProducer(bootstrap_servers=bootstrap_servers)\n",
    "    # 获取文件大小\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    # 检测文件编码\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        result = chardet.detect(f.read())\n",
    "        encoding = result[\"encoding\"]\n",
    "\n",
    "    # 读取文件内容\n",
    "    with open(file_path, \"r\", encoding=encoding) as f:\n",
    "        lines_total = len(f.readlines())\n",
    "    lines_send = 0\n",
    "    # 循环发送文件内容\n",
    "    while True:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            while True:\n",
    "                data = f.readlines(10)\n",
    "                if not data:\n",
    "                    break\n",
    "                data_str = str(data)\n",
    "                data_bytes = data_str.encode()\n",
    "                # 发送消息\n",
    "                producer.send(topic, data_bytes)\n",
    "                lines_send += 10\n",
    "                # 计算已发送的百分比\n",
    "                percent_sent = (lines_send / lines_total) * 100                \n",
    "                bytes_sent = len(data_bytes)\n",
    "                print(f\"Sent {bytes_sent} bytes {topic} {percent_sent:.2f}% sent\")\n",
    "                # 每3秒检查一次\n",
    "                time.sleep(3)\n",
    "        # 询问是否继续发送\n",
    "        user_input = input(\"Press 'c' to continue sending the file or 'q' to quit: \")\n",
    "        if user_input == \"q\":\n",
    "            break\n",
    "# 调用函数，将文件发送到Kafka\n",
    "send_file_to_kafka(\"./data.csv\",  \"data\", \"localhost:9092\")\n",
    "\n",
    "# 在这个代码中，send_file_to_kafka 函数接受三个参数：file_path、topic 和 bootstrap_servers。\n",
    "# file_path 是本地文件的路径，topic 是要将数据发送到的 Kafka 主题，bootstrap_servers 是 Kafka 集群的地址。\n",
    "# 该函数使用 with 语句打开文件，读取其内容，并将其作为流数据发送到指定的 Kafka 主题。在发送过程中，它会打印出传输进度，并使用 time.sleep 方法暂停 3 秒以控制发送速率。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 输出CSV数据\n",
    "\n",
    "Data_CSV_Stream_Shower.py 是一个使用 DataStream 处理 CSV 文件的 Python 脚本。实际上，下面的代码使用 re 函数。\n",
    "但这不重要，只是对从 CSV 文件生成的 DataStream 再进行输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyflink.common import Types\n",
    "from pyflink.common.serialization import SimpleStringSchema\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.datastream.connectors.file_system import FileSink, FileSource, OutputFileConfig, RollingPolicy\n",
    "from pyflink.datastream.connectors.kafka import FlinkKafkaConsumer, FlinkKafkaProducer\n",
    "from pyflink.datastream.formats.csv import CsvRowDeserializationSchema, CsvRowSerializationSchema\n",
    "from pyflink.datastream.state import ValueStateDescriptor\n",
    "from pyflink.table import StreamTableEnvironment\n",
    "\n",
    "\n",
    "# 定义一个函数，从Kafka读取数据\n",
    "def read_from_kafka():\n",
    "    # 获取当前的StreamExecutionEnvironment\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()    \n",
    "    # 添加Flink Kafka连接器JAR包\n",
    "    env.add_jars(\"file:///home/hadoop/Desktop/PyFlink-Tutorial/flink-sql-connector-kafka-3.1-SNAPSHOT.jar\")\n",
    "    # 打印信息\n",
    "    print(\"start reading data from kafka\")\n",
    "    # 创建一个FlinkKafkaConsumer，用于从Kafka读取数据\n",
    "    kafka_consumer = FlinkKafkaConsumer(\n",
    "        topics='data', # The topic to consume messages from\n",
    "        deserialization_schema= SimpleStringSchema('UTF-8'),\n",
    "        properties={'bootstrap.servers': 'localhost:9092', 'group.id': 'my-group'} # The Kafka broker address and consumer group ID\n",
    "    )\n",
    "\n",
    "    # 从最早的记录开始读取数据\n",
    "    kafka_consumer.set_start_from_earliest()\n",
    "    # 将Kafka消费者添加到StreamExecutionEnvironment，并打印输出\n",
    "    env.add_source(kafka_consumer).print()\n",
    "    # 执行StreamExecutionEnvironment\n",
    "    env.execute()\n",
    "\n",
    "# 调用函数\n",
    "if __name__ == '__main__':\n",
    "    read_from_kafka()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MapFunction`: 将一个元素作为输入并将一个元素作为输出的函数。通过对每个元素应用转换，它可用于转换数据流。\n",
    "`FilterFunction`: 将一个元素作为输入并返回一个布尔值的函数。它可用于删除不符合特定条件的元素，从而过滤数据流。\n",
    "`KeySelector`: 从元素中提取键的函数。它可用于按键对数据流中的元素进行分组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对真实的数据流进行Map操作\n",
    "# 从所有数据中找到类似年的数据，筛选出从1999-2023的数值\n",
    "\n",
    "import re\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "from io import StringIO\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from pyflink.table import StreamTableEnvironment\n",
    "from pyflink.common import WatermarkStrategy, Encoder, Types\n",
    "from pyflink.datastream import StreamExecutionEnvironment, RuntimeExecutionMode\n",
    "from pyflink.datastream.connectors.file_system import FileSource, StreamFormat, FileSink, OutputFileConfig, RollingPolicy\n",
    "from pyflink.common import Types, SimpleStringSchema\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.datastream.connectors.kafka import FlinkKafkaProducer, FlinkKafkaConsumer\n",
    "\n",
    "\n",
    "# 定义开始年份和结束年份\n",
    "Year_Begin =1999\n",
    "Year_End = 2023\n",
    "\n",
    "def extract_numbers(x):\n",
    "    return ' '.join(re.findall(r'\\d+', x))\n",
    "\n",
    "def filter_years(x):\n",
    "    return any([Year_Begin <= int(i) <= Year_End for i in x.split()])\n",
    "\n",
    "def map_years(x):\n",
    "    return [i for i in x.split() if Year_Begin <= int(i) <= Year_End][0]\n",
    "\n",
    "\n",
    "def read_from_kafka():\n",
    "    # 获取流环境\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()    \n",
    "    # 添加jar包\n",
    "    env.add_jars(\"file:///home/hadoop/Desktop/PyFlink-Tutorial/flink-sql-connector-kafka-3.1-SNAPSHOT.jar\")\n",
    "    print(\"start reading data from kafka\")\n",
    "    # 创建kafka消费者\n",
    "    kafka_consumer = FlinkKafkaConsumer(\n",
    "        topics='data', \n",
    "        deserialization_schema= SimpleStringSchema('UTF-8'),\n",
    "        properties={'bootstrap.servers': 'localhost:9092', 'group.id': 'my-group'} \n",
    "    )\n",
    "    # 从最早开始读取数据\n",
    "    kafka_consumer.set_start_from_earliest()\n",
    "    # 创建输出流\n",
    "    output = StringIO()\n",
    "    sys.stdout = output\n",
    "    # 添加源，并过滤出指定年份的数据\n",
    "    ds = env.add_source(kafka_consumer)\n",
    "    ds = ds.map(extract_numbers)\n",
    "    ds = ds.filter(filter_years)\n",
    "    ds = ds.map(map_years)\n",
    "    ds.print()\n",
    "    env.execute()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    read_from_kafka()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建筑数据的检测\n",
    "\n",
    "只做简单筛选咱们试过了。\n",
    "接下来是是稍微复杂的，对建筑沉降数据进行检测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建筑沉降数据的流化传输\n",
    "# 将building_data.csv 发送到Dockers中的Kafka服务器，主题为building\n",
    "\n",
    "from kafka import KafkaProducer\n",
    "import time\n",
    "import os\n",
    "import chardet\n",
    "\n",
    "# 定义一个函数，用于将文件发送到Kafka，参数为文件路径、主题和Kafka服务器地址\n",
    "def send_file_to_kafka(file_path: str, topic: str, bootstrap_servers: str):\n",
    "    # 创建一个KafkaProducer对象，用于发送消息\n",
    "    producer = KafkaProducer(bootstrap_servers=bootstrap_servers)\n",
    "    # 获取文件大小\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    # 检测文件编码\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        result = chardet.detect(f.read())\n",
    "        encoding = result[\"encoding\"]\n",
    "    # 读取文件内容\n",
    "    with open(file_path, \"r\", encoding=encoding) as f:\n",
    "        lines_total = len(f.readlines())\n",
    "    lines_send = 0\n",
    "    # 循环发送文件内容\n",
    "    while True:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            while True:\n",
    "                data = f.readlines(10)\n",
    "                if not data:\n",
    "                    break\n",
    "                data_str = str(data)\n",
    "                data_bytes = data_str.encode()\n",
    "                # 发送消息\n",
    "                producer.send(topic, data_bytes)\n",
    "                lines_send += 10\n",
    "                # 计算已发送的百分比\n",
    "                percent_sent = (lines_send / lines_total) * 100                \n",
    "                bytes_sent = len(data_bytes)\n",
    "                print(f\"Sent {bytes_sent} bytes {topic} {percent_sent:.2f}% sent\")\n",
    "                # 每3秒检查一次\n",
    "                time.sleep(3)\n",
    "        # 询问是否继续发送\n",
    "        user_input = input(\"Press 'c' to continue sending the file or 'q' to quit: \")\n",
    "        if user_input == \"q\":\n",
    "            break\n",
    "\n",
    "# 调用函数，将文件发送到Kafka，主题为building，服务器地址为localhost:9092\n",
    "send_file_to_kafka(\"./building_data.csv\",  \"building\", \"localhost:9092\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 显示出刚刚生成的数据流\n",
    "\n",
    "import platform\n",
    "import os\n",
    "import argparse\n",
    "import csv\n",
    "import io\n",
    "import logging\n",
    "import sys\n",
    "from typing import Iterable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from pyflink.common import Types, WatermarkStrategy, Time, Encoder\n",
    "from pyflink.common.watermark_strategy import TimestampAssigner\n",
    "from pyflink.datastream import StreamExecutionEnvironment, ProcessWindowFunction\n",
    "from pyflink.datastream.connectors.file_system import FileSink, OutputFileConfig, RollingPolicy\n",
    "from pyflink.datastream.connectors.kafka import FlinkKafkaProducer, FlinkKafkaConsumer\n",
    "from pyflink.datastream.window import SlidingEventTimeWindows, TimeWindow\n",
    "from pyflink.table import StreamTableEnvironment\n",
    "from pyflink.datastream import StreamExecutionEnvironment, RuntimeExecutionMode\n",
    "from pyflink.datastream.connectors.file_system import FileSource, StreamFormat\n",
    "from pyflink.common import SimpleStringSchema\n",
    "\n",
    "# 定义一个函数parse_csv_old，用于解析csv文件\n",
    "def parse_csv_old(x):\n",
    "    # 使用csv模块的reader函数读取csv文件\n",
    "    result = csv.reader(io.StringIO(x))    \n",
    "    # 返回csv文件的第一行\n",
    "    return next(result)\n",
    "\n",
    "# 定义一个函数parse_csv，用于解析csv文件\n",
    "def parse_csv(x):\n",
    "    # 将x中的[b'替换为空字符\n",
    "    x = x.replace(\"[b'\", \"\")\n",
    "    # 将x中的\\\\n']替换为空字符\n",
    "    x = x.replace(\"\\\\n']\", \"\")\n",
    "    # 使用csv模块的reader函数读取csv文件\n",
    "    result = csv.reader(io.StringIO(x))\n",
    "    # 返回csv文件的第一行\n",
    "    return next(result)\n",
    "\n",
    "# 定义一个函数，用于计算传入数据的行数\n",
    "def count_rows(data):\n",
    "    # 计算传入数据的行数\n",
    "    row_count = len(data)\n",
    "    # 计算传入数据的类型\n",
    "    type_count = type(data)\n",
    "    # 打印出传入数据的行数和类型\n",
    "    print(f\"Received {row_count} rows of {type_count} data.\")\n",
    "    # 返回传入数据\n",
    "    return data \n",
    "\n",
    "# 定义一个函数，用于解析元组\n",
    "def parse_tuple(x):\n",
    "    \n",
    "    # 打印出传入数据的第一个元素的类型、第二个元素的类型和第一个元素的长度\n",
    "    print(f\"x[0] type is {type(x[0])}\",f\"x[0][1] type is {type(x[0][1])}\",f\"x[0] len is {len(x[0])}\")\n",
    "    try:\n",
    "        # 尝试使用datetime.strptime函数将传入数据的第一个元素转换为时间戳，并将其第二个元素转换为float类型\n",
    "        return (datetime.strptime(str(x[0][0]), \"%Y-%m-%d %H:%M:%S\").timestamp(), float(x[0][1]))\n",
    "    except ValueError:\n",
    "        # 如果转换失败，则打印出传入数据的值，并返回None\n",
    "        logging.error(f\"Failed to parse tuple: {x}\")\n",
    "        return None\n",
    "\n",
    "# 定义一个函数read_from_kafka，用于从Kafka读取数据\n",
    "def read_from_kafka():\n",
    "    # 获取StreamExecutionEnvironment实例\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()    \n",
    "    # 添加flink-sql-connector-kafka-3.1-SNAPSHOT.jar包\n",
    "    env.add_jars(\"file:///home/hadoop/Desktop/PyFlink-Tutorial/flink-sql-connector-kafka-3.1-SNAPSHOT.jar\")\n",
    "    # 打印信息\n",
    "    print(\"start reading data from kafka\")\n",
    "    # 创建一个FlinkKafkaConsumer实例，用于从Kafka读取数据\n",
    "    kafka_consumer = FlinkKafkaConsumer(\n",
    "        topics='building', # The topic to consume messages from\n",
    "        deserialization_schema= SimpleStringSchema('UTF-8'), # The schema to deserialize messages\n",
    "        properties={'bootstrap.servers': 'localhost:9092', 'group.id': 'my-group'} # The Kafka broker address and consumer group ID\n",
    "    )\n",
    "    # 从最早的记录开始读取数据\n",
    "    kafka_consumer.set_start_from_earliest()\n",
    "    # 将kafka_consumer添加到StreamExecutionEnvironment中\n",
    "    stream = env.add_source(kafka_consumer)\n",
    "    # 将stream中的每一条数据解析为csv文件\n",
    "    parsed_stream = stream.map(parse_csv)\n",
    "    # 打印解析后的数据\n",
    "    parsed_stream.print()\n",
    "    # 执行StreamExecutionEnvironment\n",
    "    env.execute()\n",
    "\n",
    "# 调用函数read_from_kafka\n",
    "if __name__ == '__main__':\n",
    "    read_from_kafka()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对建筑沉降数据进行检测\n",
    "# 沉降绝对值大于0.5则警报\n",
    "\n",
    "import platform\n",
    "import os\n",
    "import argparse\n",
    "import csv\n",
    "import io\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Iterable\n",
    "from datetime import datetime\n",
    "from pyflink.common import Types, WatermarkStrategy, Time, Encoder\n",
    "from pyflink.common.watermark_strategy import TimestampAssigner\n",
    "from pyflink.datastream import StreamExecutionEnvironment, ProcessWindowFunction\n",
    "from pyflink.datastream.connectors.file_system import FileSink, OutputFileConfig, RollingPolicy\n",
    "from pyflink.datastream.connectors.kafka import FlinkKafkaProducer, FlinkKafkaConsumer\n",
    "from pyflink.datastream.window import SlidingEventTimeWindows, TimeWindow\n",
    "from pyflink.table import StreamTableEnvironment\n",
    "from pyflink.datastream import StreamExecutionEnvironment, RuntimeExecutionMode\n",
    "from pyflink.datastream.connectors.file_system import FileSource, StreamFormat\n",
    "from pyflink.common import SimpleStringSchema\n",
    "\n",
    "# 定义一个beep函数，根据不同的操作系统，播放不同的声音\n",
    "def beep():\n",
    "    # 如果是Windows系统\n",
    "    if platform.system() == \"Windows\":\n",
    "        # 导入winsound模块\n",
    "        import winsound\n",
    "        # 播放440Hz的音调，持续1000ms\n",
    "        winsound.Beep(440, 1000)\n",
    "    # 如果是Linux系统\n",
    "    elif platform.system() == \"Linux\":\n",
    "        # 播放beep命令\n",
    "        os.system(\"beep\")\n",
    "    # 如果是其他系统\n",
    "    else:\n",
    "        # 打印不支持的平台\n",
    "        print(\"Unsupported platform\")\n",
    "\n",
    "# 定义一个parse_csv函数，用于解析csv文件\n",
    "def parse_csv(x):    \n",
    "    # 将x中的[b'替换为空\n",
    "    x = x.replace(\"[b'\", \"\")\n",
    "    # 将x中的\\n']替换为空\n",
    "    x = x.replace(\"\\n']\", \"\")\n",
    "    # 将x中的\\\\n']替换为空\n",
    "    x = x.replace(\"\\\\n']\", \"\")\n",
    "    # 将x中的\\r']替换为空\n",
    "    x = x.replace(\"\\r\", \"\")\n",
    "    # 将x中的\\\\r']替换为空\n",
    "    x = x.replace(\"\\\\r\", \"\")\n",
    "    # 将x转换为csv格式\n",
    "    result = csv.reader(io.StringIO(x))\n",
    "    # 创建一个空列表，用于存放解析后的结果\n",
    "    parsed_result = []\n",
    "    # 遍历result中的每一项\n",
    "    for item in result:\n",
    "        # 创建一个空列表，用于存放解析后的每一项\n",
    "        parsed_item = []\n",
    "        # 遍历item中的每一项\n",
    "        for element in item:\n",
    "            # 尝试将element转换为整数\n",
    "            try:\n",
    "                parsed_element = int(element)\n",
    "            # 如果转换失败，则将element的值赋给parsed_element\n",
    "            except ValueError:\n",
    "                parsed_element = element\n",
    "            # 将parsed_element添加到parsed_item中\n",
    "            parsed_item.append(parsed_element)\n",
    "        # 将parsed_item添加到parsed_result中\n",
    "        parsed_result.append(parsed_item)\n",
    "    # 返回解析后的结果\n",
    "    return parsed_result\n",
    "\n",
    "# 定义一个count_rows函数，用于计算data中行数\n",
    "def count_rows(data):\n",
    "    # 计算data中行数\n",
    "    row_count = len(data)\n",
    "    # 获取data的类型\n",
    "    type_count = type(data)\n",
    "    # 打印data中行数和类型\n",
    "    print(f\"Received {row_count} rows of {type_count} data.\")\n",
    "    # 返回data\n",
    "    return data \n",
    "\n",
    "# 定义一个check_data函数，用于检查data中每一行的数据\n",
    "def check_data(data):\n",
    "    # 检查data中第一行的数据是否大于0.5\n",
    "    if abs(float(data[0][1])) >= 0.5:\n",
    "        # 如果大于0.5，则播放beep函数\n",
    "        beep()\n",
    "        # 打印data中第一行的数据和ABS值\n",
    "        # print(f\"data at {data[0][0]} is {(data[0][1])}\",f\" ABS Larger than 0.5!\\n\")\n",
    "    # 返回data\n",
    "    return abs(float(data[0][1])) >= 0.5\n",
    "\n",
    "# 定义一个read_from_kafka函数，用于从kafka中读取数据\n",
    "def read_from_kafka():\n",
    "    # 获取StreamExecutionEnvironment实例\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()\n",
    "    # 添加jars\n",
    "    env.add_jars(\"file:///home/hadoop/Desktop/PyFlink-Tutorial/flink-sql-connector-kafka-3.1-SNAPSHOT.jar\")\n",
    "    # 打印提示信息\n",
    "    print(\"start reading data from kafka\")\n",
    "    # 创建一个FlinkKafkaConsumer实例，用于从kafka中读取数据\n",
    "    kafka_consumer = FlinkKafkaConsumer(\n",
    "        topics='building',\n",
    "        deserialization_schema= SimpleStringSchema('UTF-8'), \n",
    "        properties={'bootstrap.servers': 'localhost:9092', 'group.id': 'my-group'} \n",
    "    )\n",
    "        \n",
    "    # 将kafka_consumer添加到StreamExecutionEnvironment中\n",
    "    stream = env.add_source(kafka_consumer)\n",
    "    # 将stream中的每一行数据转换为csv格式\n",
    "    parsed_stream = stream.map(parse_csv)\n",
    "\n",
    "    # 将parsed_stream中的每一行数据传入check_data函数，检查数据是否符合要求\n",
    "    data_stream = parsed_stream.filter(check_data)\n",
    "\n",
    "    # 将data_stream中的数据打印到标准输出中\n",
    "    print(\"Printing result to stdout.\")\n",
    "    data_stream.print()\n",
    "\n",
    "    # 执行StreamExecutionEnvironment\n",
    "    env.execute()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 调用read_from_kafka函数，从kafka中读取数据\n",
    "    read_from_kafka()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
