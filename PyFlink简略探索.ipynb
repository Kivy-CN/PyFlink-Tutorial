{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyFlink 安装\n",
    "\n",
    "### 作者：胖胖揽住\n",
    "\n",
    "### 版本 2023.11.15\n",
    "\n",
    "## Anaconda3 安装\n",
    "\n",
    "首先从TUNA下载Anaconda3安装包。\n",
    "\n",
    "```Bash\n",
    "wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-2023.09-0-Linux-x86_64.sh\n",
    "sh Anaconda3-2023.09-0-Linux-x86_64.sh\n",
    "```\n",
    "安装过程中，请使用默认设置。\n",
    "应该安装在`~/anaconda3`。\n",
    "\n",
    "## Python 3.9 安装\n",
    "\n",
    "通过 conda 安装 Python 3.9 将变得简单可靠。\n",
    "\n",
    "```Bash\n",
    "conda create -n pyflink_39 python=3.9\n",
    "conda activate pyflink_39\n",
    "```\n",
    "\n",
    "\n",
    "## Apache-Flink 安装\n",
    "\n",
    "先去[Apache 官网](https://dlcdn.apache.org/flink/)下载安装 flink，这里以 1.18.0 为例：\n",
    "\n",
    "```Bash\n",
    "wget https://dlcdn.apache.org/flink/flink-1.18.0/flink-1.18.0-bin-scala_2.12.tgz\n",
    "sudo tar -zxvf flink-1.18.0-bin-scala_2.12.tgz  -C /usr/local   \n",
    "```\n",
    "\n",
    "修改目录名称，并设置权限，命令如下：\n",
    "```Bash\n",
    "cd /usr/local\n",
    "sudo mv / flink-1.18.0 ./flink #这里是因为我这里下的是这个版本，读者需要酌情调整\n",
    "sudo chown -R hadoop:hadoop ./flink ##这里是因为我这里虚拟机的用户名是这个，读者需要酌情调整\n",
    "```\n",
    "\n",
    "Flink解压缩并且设置好权限后，直接就可以在本地模式运行，不需要修改任何配置。\n",
    "如果要做调整，可以编辑`“/usr/local/flink/conf/flink-conf.yam`这个文件。\n",
    "比如其中的`env.java.home`参就可以设置为本地Java的绝对路径\n",
    "不过一般不需要手动修改什么配置。\n",
    "\n",
    "不过，需要注意的是，Flink现在需要的是Java11，所以需要用下列命令手动安装一下：\n",
    "```Bash\n",
    "sudo apt install openjdk-11-jdk -y\n",
    "```\n",
    "\n",
    "接下来还需要修接下来还需要修改配置文件，添加环境变量：\n",
    "\n",
    "```Bash\n",
    "nano ~/.bashrc\n",
    "```\n",
    "\n",
    "文件中添加如下内容：\n",
    "```\n",
    "export FLINK_HOME=/usr/local/flink\n",
    "export PATH=$FLINK_HOME/bin:$PATH\n",
    "```\n",
    "\n",
    "保存并退出.bashrc文件，然后执行如下命令让配置文件生效：\n",
    "```Bash\n",
    "source ~/.bashrc\n",
    "```\n",
    "\n",
    "## 安装 Python 依赖包\n",
    "\n",
    "然后使用 pip 安装 apache-flink 包， 以及 Kafka-python 等等依赖包\n",
    "\n",
    "```Bash\n",
    "pip install apache-flink \n",
    "pip install kafka-python chardet pandas numpy scipy simpy \n",
    "pip install matplotlib cython sympy xlrd pyopengl BeautifulSoup4 pyqt6 scikit-learn requests tensorflow torch keras tqdm gym DRL\n",
    "```\n",
    "\n",
    "## 代码说明\n",
    "\n",
    "本文代码修改自官方[文档版本1.18](https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/dev/python/datastream_tutorial/)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 Flink Python DataStream API 的词频统计\n",
    "# 以下代码来自官方[文档版本1.18](https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/dev/python/datastream_tutorial/)。\n",
    "# 将下面的代码保存为`DataStream_API_word_count.py`。\n",
    "\n",
    "import os\n",
    "# Get current absolute path\n",
    "current_file_path = os.path.abspath(__file__)\n",
    "# Get current dir path\n",
    "current_dir_path = os.path.dirname(current_file_path)\n",
    "# Change into current dir path\n",
    "os.chdir(current_dir_path)\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from pyflink.table import StreamTableEnvironment\n",
    "from pyflink.common import WatermarkStrategy, Encoder, Types\n",
    "from pyflink.datastream import StreamExecutionEnvironment, RuntimeExecutionMode\n",
    "from pyflink.datastream.connectors.file_system import FileSource, StreamFormat, FileSink, OutputFileConfig, RollingPolicy\n",
    "\n",
    "\n",
    "word_count_data = [\"To be, or not to be,--that is the question:--\",\n",
    "                   \"Whether 'tis nobler in the mind to suffer\",\n",
    "                   \"The slings and arrows of outrageous fortune\",\n",
    "                   \"Or to take arms against a sea of troubles,\",\n",
    "                   \"And by opposing end them?--To die,--to sleep,--\",\n",
    "                   \"No more; and by a sleep to say we end\",\n",
    "                   \"The heartache, and the thousand natural shocks\",\n",
    "                   \"That flesh is heir to,--'tis a consummation\",\n",
    "                   \"Devoutly to be wish'd. To die,--to sleep;--\",\n",
    "                   \"To sleep! perchance to dream:--ay, there's the rub;\",\n",
    "                   \"For in that sleep of death what dreams may come,\",\n",
    "                   \"When we have shuffled off this mortal coil,\",\n",
    "                   \"Must give us pause: there's the respect\",\n",
    "                   \"That makes calamity of so long life;\",\n",
    "                   \"For who would bear the whips and scorns of time,\",\n",
    "                   \"The oppressor's wrong, the proud man's contumely,\",\n",
    "                   \"The pangs of despis'd love, the law's delay,\",\n",
    "                   \"The insolence of office, and the spurns\",\n",
    "                   \"That patient merit of the unworthy takes,\",\n",
    "                   \"When he himself might his quietus make\",\n",
    "                   \"With a bare bodkin? who would these fardels bear,\",\n",
    "                   \"To grunt and sweat under a weary life,\",\n",
    "                   \"But that the dread of something after death,--\",\n",
    "                   \"The undiscover'd country, from whose bourn\",\n",
    "                   \"No traveller returns,--puzzles the will,\",\n",
    "                   \"And makes us rather bear those ills we have\",\n",
    "                   \"Than fly to others that we know not of?\",\n",
    "                   \"Thus conscience does make cowards of us all;\",\n",
    "                   \"And thus the native hue of resolution\",\n",
    "                   \"Is sicklied o'er with the pale cast of thought;\",\n",
    "                   \"And enterprises of great pith and moment,\",\n",
    "                   \"With this regard, their currents turn awry,\",\n",
    "                   \"And lose the name of action.--Soft you now!\",\n",
    "                   \"The fair Ophelia!--Nymph, in thy orisons\",\n",
    "                   \"Be all my sins remember'd.\"]\n",
    "\n",
    "\n",
    "# 定义word_count函数，用于计算输入文件中单词的数量\n",
    "def word_count(input_path, output_path):\n",
    "    # 获取StreamExecutionEnvironment实例\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()\n",
    "    # 设置运行模式为批处理模式\n",
    "    env.set_runtime_mode(RuntimeExecutionMode.BATCH)\n",
    "    # 设置并行度为1\n",
    "    env.set_parallelism(1)\n",
    "\n",
    "    # 如果输入路径不为空，则从输入路径中读取数据\n",
    "    if input_path is not None:\n",
    "        ds = env.from_source(\n",
    "            source=FileSource.for_record_stream_format(StreamFormat.text_line_format(),\n",
    "                                                       input_path)\n",
    "                             .process_static_file_set().build(),\n",
    "            # 设置水印策略为单调时间戳\n",
    "            watermark_strategy=WatermarkStrategy.for_monotonous_timestamps(),\n",
    "            source_name=\"file_source\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"Executing word_count example with default input data set.\")\n",
    "        print(\"Use --input to specify file input.\")\n",
    "        # 使用word_count_data作为输入数据集\n",
    "        ds = env.from_collection(word_count_data)\n",
    "\n",
    "    # 将输入数据集拆分成单词\n",
    "    def split(line):\n",
    "        yield from line.split()\n",
    "    ds = ds.flat_map(split) \\\n",
    "        .map(lambda i: (i, 1), output_type=Types.TUPLE([Types.STRING(), Types.INT()])) \\\n",
    "        .key_by(lambda i: i[0]) \\\n",
    "        .reduce(lambda i, j: (i[0], i[1] + j[1]))\n",
    "\n",
    "    # 如果输出路径不为空，则将结果写入输出路径\n",
    "    if output_path is not None:\n",
    "        ds.sink_to(\n",
    "            sink=FileSink.for_row_format(\n",
    "                base_path=output_path,\n",
    "                encoder=Encoder.simple_string_encoder())\n",
    "            .with_output_file_config(\n",
    "                OutputFileConfig.builder()\n",
    "                .with_part_prefix(\"prefix\")\n",
    "                .with_part_suffix(\".ext\")\n",
    "                .build())\n",
    "            .with_rolling_policy(RollingPolicy.default_rolling_policy())\n",
    "            .build()\n",
    "        )\n",
    "    else:\n",
    "        print(\"Printing result to stdout. Use --output to specify output path.\")\n",
    "        t_env = StreamTableEnvironment.create(env)\n",
    "        table = t_env.from_data_stream(ds)\n",
    "        df = table.to_pandas()\n",
    "        \n",
    "        # 将结果写入csv文件\n",
    "        df.to_csv('./DataStream_API_word_count.csv', index=False)\n",
    "        print(df)\n",
    "    # 执行word_count函数\n",
    "    env.execute()\n",
    "\n",
    "\n",
    "# 定义word_count函数，用于计算输入文件中单词的数量\n",
    "if __name__ == '__main__':\n",
    "    # 设置日志输出格式\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=\"%(message)s\")\n",
    "    # 创建参数解析器\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # 添加参数\n",
    "    parser.add_argument(\n",
    "        '--input',\n",
    "        dest='input',\n",
    "        required=False,\n",
    "        help='Input file to process.')\n",
    "    parser.add_argument(\n",
    "        '--output',\n",
    "        dest='output',\n",
    "        required=False,\n",
    "        help='Output file to write results to.')\n",
    "\n",
    "    # 解析参数\n",
    "    argv = sys.argv[1:]\n",
    "    known_args, _ = parser.parse_known_args(argv)\n",
    "    # 调用word_count函数\n",
    "    word_count(known_args.input, known_args.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing word_count example with default input data set.\n",
      "Use --input to specify file input.\n",
      "Printing result to stdout. Use --output to specify output path.\n"
     ]
    }
   ],
   "source": [
    "# 使用 Flink Python Table API 的词频统计\n",
    "# 以下代码来自官方[文档版本1.18](https://nightlies.apache.org/flink/flink-docs-release-1.18/zh/docs/dev/python/table_api_tutorial/)。\n",
    "# 将下面的代码保存为`Table_API_word_count.py`。\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from pyflink.common import Row\n",
    "from pyflink.table import (EnvironmentSettings, TableEnvironment, TableDescriptor, Schema,\n",
    "                           DataTypes, FormatDescriptor)\n",
    "from pyflink.table.expressions import lit, col\n",
    "from pyflink.table.udf import udtf\n",
    "\n",
    "word_count_data = [\"To be, or not to be,--that is the question:--\",\n",
    "                   \"Whether 'tis nobler in the mind to suffer\",\n",
    "                   \"The slings and arrows of outrageous fortune\",\n",
    "                   \"Or to take arms against a sea of troubles,\",\n",
    "                   \"And by opposing end them?--To die,--to sleep,--\",\n",
    "                   \"No more; and by a sleep to say we end\",\n",
    "                   \"The heartache, and the thousand natural shocks\",\n",
    "                   \"That flesh is heir to,--'tis a consummation\",\n",
    "                   \"Devoutly to be wish'd. To die,--to sleep;--\",\n",
    "                   \"To sleep! perchance to dream:--ay, there's the rub;\",\n",
    "                   \"For in that sleep of death what dreams may come,\",\n",
    "                   \"When we have shuffled off this mortal coil,\",\n",
    "                   \"Must give us pause: there's the respect\",\n",
    "                   \"That makes calamity of so long life;\",\n",
    "                   \"For who would bear the whips and scorns of time,\",\n",
    "                   \"The oppressor's wrong, the proud man's contumely,\",\n",
    "                   \"The pangs of despis'd love, the law's delay,\",\n",
    "                   \"The insolence of office, and the spurns\",\n",
    "                   \"That patient merit of the unworthy takes,\",\n",
    "                   \"When he himself might his quietus make\",\n",
    "                   \"With a bare bodkin? who would these fardels bear,\",\n",
    "                   \"To grunt and sweat under a weary life,\",\n",
    "                   \"But that the dread of something after death,--\",\n",
    "                   \"The undiscover'd country, from whose bourn\",\n",
    "                   \"No traveller returns,--puzzles the will,\",\n",
    "                   \"And makes us rather bear those ills we have\",\n",
    "                   \"Than fly to others that we know not of?\",\n",
    "                   \"Thus conscience does make cowards of us all;\",\n",
    "                   \"And thus the native hue of resolution\",\n",
    "                   \"Is sicklied o'er with the pale cast of thought;\",\n",
    "                   \"And enterprises of great pith and moment,\",\n",
    "                   \"With this regard, their currents turn awry,\",\n",
    "                   \"And lose the name of action.--Soft you now!\",\n",
    "                   \"The fair Ophelia!--Nymph, in thy orisons\",\n",
    "                   \"Be all my sins remember'd.\"]\n",
    "\n",
    "\n",
    "# 定义一个函数word_count，用于计算单词出现次数\n",
    "# 参数input_path和output_path分别表示输入和输出路径\n",
    "def word_count(input_path, output_path):\n",
    "    # 创建一个TableEnvironment对象，用于执行流式计算\n",
    "    t_env = TableEnvironment.create(EnvironmentSettings.in_streaming_mode())\n",
    "    # 设置并行度为1\n",
    "    t_env.get_config().set(\"parallelism.default\", \"1\")\n",
    "    # 如果输入路径不为空，则创建一个临时表，用于读取输入数据\n",
    "    if input_path is not None:\n",
    "        t_env.create_temporary_table(\n",
    "            'source',\n",
    "            TableDescriptor.for_connector('filesystem')\n",
    "                .schema(Schema.new_builder()\n",
    "                        .column('word', DataTypes.STRING())\n",
    "                        .build())\n",
    "                .option('path', input_path)\n",
    "                .format('csv')\n",
    "                .build())\n",
    "        tab = t_env.from_path('source')\n",
    "    # 否则，使用默认的数据集\n",
    "    else:\n",
    "        print(\"Executing word_count example with default input data set.\")\n",
    "        print(\"Use --input to specify file input.\")\n",
    "        tab = t_env.from_elements(map(lambda i: (i,), word_count_data),\n",
    "                                  DataTypes.ROW([DataTypes.FIELD('line', DataTypes.STRING())]))\n",
    "    # 如果输出路径不为空，则创建一个临时表，用于存储计算结果\n",
    "    if output_path is not None:\n",
    "        t_env.create_temporary_table(\n",
    "            'sink',\n",
    "            TableDescriptor.for_connector('filesystem')\n",
    "                .schema(Schema.new_builder()\n",
    "                        .column('word', DataTypes.STRING())\n",
    "                        .column('count', DataTypes.BIGINT())\n",
    "                        .build())\n",
    "                .option('path', output_path)\n",
    "                .format(FormatDescriptor.for_format('canal-json')\n",
    "                        .build())\n",
    "                .build())\n",
    "    # 否则，将计算结果打印到标准输出\n",
    "    else:\n",
    "        print(\"Printing result to stdout. Use --output to specify output path.\")\n",
    "        t_env.create_temporary_table(\n",
    "            'sink',\n",
    "            TableDescriptor.for_connector('print')\n",
    "                .schema(Schema.new_builder()\n",
    "                        .column('word', DataTypes.STRING())\n",
    "                        .column('count', DataTypes.BIGINT())\n",
    "                        .build())\n",
    "                .build())\n",
    "    # 定义一个UDF，用于将每一行文本拆分成单词\n",
    "    @udtf(result_types=[DataTypes.STRING()])\n",
    "    def split(line: Row):\n",
    "        for s in line[0].split():\n",
    "            yield Row(s)\n",
    "    # 将文本拆分成单词，并计算每个单词出现的次数\n",
    "    tab.flat_map(split).alias('word') \\\n",
    "        .group_by(col('word')) \\\n",
    "        .select(col('word'), lit(1).count) \\\n",
    "        .execute_insert('sink') \\\n",
    "        .wait()\n",
    "    # 将计算结果转换为Pandas数据框，并保存到csv文件中\n",
    "    df = tab.to_pandas()\n",
    "    df.to_csv('./Table_API_word_count.csv', index=False)\n",
    "    print(df)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 设置日志输出格式\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=\"%(message)s\")\n",
    "    # 创建参数解析器\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # 添加参数\n",
    "    parser.add_argument(\n",
    "        '--input',\n",
    "        dest='input',\n",
    "        required=False,\n",
    "        help='Input file to process.')\n",
    "    parser.add_argument(\n",
    "        '--output',\n",
    "        dest='output',\n",
    "        required=False,\n",
    "        help='Output file to write results to.')\n",
    "    # 解析参数\n",
    "    argv = sys.argv[1:]\n",
    "    known_args, _ = parser.parse_known_args(argv)\n",
    "\n",
    "    # 调用word_count函数处理输入文件\n",
    "    word_count(known_args.input, known_args.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用Docker搭建本地Kafka集群\n",
    "\n",
    "操作系统选择 Ubuntu 22.04.3   \n",
    "\n",
    "1. 安装 Docker 和 Docker Compose:\n",
    "```Bash\n",
    "sudo apt install Docker Docker-compose\n",
    "```\n",
    "2. 创建本地 `docker-compose.yml` 文件，其中包含以下内容：\n",
    "\n",
    "```yaml\n",
    "version: '3'\n",
    "services:\n",
    "  zookeeper:\n",
    "    image: 'bitnami/zookeeper:latest'\n",
    "    environment:\n",
    "      - ALLOW_ANONYMOUS_LOGIN=yes\n",
    "  kafka:\n",
    "    image: 'bitnami/kafka:latest'\n",
    "    ports:\n",
    "      - '9092:9092'\n",
    "    environment:\n",
    "      - KAFKA_ADVERTISED_HOST_NAME=localhost\n",
    "      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181\n",
    "      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092\n",
    "      - KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9092\n",
    "      - KAFKA_CREATE_TOPICS=test:1:1\n",
    "      - ALLOW_PLAINTEXT_LISTENER=yes\n",
    "    depends_on:\n",
    "      - zookeeper\n",
    "```\n",
    "\n",
    "3. 找到“docker-compose.yml”所在目录并运行以下命令：\n",
    "\n",
    "````Bash\n",
    "docker-compose up -d\n",
    "````\n",
    "\n",
    "这将运行一个包含 Zookeeper 实例和 Kafka 实例的本地 Kafka 集群，该集群将在本地主机的端口 9092 上运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 kafka-python 生成流的简单方法\n",
    "\n",
    "# 以下代码使用kafka-python模块将数据发送到本地Kafka集群。\n",
    "# 此代码打开一个名为 `hamlet.txt` 的文本文件，并将其内容作为流发送到指定的 Kafka 主题 `hamlet`：\n",
    "\n",
    "# 导入KafkaProducer模块\n",
    "from kafka import KafkaProducer\n",
    "# 导入time模块\n",
    "import time\n",
    "# 导入os模块\n",
    "import os\n",
    "\n",
    "# 定义一个函数，用于将文件发送到Kafka\n",
    "def send_file_to_kafka(file_path: str, topic: str, bootstrap_servers: str):\n",
    "    # 创建一个KafkaProducer实例，用于发送消息\n",
    "    producer = KafkaProducer(bootstrap_servers=bootstrap_servers)\n",
    "    # 获取文件大小\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    # 循环发送文件\n",
    "    while True:\n",
    "        # 打开文件\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            # 循环读取文件\n",
    "            while True:\n",
    "                # 读取文件内容\n",
    "                data = f.read(1024)\n",
    "                # 如果没有内容，则跳出循环\n",
    "                if not data:\n",
    "                    break\n",
    "                # 将文件内容发送到Kafka\n",
    "                producer.send(topic, data)\n",
    "                # 计算发送的字节数\n",
    "                bytes_sent = len(data)\n",
    "                # 打印发送的字节数\n",
    "                print(f\"Sent {bytes_sent} bytes to Kafka topic {topic}\")\n",
    "                # 计算发送的百分比\n",
    "                percent_sent = (f.tell() / file_size) * 100\n",
    "                # 打印发送的百分比\n",
    "                print(f\"{percent_sent:.2f}% of the file sent\")\n",
    "                # 等待3秒\n",
    "                time.sleep(3)\n",
    "        # 获取用户输入\n",
    "        user_input = input(\"Press 'c' to continue sending the file or 'q' to quit: \")\n",
    "        # 如果用户输入q，则退出循环\n",
    "        if user_input == \"q\":\n",
    "            break\n",
    "# 调用函数，将hamlet.txt文件发送到Kafka的hamlet主题\n",
    "send_file_to_kafka(\"./hamlet.txt\",  \"hamlet\", \"localhost:9092\")\n",
    "# 在此代码中，send_file_to_kafka 函数接受三个参数：file_path、topic 和 bootstrap_servers。\n",
    "# file_path是本地文件的路径，topic是数据要发送到的Kafka主题，bootstrap_servers是Kafka集群的地址。\n",
    "# 该函数使用with语句打开文件，读取其内容，并将它们作为流数据发送到指定的Kafka主题。\n",
    "# 发送过程中，打印出发送进度，并使用time.sleep方法暂停0.1秒来控制发送速率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 kafka-python 展现流数据的简单方法\n",
    "\n",
    "from kafka import KafkaConsumer\n",
    "\n",
    "# 创建一个KafkaConsumer实例，用于从Kafka主题中读取消息\n",
    "consumer = KafkaConsumer(\n",
    "    # 指定要读取的消息主题\n",
    "    \"hamlet\",\n",
    "    # 指定Kafka服务器的地址和端口\n",
    "    bootstrap_servers=[\"localhost:9092\"],\n",
    "    # 指定当消费者重新启动时，它应该从哪个偏移量开始读取消息\n",
    "    auto_offset_reset=\"earliest\",\n",
    "    # 指定是否在消费者处理消息时，应该提交偏移量\n",
    "    enable_auto_commit=True,\n",
    "    # 指定消费者组，用于提交偏移量\n",
    "    group_id=\"my-group\",\n",
    "    # 指定消息的解码方式\n",
    "    value_deserializer=lambda x: x.decode(\"utf-8\")\n",
    ")\n",
    "\n",
    "# 循环读取Kafka主题中的消息，并打印消息长度和消息内容\n",
    "for message in consumer:\n",
    "    print(f\"Received {len(message.value)} bytes from Kafka topic {message.topic}\")\n",
    "    print(f\"{message.value}\")\n",
    "\n",
    "# 在上面的代码中，我们使用`KafkaConsumer`类来创建一个消费者对象。\n",
    "# 我们将 `hamlet` 作为主题名称传递给构造函数。\n",
    "# 我们还传递 `localhost:9092` 作为引导服务器的地址。\n",
    "# 我们使用 `value_deserializer` 参数来解码从 Kafka 主题收到的消息。\n",
    "# 我们使用 `for` 循环来迭代消费者对象，并使用 `print` 函数来打印消息的内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start reading data from kafka\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o0.execute.\n: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.\n\tat org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)\n\tat org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)\n\tat java.base/java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:646)\n\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)\n\tat java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2147)\n\tat org.apache.flink.runtime.rpc.pekko.PekkoInvocationHandler.lambda$invokeRpc$1(PekkoInvocationHandler.java:268)\n\tat java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:863)\n\tat java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:841)\n\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)\n\tat java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2147)\n\tat org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1267)\n\tat org.apache.flink.runtime.concurrent.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)\n\tat org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)\n\tat org.apache.flink.runtime.concurrent.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)\n\tat java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:863)\n\tat java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:841)\n\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)\n\tat java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2147)\n\tat org.apache.flink.runtime.concurrent.pekko.ScalaFutureUtils$1.onComplete(ScalaFutureUtils.java:47)\n\tat org.apache.pekko.dispatch.OnComplete.internal(Future.scala:310)\n\tat org.apache.pekko.dispatch.OnComplete.internal(Future.scala:307)\n\tat org.apache.pekko.dispatch.japi$CallbackBridge.apply(Future.scala:234)\n\tat org.apache.pekko.dispatch.japi$CallbackBridge.apply(Future.scala:231)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat org.apache.flink.runtime.concurrent.pekko.ScalaFutureUtils$DirectExecutionContext.execute(ScalaFutureUtils.java:65)\n\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n\tat org.apache.pekko.pattern.PromiseActorRef.$bang(AskSupport.scala:629)\n\tat org.apache.pekko.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:34)\n\tat org.apache.pekko.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:33)\n\tat scala.concurrent.Future.$anonfun$andThen$1(Future.scala:536)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat org.apache.pekko.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:73)\n\tat org.apache.pekko.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:110)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n\tat org.apache.pekko.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:110)\n\tat org.apache.pekko.dispatch.TaskInvocation.run(AbstractDispatcher.scala:59)\n\tat org.apache.pekko.dispatch.ForkJoinExecutorConfigurator$PekkoForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:57)\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)\nCaused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy\n\tat org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:176)\n\tat org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:107)\n\tat org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:285)\n\tat org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:276)\n\tat org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:269)\n\tat org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:764)\n\tat org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:741)\n\tat org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:83)\n\tat org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:488)\n\tat jdk.internal.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.lambda$handleRpcInvocation$1(PekkoRpcActor.java:309)\n\tat org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)\n\tat org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcInvocation(PekkoRpcActor.java:307)\n\tat org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcMessage(PekkoRpcActor.java:222)\n\tat org.apache.flink.runtime.rpc.pekko.FencedPekkoRpcActor.handleRpcMessage(FencedPekkoRpcActor.java:85)\n\tat org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleMessage(PekkoRpcActor.java:168)\n\tat org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:33)\n\tat org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:29)\n\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\tat org.apache.pekko.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:29)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)\n\tat org.apache.pekko.actor.Actor.aroundReceive(Actor.scala:547)\n\tat org.apache.pekko.actor.Actor.aroundReceive$(Actor.scala:545)\n\tat org.apache.pekko.actor.AbstractActor.aroundReceive(AbstractActor.scala:229)\n\tat org.apache.pekko.actor.ActorCell.receiveMessage(ActorCell.scala:590)\n\tat org.apache.pekko.actor.ActorCell.invoke(ActorCell.scala:557)\n\tat org.apache.pekko.dispatch.Mailbox.processMailbox(Mailbox.scala:280)\n\tat org.apache.pekko.dispatch.Mailbox.run(Mailbox.scala:241)\n\tat org.apache.pekko.dispatch.Mailbox.exec(Mailbox.scala:253)\n\t... 5 more\nCaused by: org.apache.flink.kafka.shaded.org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m     env\u001b[38;5;241m.\u001b[39mexecute()\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 42\u001b[0m     \u001b[43mread_from_kafka\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 39\u001b[0m, in \u001b[0;36mread_from_kafka\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m env\u001b[38;5;241m.\u001b[39madd_source(kafka_consumer)\u001b[38;5;241m.\u001b[39mprint()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# 执行执行环境\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pyflink_39/lib/python3.9/site-packages/pyflink/datastream/stream_execution_environment.py:773\u001b[0m, in \u001b[0;36mStreamExecutionEnvironment.execute\u001b[0;34m(self, job_name)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;124;03mTriggers the program execution. The environment will execute all parts of\u001b[39;00m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;124;03mthe program that have resulted in a \"sink\" operation. Sink operations are\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;124;03m:return: The result of the job execution, containing elapsed time and accumulators.\u001b[39;00m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    772\u001b[0m j_stream_graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_stream_graph(clear_transformations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, job_name\u001b[38;5;241m=\u001b[39mjob_name)\n\u001b[0;32m--> 773\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m JobExecutionResult(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_j_stream_execution_environment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mj_stream_graph\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pyflink_39/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/pyflink_39/lib/python3.9/site-packages/pyflink/util/exceptions.py:146\u001b[0m, in \u001b[0;36mcapture_java_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyflink\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjava_gateway\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_gateway\n",
      "File \u001b[0;32m~/anaconda3/envs/pyflink_39/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o0.execute.\n: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.\n\tat org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)\n\tat org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)\n\tat java.base/java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:646)\n\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)\n\tat java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2147)\n\tat org.apache.flink.runtime.rpc.pekko.PekkoInvocationHandler.lambda$invokeRpc$1(PekkoInvocationHandler.java:268)\n\tat java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:863)\n\tat java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:841)\n\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)\n\tat java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2147)\n\tat org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1267)\n\tat org.apache.flink.runtime.concurrent.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)\n\tat org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)\n\tat org.apache.flink.runtime.concurrent.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)\n\tat java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:863)\n\tat java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:841)\n\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)\n\tat java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2147)\n\tat org.apache.flink.runtime.concurrent.pekko.ScalaFutureUtils$1.onComplete(ScalaFutureUtils.java:47)\n\tat org.apache.pekko.dispatch.OnComplete.internal(Future.scala:310)\n\tat org.apache.pekko.dispatch.OnComplete.internal(Future.scala:307)\n\tat org.apache.pekko.dispatch.japi$CallbackBridge.apply(Future.scala:234)\n\tat org.apache.pekko.dispatch.japi$CallbackBridge.apply(Future.scala:231)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat org.apache.flink.runtime.concurrent.pekko.ScalaFutureUtils$DirectExecutionContext.execute(ScalaFutureUtils.java:65)\n\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n\tat org.apache.pekko.pattern.PromiseActorRef.$bang(AskSupport.scala:629)\n\tat org.apache.pekko.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:34)\n\tat org.apache.pekko.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:33)\n\tat scala.concurrent.Future.$anonfun$andThen$1(Future.scala:536)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat org.apache.pekko.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:73)\n\tat org.apache.pekko.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:110)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n\tat org.apache.pekko.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:110)\n\tat org.apache.pekko.dispatch.TaskInvocation.run(AbstractDispatcher.scala:59)\n\tat org.apache.pekko.dispatch.ForkJoinExecutorConfigurator$PekkoForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:57)\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)\nCaused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy\n\tat org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:176)\n\tat org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:107)\n\tat org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:285)\n\tat org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:276)\n\tat org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:269)\n\tat org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:764)\n\tat org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:741)\n\tat org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:83)\n\tat org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:488)\n\tat jdk.internal.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.lambda$handleRpcInvocation$1(PekkoRpcActor.java:309)\n\tat org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)\n\tat org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcInvocation(PekkoRpcActor.java:307)\n\tat org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcMessage(PekkoRpcActor.java:222)\n\tat org.apache.flink.runtime.rpc.pekko.FencedPekkoRpcActor.handleRpcMessage(FencedPekkoRpcActor.java:85)\n\tat org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleMessage(PekkoRpcActor.java:168)\n\tat org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:33)\n\tat org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:29)\n\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\tat org.apache.pekko.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:29)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)\n\tat org.apache.pekko.actor.Actor.aroundReceive(Actor.scala:547)\n\tat org.apache.pekko.actor.Actor.aroundReceive$(Actor.scala:545)\n\tat org.apache.pekko.actor.AbstractActor.aroundReceive(AbstractActor.scala:229)\n\tat org.apache.pekko.actor.ActorCell.receiveMessage(ActorCell.scala:590)\n\tat org.apache.pekko.actor.ActorCell.invoke(ActorCell.scala:557)\n\tat org.apache.pekko.dispatch.Mailbox.processMailbox(Mailbox.scala:280)\n\tat org.apache.pekko.dispatch.Mailbox.run(Mailbox.scala:241)\n\tat org.apache.pekko.dispatch.Mailbox.exec(Mailbox.scala:253)\n\t... 5 more\nCaused by: org.apache.flink.kafka.shaded.org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata\n"
     ]
    }
   ],
   "source": [
    "# 使用 pyflink 进行流数据展现\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from pyflink.table import StreamTableEnvironment\n",
    "from pyflink.common import WatermarkStrategy, Encoder, Types\n",
    "from pyflink.datastream import StreamExecutionEnvironment, RuntimeExecutionMode\n",
    "from pyflink.datastream.connectors.file_system import FileSource, StreamFormat, FileSink, OutputFileConfig, RollingPolicy\n",
    "from pyflink.common import Types, SimpleStringSchema\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.datastream.connectors.kafka import FlinkKafkaProducer, FlinkKafkaConsumer\n",
    "\n",
    "def split(line):\n",
    "    # 将行拆分成单词\n",
    "    yield from line.split()\n",
    "\n",
    "def read_from_kafka():\n",
    "    # 从Kafka读取数据\n",
    "    # 获取当前的执行环境\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()    \n",
    "    # 添加kafka连接器\n",
    "    env.add_jars(\"file:///home/hadoop/Desktop/PyFlink-Tutorial/flink-sql-connector-kafka-3.1-SNAPSHOT.jar\")\n",
    "    print(\"start reading data from kafka\")\n",
    "    # 创建一个kafka消费者，用于从kafka中读取消息\n",
    "    kafka_consumer = FlinkKafkaConsumer(\n",
    "        topics='hamlet', # The topic to consume messages from\n",
    "        deserialization_schema= SimpleStringSchema('UTF-8'), # The schema to deserialize messages\n",
    "        properties={'bootstrap.servers': 'localhost:9092', 'group.id': 'my-group'} # The Kafka broker address and consumer group ID\n",
    "    )\n",
    "    # 从最早的记录开始读取数据\n",
    "    kafka_consumer.set_start_from_earliest()\n",
    "    # 将kafka消费者添加到执行环境中，并打印输出\n",
    "    env.add_source(kafka_consumer).print()\n",
    "    # 执行执行环境\n",
    "    env.execute()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    read_from_kafka()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简单的词频统计\n",
    "\n",
    "# 导入os模块\n",
    "import os\n",
    "# 导入re模块\n",
    "import re\n",
    "# 导入Counter模块\n",
    "from collections import Counter\n",
    "# 导入StreamTableEnvironment模块\n",
    "from pyflink.table import StreamTableEnvironment\n",
    "# 导入StreamExecutionEnvironment模块\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "# 导入FlinkKafkaConsumer模块\n",
    "from pyflink.datastream.connectors.kafka import FlinkKafkaConsumer\n",
    "# 导入SimpleStringSchema模块\n",
    "from pyflink.common import SimpleStringSchema\n",
    "\n",
    "# 定义去除标点符号的函数\n",
    "def remove_punctuation(text):\n",
    "    # 使用正则表达式去除标点符号\n",
    "    return re.sub(r'[^\\w\\s]','',text)\n",
    "\n",
    "# 定义统计单词的函数\n",
    "def count_words(text):\n",
    "    # 将文本按空格分割成单词列表\n",
    "    words = text.split()\n",
    "    # 使用Counter模块统计单词出现次数\n",
    "    return Counter(words)\n",
    "\n",
    "# 定义从Kafka读取数据的函数\n",
    "def read_from_kafka():\n",
    "    # 获取StreamExecutionEnvironment实例\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()    \n",
    "    # 添加flink-sql-connector-kafka-3.1-SNAPSHOT.jar包\n",
    "    env.add_jars(\"file:///home/hadoop/Desktop/PyFlink-Tutorial/flink-sql-connector-kafka-3.1-SNAPSHOT.jar\")\n",
    "    # 打印从Kafka读取数据的信息\n",
    "    print(\"start reading data from kafka\")\n",
    "    # 创建FlinkKafkaConsumer实例\n",
    "    kafka_consumer = FlinkKafkaConsumer(\n",
    "        topics='hamlet', # The topic to consume messages from\n",
    "        deserialization_schema= SimpleStringSchema('UTF-8'), # The schema to deserialize messages\n",
    "        properties={'bootstrap.servers': 'localhost:9092', 'group.id': 'my-group'} # The Kafka broker address and consumer group ID\n",
    "    )\n",
    "    # 从最早的记录开始读取数据\n",
    "    kafka_consumer.set_start_from_earliest()\n",
    "    # 将FlinkKafkaConsumer实例添加到StreamExecutionEnvironment实例中\n",
    "    stream = env.add_source(kafka_consumer)\n",
    "    # 将StreamExecutionEnvironment实例中的数据映射为去除标点符号的文本\n",
    "    stream_remove_punctuation = stream.map(lambda x: remove_punctuation(x))\n",
    "    # 将去除标点符号的文本映射为统计单词的文本\n",
    "    stream_count_words = stream_remove_punctuation.map(lambda x: count_words(x))\n",
    "    # 打印统计单词的文本\n",
    "    stream_count_words.print()\n",
    "    # 执行StreamExecutionEnvironment实例\n",
    "    env.execute()\n",
    "\n",
    "# 调用read_from_kafka函数\n",
    "read_from_kafka()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更详细的词频统计\n",
    "\n",
    "# 导入 argparse、io、json、logging、os、pandas、re、Counter、StringIO、FlinkKafkaConsumer、StreamExecutionEnvironment、DataTypes、EnvironmentSettings、FormatDescriptor、Schema、StreamTableEnvironment、TableEnvironment、udf 模块\n",
    "import argparse\n",
    "import io\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from io import StringIO\n",
    "from pyflink.common import SimpleStringSchema, Time\n",
    "from pyflink.datastream.connectors.kafka import FlinkKafkaConsumer\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.table import (DataTypes, EnvironmentSettings, FormatDescriptor,\n",
    "                           Schema, StreamTableEnvironment, TableDescriptor,\n",
    "                           TableEnvironment, udf)\n",
    "from pyflink.table.expressions import col, lit\n",
    "\n",
    "# 定义去除标点符号的函数\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]','',text)\n",
    "\n",
    "# 定义计算字节数的函数\n",
    "def count_bytes(text):\n",
    "    return len(text.encode('utf-8'))\n",
    "\n",
    "# 定义计算单词数量的函数\n",
    "def count_words(text):\n",
    "    words = text.split()\n",
    "    result = dict(Counter(words))\n",
    "    max_word = max(result, key=result.get)\n",
    "    return {'total_bytes': count_bytes(text), 'total_words': len(words), 'most_frequent_word': max_word, 'most_frequent_word_count': result[max_word]}\n",
    "\n",
    "# 定义从Kafka读取数据的函数\n",
    "def read_from_kafka():\n",
    "    # 获取StreamExecutionEnvironment实例\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()  \n",
    "    # 添加flink-sql-connector-kafka-3.1-SNAPSHOT.jar包\n",
    "    env.add_jars(\"file:///home/hadoop/Desktop/PyFlink-Tutorial/flink-sql-connector-kafka-3.1-SNAPSHOT.jar\")\n",
    "    print(\"start reading data from kafka\")\n",
    "    # 创建FlinkKafkaConsumer实例，指定主题、反序列化函数、配置参数\n",
    "    kafka_consumer = FlinkKafkaConsumer(\n",
    "        topics='hamlet', \n",
    "        deserialization_schema= SimpleStringSchema('UTF-8'), \n",
    "        properties={'bootstrap.servers': 'localhost:9092', 'group.id': 'my-group'} \n",
    "    )\n",
    "    # 从最早的日志开始读取\n",
    "    kafka_consumer.set_start_from_earliest()\n",
    "    # 将Kafka日志流转换为流表\n",
    "    stream_original_text = env.add_source(kafka_consumer)\n",
    "    # 对流表中的每一行进行去除标点符号操作\n",
    "    stream_remove_punctuation = stream_original_text.map(lambda x: remove_punctuation(x))\n",
    "    # 对流表中的每一行进行计算单词数量的操作\n",
    "    stream_count_words = stream_remove_punctuation.map(lambda x: count_words(x))\n",
    "    # 将流表中的每一行打印出来\n",
    "    stream_count_words.print()\n",
    "    # 执行流计算\n",
    "    env.execute()\n",
    "read_from_kafka()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 玩转 CSV\n",
    "\n",
    "假设我们得到一个“data.csv”文件，其中包含任何内容，并且该文件中只有年份数据才是我们需要的。\n",
    "我们首先使用以下代码生成“StreamGeneratorCSV”，将“CSV”文件转换为“Kafka Stream”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一个简单的 CSV 流生成器\n",
    "\n",
    "#以下代码使用kafka-python模块将数据发送到本地Kafka集群。\n",
    "#此代码打开一个名为“hamlet.txt”的文本文件，并将其内容作为流发送到指定的 Kafka 主题“hamlet”：\n",
    "\n",
    "from kafka import KafkaProducer\n",
    "import time\n",
    "import os\n",
    "import chardet\n",
    "\n",
    "def send_file_to_kafka(file_path: str, topic: str, bootstrap_servers: str):\n",
    "    '''\n",
    "    Send a file to a Kafka topic\n",
    "    :param file_path: path to the local file\n",
    "    :param topic: Kafka topic to which the data should be sent\n",
    "    :param bootstrap_servers: address of the Kafka cluster\n",
    "    '''\n",
    "    # 创建一个KafkaProducer实例\n",
    "    producer = KafkaProducer(bootstrap_servers=bootstrap_servers)\n",
    "    # 获取文件大小\n",
    "    file_size = os.path.getsize(file_path)\n",
    "\n",
    "    # 检测文件编码\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        result = chardet.detect(f.read())\n",
    "        encoding = result[\"encoding\"]\n",
    "\n",
    "    # 获取文件行数\n",
    "    with open(file_path, \"r\", encoding=encoding) as f:\n",
    "        lines_total = len(f.readlines())\n",
    "\n",
    "    lines_send = 0\n",
    "    while True:\n",
    "        # 打开文件\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            while True:\n",
    "                # 读取文件10行\n",
    "                data = f.readlines(10)\n",
    "                if not data:\n",
    "                    break\n",
    "                # 将数据转换为字符串\n",
    "                data_str = str(data)\n",
    "                # 将字符串转换为字节\n",
    "                data_bytes = data_str.encode()\n",
    "                # 将字节发送到Kafka\n",
    "                producer.send(topic, data_bytes)\n",
    "                # 记录发送的行数\n",
    "                lines_send += 10\n",
    "                # 计算已发送的百分比\n",
    "                percent_sent = (lines_send / lines_total) * 100                \n",
    "                # 计算已发送的字节数\n",
    "                bytes_sent = len(data_bytes)\n",
    "                print(f\"Sent {bytes_sent} bytes {topic} {percent_sent:.2f}% sent\")\n",
    "                # 每3秒检查一次\n",
    "                time.sleep(3)\n",
    "                \n",
    "        # 询问是否继续发送\n",
    "        user_input = input(\"Press 'c' to continue sending the file or 'q' to quit: \")\n",
    "        if user_input == \"q\":\n",
    "            break\n",
    "# 调用send_file_to_kafka函数，将文件data.csv发送到Kafka主题data，Kafka集群的地址为localhost:9092\n",
    "send_file_to_kafka(\"./data.csv\",  \"data\", \"localhost:9092\")\n",
    "\n",
    "# 解释以上代码\n",
    "# 在这个代码中，send_file_to_kafka 函数接受三个参数：file_path、topic 和 bootstrap_servers。\n",
    "# file_path 是本地文件的路径，topic 是要将数据发送到的 Kafka 主题，bootstrap_servers 是 Kafka 集群的地址。\n",
    "# 该函数使用 with 语句打开文件，读取其内容，并将其作为流数据发送到指定的 Kafka 主题。在发送过程中，它会打印出传输进度，并使用 time.sleep 方法暂停 3 秒以控制发送速率。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 输出年份数值\n",
    "\n",
    "StreamShowerWithFlinkCSV.py 是一个使用 DataStream 处理 CSV 文件的 Python 脚本。实际上，下面的代码使用 re 函数。\n",
    "但这不重要，只是对从 CSV 文件生成的 DataStream 随便试试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StreamShowerWithFlinkCSV.py\n",
    "\n",
    "# 导入正则表达式模块、参数解析模块、日志模块、系统模块、numpy模块、pandas模块、pyflink模块\n",
    "import re\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from pyflink.table import StreamTableEnvironment\n",
    "from pyflink.common import WatermarkStrategy, Encoder, Types\n",
    "from pyflink.datastream import StreamExecutionEnvironment, RuntimeExecutionMode\n",
    "from pyflink.datastream.connectors.file_system import FileSource, StreamFormat, FileSink, OutputFileConfig, RollingPolicy\n",
    "from pyflink.common import Types, SimpleStringSchema\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.datastream.connectors.kafka import FlinkKafkaProducer, FlinkKafkaConsumer\n",
    "\n",
    "# 定义split函数，用于将字符串拆分成单个单词\n",
    "def split(line):\n",
    "    yield from line.split()\n",
    "\n",
    "# 定义read_from_kafka函数，用于从Kafka读取数据\n",
    "def read_from_kafka():\n",
    "    # 定义Kafka消费的起始年份\n",
    "    Year_Begin =1999\n",
    "    # 定义Kafka消费的结束年份\n",
    "    Year_End = 2023\n",
    "    # 获取StreamExecutionEnvironment实例\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()    \n",
    "    # 添加jars包\n",
    "    env.add_jars(\"file:///home/hadoop/Desktop/PyFlink-Tutorial/flink-sql-connector-kafka-3.1-SNAPSHOT.jar\")\n",
    "    # 打印开始读取Kafka数据\n",
    "    print(\"start reading data from kafka\")\n",
    "\n",
    "    # 创建Kafka消费者，用于从Kafka读取数据\n",
    "    kafka_consumer = FlinkKafkaConsumer(\n",
    "        topics='data', # The topic to consume messages from\n",
    "        deserialization_schema= SimpleStringSchema('UTF-8'), # The schema to deserialize messages\n",
    "        properties={'bootstrap.servers': 'localhost:9092', 'group.id': 'my-group'} # The Kafka broker address and consumer group ID\n",
    "    )\n",
    "\n",
    "    # 从最早的偏移量开始读取Kafka数据\n",
    "    kafka_consumer.set_start_from_earliest()\n",
    "\n",
    "    # 添加Kafka消费者，并过滤掉不在指定年份范围内的数据\n",
    "    env.add_source(kafka_consumer).map(lambda x: ' '.join(re.findall(r'\\d+', x))).filter(lambda x: any([Year_Begin <= int(i) <= Year_End for i in x.split()])).map(lambda x:  [i for i in x.split() if Year_Begin <= int(i) <= Year_End][0]).print()\n",
    "    # 执行StreamExecutionEnvironment\n",
    "    env.execute()\n",
    "\n",
    "# 调用read_from_kafka函数\n",
    "if __name__ == '__main__':\n",
    "    read_from_kafka()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MapFunction`: 将一个元素作为输入并将一个元素作为输出的函数。通过对每个元素应用转换，它可用于转换数据流。\n",
    "`FlatMapFunction`：将一个元素作为输入，并将零个、一个或多个元素作为输出的函数。它可通过对每个元素应用变换来转换数据流。\n",
    "`FilterFunction`: 将一个元素作为输入并返回一个布尔值的函数。它可用于删除不符合特定条件的元素，从而过滤数据流。\n",
    "`KeySelector`: 从元素中提取键的函数。它可用于按键对数据流中的元素进行分组。\n",
    "`ReduceFunction`: 还原函数 将两个元素作为输入并将一个元素作为输出的函数。它可以通过组合共享一个共同键的元素来聚合数据流。\n",
    "`WindowFunction`: 将元素窗口作为输入并将一个或多个元素作为输出的函数。它可用于在数据流上定义窗口，并对每个窗口内的元素进行转换。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
