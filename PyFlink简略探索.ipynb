{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyFlink 安装\n",
    "\n",
    "### 作者：胖胖揽住\n",
    "\n",
    "### 版本 2023.11.15\n",
    "\n",
    "## Anaconda3 安装\n",
    "\n",
    "首先从TUNA下载Anaconda3安装包。\n",
    "\n",
    "```Bash\n",
    "wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-2023.09-0-Linux-x86_64.sh\n",
    "sh Anaconda3-2023.09-0-Linux-x86_64.sh\n",
    "```\n",
    "安装过程中，请使用默认设置。\n",
    "应该安装在`~/anaconda3`。\n",
    "\n",
    "## Python 3.9 安装\n",
    "\n",
    "通过 conda 安装 Python 3.9 将变得简单可靠。\n",
    "\n",
    "```Bash\n",
    "conda create -n pyflink_39 python=3.9\n",
    "conda activate pyflink_39\n",
    "```\n",
    "\n",
    "\n",
    "## Apache-Flink 安装\n",
    "\n",
    "先去[Apache 官网](https://dlcdn.apache.org/flink/)下载安装 flink，这里以 1.18.0 为例：\n",
    "\n",
    "```Bash\n",
    "wget https://dlcdn.apache.org/flink/flink-1.18.0/flink-1.18.0-bin-scala_2.12.tgz\n",
    "sudo tar -zxvf flink-1.18.0-bin-scala_2.12.tgz  -C /usr/local   \n",
    "```\n",
    "\n",
    "修改目录名称，并设置权限，命令如下：\n",
    "```Bash\n",
    "cd /usr/local\n",
    "sudo mv / flink-1.18.0 ./flink #这里是因为我这里下的是这个版本，读者需要酌情调整\n",
    "sudo chown -R hadoop:hadoop ./flink ##这里是因为我这里虚拟机的用户名是这个，读者需要酌情调整\n",
    "```\n",
    "\n",
    "Flink解压缩并且设置好权限后，直接就可以在本地模式运行，不需要修改任何配置。\n",
    "如果要做调整，可以编辑`“/usr/local/flink/conf/flink-conf.yam`这个文件。\n",
    "比如其中的`env.java.home`参就可以设置为本地Java的绝对路径\n",
    "不过一般不需要手动修改什么配置。\n",
    "\n",
    "不过，需要注意的是，Flink现在需要的是Java11，所以需要用下列命令手动安装一下：\n",
    "```Bash\n",
    "sudo apt install openjdk-11-jdk -y\n",
    "```\n",
    "\n",
    "接下来还需要修接下来还需要修改配置文件，添加环境变量：\n",
    "\n",
    "```Bash\n",
    "nano ~/.bashrc\n",
    "```\n",
    "\n",
    "文件中添加如下内容：\n",
    "```\n",
    "export FLINK_HOME=/usr/local/flink\n",
    "export PATH=$FLINK_HOME/bin:$PATH\n",
    "```\n",
    "\n",
    "保存并退出.bashrc文件，然后执行如下命令让配置文件生效：\n",
    "```Bash\n",
    "source ~/.bashrc\n",
    "```\n",
    "\n",
    "## 安装 Python 依赖包\n",
    "\n",
    "然后使用 pip 安装 apache-flink 包， 以及 Kafka-python 等等依赖包\n",
    "\n",
    "```Bash\n",
    "pip install apache-flink \n",
    "pip install kafka-python chardet pandas numpy scipy simpy \n",
    "pip install matplotlib cython sympy xlrd pyopengl BeautifulSoup4 pyqt6 scikit-learn requests tensorflow torch keras tqdm gym DRL\n",
    "```\n",
    "\n",
    "## 代码说明\n",
    "\n",
    "本文代码修改自官方[文档版本1.18](https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/dev/python/datastream_tutorial/)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本操作：Map, Filter, Keyby\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "from pyflink.common import Types\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "\n",
    "# 定义show函数，用于显示数据流\n",
    "def show(ds, env):\n",
    "    ds.print()\n",
    "    env.execute()\n",
    "\n",
    "# 定义update_tel函数，用于更新tel字段\n",
    "def update_tel(data):\n",
    "    json_data = json.loads(data.info)\n",
    "    json_data['tel'] += 1\n",
    "    return data.id, json.dumps(json_data)\n",
    "\n",
    "# 定义filter_by_id函数，用于过滤id字段\n",
    "def filter_by_id(data):\n",
    "    return data.id == 1\n",
    "\n",
    "# 定义map_country_tel函数，用于将国家字段和tel字段映射到元组中\n",
    "def map_country_tel(data):\n",
    "    json_data = json.loads(data.info)\n",
    "    return json_data['addr']['country'], json_data['tel']\n",
    "\n",
    "# 定义key_by_country函数，用于将元组中的国家字段作为key\n",
    "def key_by_country(data):\n",
    "    return data[0]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=\"%(message)s\")\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()\n",
    "    env.set_parallelism(1)\n",
    "    ds = env.from_collection(\n",
    "        collection=[\n",
    "            (1, '{\"name\": \"Flink\", \"tel\": 111, \"addr\": {\"country\": \"Germany\", \"city\": \"Berlin\"}}'),\n",
    "            (2, '{\"name\": \"hello\", \"tel\": 222, \"addr\": {\"country\": \"China\", \"city\": \"Shanghai\"}}'),\n",
    "            (3, '{\"name\": \"world\", \"tel\": 333, \"addr\": {\"country\": \"USA\", \"city\": \"NewYork\"}}'),\n",
    "            (4, '{\"name\": \"PyFlink\", \"tel\": 444, \"addr\": {\"country\": \"China\", \"city\": \"Hangzhou\"}}')\n",
    "        ],\n",
    "        type_info=Types.ROW_NAMED([\"id\", \"info\"], [Types.INT(), Types.STRING()])\n",
    "    )\n",
    "    print('\\nFirst we map it: \\n')\n",
    "    # 调用show函数，显示数据流\n",
    "    show(ds.map(update_tel), env)\n",
    "    \n",
    "    print('\\nThen we filter it: \\n')\n",
    "    # 调用show函数，显示过滤后的数据流\n",
    "    show(ds.filter(filter_by_id), env)\n",
    "\n",
    "    print('\\nThen we select it: \\n')\n",
    "    # 调用show函数，显示按照国家字段分组后的数据流\n",
    "    show(ds.map(map_country_tel).key_by(key_by_country), env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing word_count example with default input data set.\n",
      "Use --input to specify file input.\n",
      "Printing result to stdout. Use --output to specify output path.\n"
     ]
    }
   ],
   "source": [
    "# 处理 Json 数据\n",
    "\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "\n",
    "\n",
    "def process_json_data():\n",
    "    # 获取执行环境\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()\n",
    "\n",
    "    # define the source\n",
    "    # 定义源数据\n",
    "    ds = env.from_collection(\n",
    "        collection=[\n",
    "            (1, '{\"name\": \"Flink\", \"tel\": 111, \"addr\": {\"country\": \"Germany\", \"city\": \"Berlin\"}}'),\n",
    "            (2, '{\"name\": \"hello\", \"tel\": 222, \"addr\": {\"country\": \"China\", \"city\": \"Shanghai\"}}'),\n",
    "            (3, '{\"name\": \"world\", \"tel\": 333, \"addr\": {\"country\": \"USA\", \"city\": \"NewYork\"}}'),\n",
    "            (4, '{\"name\": \"PyFlink\", \"tel\": 444, \"addr\": {\"country\": \"China\", \"city\": \"Hangzhou\"}}')]\n",
    "    )\n",
    "\n",
    "    # 定义更新电话号码的函数\n",
    "    def update_tel(data):\n",
    "        # parse the json\n",
    "        # 解析json数据\n",
    "        json_data = json.loads(data[1])\n",
    "        # 更新电话号码\n",
    "        json_data['tel'] += 1\n",
    "        # 返回更新后的数据\n",
    "        return data[0], json_data\n",
    "\n",
    "    # 定义过滤函数，过滤掉国家不是中国的数据\n",
    "    def filter_by_country(data):\n",
    "        # the json data could be accessed directly, there is no need to parse it again using\n",
    "        # json.loads\n",
    "        # 直接访问json数据，不需要使用json.loads\n",
    "        return \"China\" in data[1]['addr']['country']\n",
    "\n",
    "    # 调用map函数，更新电话号码，并过滤掉国家不是中国的数据\n",
    "    ds.map(update_tel).filter(filter_by_country).print()\n",
    "\n",
    "    # submit for execution\n",
    "    # 提交执行\n",
    "    env.execute()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 设置日志输出格式\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=\"%(message)s\")\n",
    "\n",
    "    # 调用process_json_data函数\n",
    "    process_json_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 状态读取\n",
    "\n",
    "from pyflink.common import Time\n",
    "from pyflink.common.typeinfo import Types\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.datastream.functions import KeyedProcessFunction, RuntimeContext\n",
    "from pyflink.datastream.state import ValueStateDescriptor, StateTtlConfig\n",
    "\n",
    "\n",
    "# 定义一个Sum类，继承自KeyedProcessFunction\n",
    "class Sum(KeyedProcessFunction):\n",
    "\n",
    "    # 初始化函数\n",
    "    def __init__(self):\n",
    "        self.state = None\n",
    "\n",
    "    # 打开函数，获取运行时上下文\n",
    "    def open(self, runtime_context: RuntimeContext):\n",
    "        # 创建一个状态描述符，类型为float\n",
    "        state_descriptor = ValueStateDescriptor(\"state\", Types.FLOAT())\n",
    "        # 创建一个状态TTL配置，设置TTL时间为1秒，更新类型为OnReadAndWrite，禁用后台清理\n",
    "        state_ttl_config = StateTtlConfig \\\n",
    "            .new_builder(Time.seconds(1)) \\\n",
    "            .set_update_type(StateTtlConfig.UpdateType.OnReadAndWrite) \\\n",
    "            .disable_cleanup_in_background() \\\n",
    "            .build()\n",
    "        # 启用TTL，并传入TTL配置\n",
    "        state_descriptor.enable_time_to_live(state_ttl_config)\n",
    "        # 获取状态\n",
    "        self.state = runtime_context.get_state(state_descriptor)\n",
    "\n",
    "    # 处理元素函数\n",
    "    def process_element(self, value, ctx: 'KeyedProcessFunction.Context'):\n",
    "        # retrieve the current count\n",
    "        # 获取当前状态\n",
    "        current = self.state.value()\n",
    "        # 如果当前状态为空，则设置为0\n",
    "        if current is None:\n",
    "            current = 0\n",
    "\n",
    "        # update the state's count\n",
    "        # 更新状态的计数\n",
    "        current += value[1]\n",
    "        # 更新状态\n",
    "        self.state.update(current)\n",
    "\n",
    "        # 返回元组\n",
    "        yield value[0], current\n",
    "\n",
    "\n",
    "# 定义一个state_access_demo函数，用于演示状态访问\n",
    "def state_access_demo():\n",
    "    # 获取运行时环境\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()\n",
    "\n",
    "    # 从集合中创建一个流\n",
    "    ds = env.from_collection(\n",
    "        collection=[\n",
    "            ('Alice', 110.1),\n",
    "            ('Bob', 30.2),\n",
    "            ('Alice', 20.0),\n",
    "            ('Bob', 53.1),\n",
    "            ('Alice', 13.1),\n",
    "            ('Bob', 3.1),\n",
    "            ('Bob', 16.1),\n",
    "            ('Alice', 20.1)\n",
    "        ],\n",
    "        type_info=Types.TUPLE([Types.STRING(), Types.FLOAT()]))\n",
    "\n",
    "    # apply the process function onto a keyed stream\n",
    "    # 应用处理函数，对流中的每一个元素进行处理\n",
    "    ds.key_by(lambda value: value[0]) \\\n",
    "      .process(Sum()) \\\n",
    "      .print()\n",
    "\n",
    "    # submit for execution\n",
    "    # 提交执行\n",
    "    env.execute()\n",
    "\n",
    "\n",
    "# 调用state_access_demo函数\n",
    "if __name__ == '__main__':\n",
    "    state_access_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 事件时间\n",
    "\n",
    "\n",
    "from pyflink.common import Time, WatermarkStrategy, Duration\n",
    "from pyflink.common.typeinfo import Types\n",
    "from pyflink.common.watermark_strategy import TimestampAssigner\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.datastream.functions import KeyedProcessFunction, RuntimeContext\n",
    "from pyflink.datastream.state import ValueStateDescriptor, StateTtlConfig\n",
    "\n",
    "\n",
    "# 定义Sum类，继承自KeyedProcessFunction\n",
    "class Sum(KeyedProcessFunction):\n",
    "\n",
    "    # 初始化函数\n",
    "    def __init__(self):\n",
    "        self.state = None\n",
    "\n",
    "    # 打开函数，获取状态描述符，设置状态TTL配置，并设置状态描述符的TTL配置\n",
    "    def open(self, runtime_context: RuntimeContext):\n",
    "        state_descriptor = ValueStateDescriptor(\"state\", Types.FLOAT())\n",
    "        state_ttl_config = StateTtlConfig \\\n",
    "            .new_builder(Time.seconds(1)) \\\n",
    "            .set_update_type(StateTtlConfig.UpdateType.OnReadAndWrite) \\\n",
    "            .disable_cleanup_in_background() \\\n",
    "            .build()\n",
    "        state_descriptor.enable_time_to_live(state_ttl_config)\n",
    "        self.state = runtime_context.get_state(state_descriptor)\n",
    "\n",
    "    # 处理元素函数，获取当前状态，更新状态，并注册一个2秒后的定时器\n",
    "    def process_element(self, value, ctx: 'KeyedProcessFunction.Context'):\n",
    "        # retrieve the current count\n",
    "        current = self.state.value()\n",
    "        if current is None:\n",
    "            current = 0\n",
    "\n",
    "        # update the state's count\n",
    "        current += value[2]\n",
    "        self.state.update(current)\n",
    "\n",
    "        # register an event time timer 2 seconds later\n",
    "        ctx.timer_service().register_event_time_timer(ctx.timestamp() + 2000)\n",
    "\n",
    "    # 定时器函数，获取当前状态，并输出\n",
    "    def on_timer(self, timestamp: int, ctx: 'KeyedProcessFunction.OnTimerContext'):\n",
    "        yield ctx.get_current_key(), self.state.value()\n",
    "\n",
    "\n",
    "# 定义MyTimestampAssigner类，继承自TimestampAssigner\n",
    "class MyTimestampAssigner(TimestampAssigner):\n",
    "\n",
    "    # 提取时间戳函数，根据value和record_timestamp获取时间戳\n",
    "    def extract_timestamp(self, value, record_timestamp: int) -> int:\n",
    "        return int(value[0])\n",
    "\n",
    "\n",
    "# 定义event_timer_timer_demo函数，获取执行环境，从集合中获取数据，设置时间戳和水位策略，并应用处理函数，提交执行\n",
    "def event_timer_timer_demo():\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()\n",
    "\n",
    "    ds = env.from_collection(\n",
    "        collection=[\n",
    "            (1000, 'Alice', 110.1),\n",
    "            (4000, 'Bob', 30.2),\n",
    "            (3000, 'Alice', 20.0),\n",
    "            (2000, 'Bob', 53.1),\n",
    "            (5000, 'Alice', 13.1),\n",
    "            (3000, 'Bob', 3.1),\n",
    "            (7000, 'Bob', 16.1),\n",
    "            (10000, 'Alice', 20.1)\n",
    "        ],\n",
    "        type_info=Types.TUPLE([Types.LONG(), Types.STRING(), Types.FLOAT()]))\n",
    "\n",
    "    ds = ds.assign_timestamps_and_watermarks(\n",
    "        WatermarkStrategy.for_bounded_out_of_orderness(Duration.of_seconds(2))\n",
    "                         .with_timestamp_assigner(MyTimestampAssigner()))\n",
    "\n",
    "    # apply the process function onto a keyed stream\n",
    "    ds.key_by(lambda value: value[1]) \\\n",
    "      .process(Sum()) \\\n",
    "      .print()\n",
    "\n",
    "    # submit for execution\n",
    "    env.execute()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    event_timer_timer_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用Docker搭建本地Kafka集群\n",
    "\n",
    "操作系统选择 Ubuntu 22.04.3   \n",
    "\n",
    "1. 安装 Docker 和 Docker Compose:\n",
    "```Bash\n",
    "sudo apt install Docker Docker-compose\n",
    "```\n",
    "2. 创建本地 `docker-compose.yml` 文件，其中包含以下内容：\n",
    "\n",
    "```yaml\n",
    "version: '3'\n",
    "services:\n",
    "  zookeeper:\n",
    "    image: 'bitnami/zookeeper:latest'\n",
    "    environment:\n",
    "      - ALLOW_ANONYMOUS_LOGIN=yes\n",
    "  kafka:\n",
    "    image: 'bitnami/kafka:latest'\n",
    "    ports:\n",
    "      - '9092:9092'\n",
    "    environment:\n",
    "      - KAFKA_ADVERTISED_HOST_NAME=localhost\n",
    "      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181\n",
    "      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092\n",
    "      - KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9092\n",
    "      - KAFKA_CREATE_TOPICS=test:1:1\n",
    "      - ALLOW_PLAINTEXT_LISTENER=yes\n",
    "    depends_on:\n",
    "      - zookeeper\n",
    "```\n",
    "\n",
    "3. 找到“docker-compose.yml”所在目录并运行以下命令：\n",
    "\n",
    "````Bash\n",
    "docker-compose up -d\n",
    "````\n",
    "\n",
    "这将运行一个包含 Zookeeper 实例和 Kafka 实例的本地 Kafka 集群，该集群将在本地主机的端口 9092 上运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 kafka-python 生成流的简单方法\n",
    "\n",
    "# 以下代码使用kafka-python模块将数据发送到本地Kafka集群。\n",
    "# 此代码打开一个名为 `hamlet.txt` 的文本文件，并将其内容作为流发送到指定的 Kafka 主题 `hamlet`：\n",
    "\n",
    "# 导入KafkaProducer模块\n",
    "from kafka import KafkaProducer\n",
    "# 导入time模块\n",
    "import time\n",
    "# 导入os模块\n",
    "import os\n",
    "\n",
    "# 定义一个函数，用于将文件发送到Kafka\n",
    "def send_file_to_kafka(file_path: str, topic: str, bootstrap_servers: str):\n",
    "    # 创建一个KafkaProducer实例，用于发送消息\n",
    "    producer = KafkaProducer(bootstrap_servers=bootstrap_servers)\n",
    "    # 获取文件大小\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    # 循环发送文件\n",
    "    while True:\n",
    "        # 打开文件\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            # 循环读取文件\n",
    "            while True:\n",
    "                # 读取文件内容\n",
    "                data = f.read(1024)\n",
    "                # 如果没有内容，则跳出循环\n",
    "                if not data:\n",
    "                    break\n",
    "                # 将文件内容发送到Kafka\n",
    "                producer.send(topic, data)\n",
    "                # 计算发送的字节数\n",
    "                bytes_sent = len(data)\n",
    "                # 打印发送的字节数\n",
    "                print(f\"Sent {bytes_sent} bytes to Kafka topic {topic}\")\n",
    "                # 计算发送的百分比\n",
    "                percent_sent = (f.tell() / file_size) * 100\n",
    "                # 打印发送的百分比\n",
    "                print(f\"{percent_sent:.2f}% of the file sent\")\n",
    "                # 等待3秒\n",
    "                time.sleep(3)\n",
    "        # 获取用户输入\n",
    "        user_input = input(\"Press 'c' to continue sending the file or 'q' to quit: \")\n",
    "        # 如果用户输入q，则退出循环\n",
    "        if user_input == \"q\":\n",
    "            break\n",
    "# 调用函数，将hamlet.txt文件发送到Kafka的hamlet主题\n",
    "send_file_to_kafka(\"./hamlet.txt\",  \"hamlet\", \"localhost:9092\")\n",
    "# 在此代码中，send_file_to_kafka 函数接受三个参数：file_path、topic 和 bootstrap_servers。\n",
    "# file_path是本地文件的路径，topic是数据要发送到的Kafka主题，bootstrap_servers是Kafka集群的地址。\n",
    "# 该函数使用with语句打开文件，读取其内容，并将它们作为流数据发送到指定的Kafka主题。\n",
    "# 发送过程中，打印出发送进度，并使用time.sleep方法暂停0.1秒来控制发送速率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 kafka-python 展现流数据的简单方法\n",
    "\n",
    "from kafka import KafkaConsumer\n",
    "\n",
    "# 创建一个KafkaConsumer实例，用于从Kafka主题中读取消息\n",
    "consumer = KafkaConsumer(\n",
    "    # 指定要读取的消息主题\n",
    "    \"hamlet\",\n",
    "    # 指定Kafka服务器的地址和端口\n",
    "    bootstrap_servers=[\"localhost:9092\"],\n",
    "    # 指定当消费者重新启动时，它应该从哪个偏移量开始读取消息\n",
    "    auto_offset_reset=\"earliest\",\n",
    "    # 指定是否在消费者处理消息时，应该提交偏移量\n",
    "    enable_auto_commit=True,\n",
    "    # 指定消费者组，用于提交偏移量\n",
    "    group_id=\"my-group\",\n",
    "    # 指定消息的解码方式\n",
    "    value_deserializer=lambda x: x.decode(\"utf-8\")\n",
    ")\n",
    "\n",
    "# 循环读取Kafka主题中的消息，并打印消息长度和消息内容\n",
    "for message in consumer:\n",
    "    print(f\"Received {len(message.value)} bytes from Kafka topic {message.topic}\")\n",
    "    print(f\"{message.value}\")\n",
    "\n",
    "# 在上面的代码中，我们使用`KafkaConsumer`类来创建一个消费者对象。\n",
    "# 我们将 `hamlet` 作为主题名称传递给构造函数。\n",
    "# 我们还传递 `localhost:9092` 作为引导服务器的地址。\n",
    "# 我们使用 `value_deserializer` 参数来解码从 Kafka 主题收到的消息。\n",
    "# 我们使用 `for` 循环来迭代消费者对象，并使用 `print` 函数来打印消息的内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start reading data from kafka\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o0.execute.\n: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.\n\tat org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)\n\tat org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)\n\tat java.base/java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:646)\n\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)\n\tat java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2147)\n\tat org.apache.flink.runtime.rpc.pekko.PekkoInvocationHandler.lambda$invokeRpc$1(PekkoInvocationHandler.java:268)\n\tat java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:863)\n\tat java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:841)\n\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)\n\tat java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2147)\n\tat org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1267)\n\tat org.apache.flink.runtime.concurrent.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)\n\tat org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)\n\tat org.apache.flink.runtime.concurrent.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)\n\tat java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:863)\n\tat java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:841)\n\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)\n\tat java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2147)\n\tat org.apache.flink.runtime.concurrent.pekko.ScalaFutureUtils$1.onComplete(ScalaFutureUtils.java:47)\n\tat org.apache.pekko.dispatch.OnComplete.internal(Future.scala:310)\n\tat org.apache.pekko.dispatch.OnComplete.internal(Future.scala:307)\n\tat org.apache.pekko.dispatch.japi$CallbackBridge.apply(Future.scala:234)\n\tat org.apache.pekko.dispatch.japi$CallbackBridge.apply(Future.scala:231)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat org.apache.flink.runtime.concurrent.pekko.ScalaFutureUtils$DirectExecutionContext.execute(ScalaFutureUtils.java:65)\n\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n\tat org.apache.pekko.pattern.PromiseActorRef.$bang(AskSupport.scala:629)\n\tat org.apache.pekko.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:34)\n\tat org.apache.pekko.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:33)\n\tat scala.concurrent.Future.$anonfun$andThen$1(Future.scala:536)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat org.apache.pekko.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:73)\n\tat org.apache.pekko.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:110)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n\tat org.apache.pekko.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:110)\n\tat org.apache.pekko.dispatch.TaskInvocation.run(AbstractDispatcher.scala:59)\n\tat org.apache.pekko.dispatch.ForkJoinExecutorConfigurator$PekkoForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:57)\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)\nCaused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy\n\tat org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:176)\n\tat org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:107)\n\tat org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:285)\n\tat org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:276)\n\tat org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:269)\n\tat org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:764)\n\tat org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:741)\n\tat org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:83)\n\tat org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:488)\n\tat jdk.internal.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.lambda$handleRpcInvocation$1(PekkoRpcActor.java:309)\n\tat org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)\n\tat org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcInvocation(PekkoRpcActor.java:307)\n\tat org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcMessage(PekkoRpcActor.java:222)\n\tat org.apache.flink.runtime.rpc.pekko.FencedPekkoRpcActor.handleRpcMessage(FencedPekkoRpcActor.java:85)\n\tat org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleMessage(PekkoRpcActor.java:168)\n\tat org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:33)\n\tat org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:29)\n\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\tat org.apache.pekko.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:29)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)\n\tat org.apache.pekko.actor.Actor.aroundReceive(Actor.scala:547)\n\tat org.apache.pekko.actor.Actor.aroundReceive$(Actor.scala:545)\n\tat org.apache.pekko.actor.AbstractActor.aroundReceive(AbstractActor.scala:229)\n\tat org.apache.pekko.actor.ActorCell.receiveMessage(ActorCell.scala:590)\n\tat org.apache.pekko.actor.ActorCell.invoke(ActorCell.scala:557)\n\tat org.apache.pekko.dispatch.Mailbox.processMailbox(Mailbox.scala:280)\n\tat org.apache.pekko.dispatch.Mailbox.run(Mailbox.scala:241)\n\tat org.apache.pekko.dispatch.Mailbox.exec(Mailbox.scala:253)\n\t... 5 more\nCaused by: org.apache.flink.kafka.shaded.org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m     env\u001b[38;5;241m.\u001b[39mexecute()\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 42\u001b[0m     \u001b[43mread_from_kafka\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 39\u001b[0m, in \u001b[0;36mread_from_kafka\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m env\u001b[38;5;241m.\u001b[39madd_source(kafka_consumer)\u001b[38;5;241m.\u001b[39mprint()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# 执行执行环境\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pyflink_39/lib/python3.9/site-packages/pyflink/datastream/stream_execution_environment.py:773\u001b[0m, in \u001b[0;36mStreamExecutionEnvironment.execute\u001b[0;34m(self, job_name)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;124;03mTriggers the program execution. The environment will execute all parts of\u001b[39;00m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;124;03mthe program that have resulted in a \"sink\" operation. Sink operations are\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;124;03m:return: The result of the job execution, containing elapsed time and accumulators.\u001b[39;00m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    772\u001b[0m j_stream_graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_stream_graph(clear_transformations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, job_name\u001b[38;5;241m=\u001b[39mjob_name)\n\u001b[0;32m--> 773\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m JobExecutionResult(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_j_stream_execution_environment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mj_stream_graph\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pyflink_39/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/pyflink_39/lib/python3.9/site-packages/pyflink/util/exceptions.py:146\u001b[0m, in \u001b[0;36mcapture_java_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyflink\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjava_gateway\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_gateway\n",
      "File \u001b[0;32m~/anaconda3/envs/pyflink_39/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o0.execute.\n: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.\n\tat org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)\n\tat org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)\n\tat java.base/java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:646)\n\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)\n\tat java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2147)\n\tat org.apache.flink.runtime.rpc.pekko.PekkoInvocationHandler.lambda$invokeRpc$1(PekkoInvocationHandler.java:268)\n\tat java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:863)\n\tat java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:841)\n\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)\n\tat java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2147)\n\tat org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1267)\n\tat org.apache.flink.runtime.concurrent.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)\n\tat org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)\n\tat org.apache.flink.runtime.concurrent.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)\n\tat java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:863)\n\tat java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:841)\n\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)\n\tat java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2147)\n\tat org.apache.flink.runtime.concurrent.pekko.ScalaFutureUtils$1.onComplete(ScalaFutureUtils.java:47)\n\tat org.apache.pekko.dispatch.OnComplete.internal(Future.scala:310)\n\tat org.apache.pekko.dispatch.OnComplete.internal(Future.scala:307)\n\tat org.apache.pekko.dispatch.japi$CallbackBridge.apply(Future.scala:234)\n\tat org.apache.pekko.dispatch.japi$CallbackBridge.apply(Future.scala:231)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat org.apache.flink.runtime.concurrent.pekko.ScalaFutureUtils$DirectExecutionContext.execute(ScalaFutureUtils.java:65)\n\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n\tat org.apache.pekko.pattern.PromiseActorRef.$bang(AskSupport.scala:629)\n\tat org.apache.pekko.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:34)\n\tat org.apache.pekko.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:33)\n\tat scala.concurrent.Future.$anonfun$andThen$1(Future.scala:536)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat org.apache.pekko.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:73)\n\tat org.apache.pekko.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:110)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n\tat org.apache.pekko.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:110)\n\tat org.apache.pekko.dispatch.TaskInvocation.run(AbstractDispatcher.scala:59)\n\tat org.apache.pekko.dispatch.ForkJoinExecutorConfigurator$PekkoForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:57)\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)\nCaused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy\n\tat org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:176)\n\tat org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:107)\n\tat org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:285)\n\tat org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:276)\n\tat org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:269)\n\tat org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:764)\n\tat org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:741)\n\tat org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:83)\n\tat org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:488)\n\tat jdk.internal.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.lambda$handleRpcInvocation$1(PekkoRpcActor.java:309)\n\tat org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)\n\tat org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcInvocation(PekkoRpcActor.java:307)\n\tat org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcMessage(PekkoRpcActor.java:222)\n\tat org.apache.flink.runtime.rpc.pekko.FencedPekkoRpcActor.handleRpcMessage(FencedPekkoRpcActor.java:85)\n\tat org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleMessage(PekkoRpcActor.java:168)\n\tat org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:33)\n\tat org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:29)\n\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\tat org.apache.pekko.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:29)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)\n\tat org.apache.pekko.actor.Actor.aroundReceive(Actor.scala:547)\n\tat org.apache.pekko.actor.Actor.aroundReceive$(Actor.scala:545)\n\tat org.apache.pekko.actor.AbstractActor.aroundReceive(AbstractActor.scala:229)\n\tat org.apache.pekko.actor.ActorCell.receiveMessage(ActorCell.scala:590)\n\tat org.apache.pekko.actor.ActorCell.invoke(ActorCell.scala:557)\n\tat org.apache.pekko.dispatch.Mailbox.processMailbox(Mailbox.scala:280)\n\tat org.apache.pekko.dispatch.Mailbox.run(Mailbox.scala:241)\n\tat org.apache.pekko.dispatch.Mailbox.exec(Mailbox.scala:253)\n\t... 5 more\nCaused by: org.apache.flink.kafka.shaded.org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata\n"
     ]
    }
   ],
   "source": [
    "# 使用 pyflink 进行流数据展现\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from pyflink.table import StreamTableEnvironment\n",
    "from pyflink.common import WatermarkStrategy, Encoder, Types\n",
    "from pyflink.datastream import StreamExecutionEnvironment, RuntimeExecutionMode\n",
    "from pyflink.datastream.connectors.file_system import FileSource, StreamFormat, FileSink, OutputFileConfig, RollingPolicy\n",
    "from pyflink.common import Types, SimpleStringSchema\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.datastream.connectors.kafka import FlinkKafkaProducer, FlinkKafkaConsumer\n",
    "\n",
    "def split(line):\n",
    "    # 将行拆分成单词\n",
    "    yield from line.split()\n",
    "\n",
    "def read_from_kafka():\n",
    "    # 从Kafka读取数据\n",
    "    # 获取当前的执行环境\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()    \n",
    "    # 添加kafka连接器\n",
    "    env.add_jars(\"file:///home/hadoop/Desktop/PyFlink-Tutorial/flink-sql-connector-kafka-3.1-SNAPSHOT.jar\")\n",
    "    print(\"start reading data from kafka\")\n",
    "    # 创建一个kafka消费者，用于从kafka中读取消息\n",
    "    kafka_consumer = FlinkKafkaConsumer(\n",
    "        topics='hamlet', # The topic to consume messages from\n",
    "        deserialization_schema= SimpleStringSchema('UTF-8'), # The schema to deserialize messages\n",
    "        properties={'bootstrap.servers': 'localhost:9092', 'group.id': 'my-group'} # The Kafka broker address and consumer group ID\n",
    "    )\n",
    "    # 从最早的记录开始读取数据\n",
    "    kafka_consumer.set_start_from_earliest()\n",
    "    # 将kafka消费者添加到执行环境中，并打印输出\n",
    "    env.add_source(kafka_consumer).print()\n",
    "    # 执行执行环境\n",
    "    env.execute()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    read_from_kafka()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简单的词频统计\n",
    "\n",
    "# 导入os模块\n",
    "import os\n",
    "# 导入re模块\n",
    "import re\n",
    "# 导入Counter模块\n",
    "from collections import Counter\n",
    "# 导入StreamTableEnvironment模块\n",
    "from pyflink.table import StreamTableEnvironment\n",
    "# 导入StreamExecutionEnvironment模块\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "# 导入FlinkKafkaConsumer模块\n",
    "from pyflink.datastream.connectors.kafka import FlinkKafkaConsumer\n",
    "# 导入SimpleStringSchema模块\n",
    "from pyflink.common import SimpleStringSchema\n",
    "\n",
    "# 定义去除标点符号的函数\n",
    "def remove_punctuation(text):\n",
    "    # 使用正则表达式去除标点符号\n",
    "    return re.sub(r'[^\\w\\s]','',text)\n",
    "\n",
    "# 定义统计单词的函数\n",
    "def count_words(text):\n",
    "    # 将文本按空格分割成单词列表\n",
    "    words = text.split()\n",
    "    # 使用Counter模块统计单词出现次数\n",
    "    return Counter(words)\n",
    "\n",
    "# 定义从Kafka读取数据的函数\n",
    "def read_from_kafka():\n",
    "    # 获取StreamExecutionEnvironment实例\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()    \n",
    "    # 添加flink-sql-connector-kafka-3.1-SNAPSHOT.jar包\n",
    "    env.add_jars(\"file:///home/hadoop/Desktop/PyFlink-Tutorial/flink-sql-connector-kafka-3.1-SNAPSHOT.jar\")\n",
    "    # 打印从Kafka读取数据的信息\n",
    "    print(\"start reading data from kafka\")\n",
    "    # 创建FlinkKafkaConsumer实例\n",
    "    kafka_consumer = FlinkKafkaConsumer(\n",
    "        topics='hamlet', # The topic to consume messages from\n",
    "        deserialization_schema= SimpleStringSchema('UTF-8'), # The schema to deserialize messages\n",
    "        properties={'bootstrap.servers': 'localhost:9092', 'group.id': 'my-group'} # The Kafka broker address and consumer group ID\n",
    "    )\n",
    "    # 从最早的记录开始读取数据\n",
    "    kafka_consumer.set_start_from_earliest()\n",
    "    # 将FlinkKafkaConsumer实例添加到StreamExecutionEnvironment实例中\n",
    "    stream = env.add_source(kafka_consumer)\n",
    "    # 将StreamExecutionEnvironment实例中的数据映射为去除标点符号的文本\n",
    "    stream_remove_punctuation = stream.map(lambda x: remove_punctuation(x))\n",
    "    # 将去除标点符号的文本映射为统计单词的文本\n",
    "    stream_count_words = stream_remove_punctuation.map(lambda x: count_words(x))\n",
    "    # 打印统计单词的文本\n",
    "    stream_count_words.print()\n",
    "    # 执行StreamExecutionEnvironment实例\n",
    "    env.execute()\n",
    "\n",
    "# 调用read_from_kafka函数\n",
    "read_from_kafka()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更详细的词频统计\n",
    "\n",
    "# 导入 argparse、io、json、logging、os、pandas、re、Counter、StringIO、FlinkKafkaConsumer、StreamExecutionEnvironment、DataTypes、EnvironmentSettings、FormatDescriptor、Schema、StreamTableEnvironment、TableEnvironment、udf 模块\n",
    "import argparse\n",
    "import io\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from io import StringIO\n",
    "from pyflink.common import SimpleStringSchema, Time\n",
    "from pyflink.datastream.connectors.kafka import FlinkKafkaConsumer\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.table import (DataTypes, EnvironmentSettings, FormatDescriptor,\n",
    "                           Schema, StreamTableEnvironment, TableDescriptor,\n",
    "                           TableEnvironment, udf)\n",
    "from pyflink.table.expressions import col, lit\n",
    "\n",
    "# 定义去除标点符号的函数\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]','',text)\n",
    "\n",
    "# 定义计算字节数的函数\n",
    "def count_bytes(text):\n",
    "    return len(text.encode('utf-8'))\n",
    "\n",
    "# 定义计算单词数量的函数\n",
    "def count_words(text):\n",
    "    words = text.split()\n",
    "    result = dict(Counter(words))\n",
    "    max_word = max(result, key=result.get)\n",
    "    return {'total_bytes': count_bytes(text), 'total_words': len(words), 'most_frequent_word': max_word, 'most_frequent_word_count': result[max_word]}\n",
    "\n",
    "# 定义从Kafka读取数据的函数\n",
    "def read_from_kafka():\n",
    "    # 获取StreamExecutionEnvironment实例\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()  \n",
    "    # 添加flink-sql-connector-kafka-3.1-SNAPSHOT.jar包\n",
    "    env.add_jars(\"file:///home/hadoop/Desktop/PyFlink-Tutorial/flink-sql-connector-kafka-3.1-SNAPSHOT.jar\")\n",
    "    print(\"start reading data from kafka\")\n",
    "    # 创建FlinkKafkaConsumer实例，指定主题、反序列化函数、配置参数\n",
    "    kafka_consumer = FlinkKafkaConsumer(\n",
    "        topics='hamlet', \n",
    "        deserialization_schema= SimpleStringSchema('UTF-8'), \n",
    "        properties={'bootstrap.servers': 'localhost:9092', 'group.id': 'my-group'} \n",
    "    )\n",
    "    # 从最早的日志开始读取\n",
    "    kafka_consumer.set_start_from_earliest()\n",
    "    # 将Kafka日志流转换为流表\n",
    "    stream_original_text = env.add_source(kafka_consumer)\n",
    "    # 对流表中的每一行进行去除标点符号操作\n",
    "    stream_remove_punctuation = stream_original_text.map(lambda x: remove_punctuation(x))\n",
    "    # 对流表中的每一行进行计算单词数量的操作\n",
    "    stream_count_words = stream_remove_punctuation.map(lambda x: count_words(x))\n",
    "    # 将流表中的每一行打印出来\n",
    "    stream_count_words.print()\n",
    "    # 执行流计算\n",
    "    env.execute()\n",
    "read_from_kafka()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 玩转 CSV\n",
    "\n",
    "假设我们得到一个“data.csv”文件，其中包含任何内容，并且该文件中只有年份数据才是我们需要的。\n",
    "我们首先使用以下代码生成“StreamGeneratorCSV”，将“CSV”文件转换为“Kafka Stream”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一个简单的 CSV 流生成器\n",
    "\n",
    "#以下代码使用kafka-python模块将数据发送到本地Kafka集群。\n",
    "#此代码打开一个名为“hamlet.txt”的文本文件，并将其内容作为流发送到指定的 Kafka 主题“hamlet”：\n",
    "\n",
    "from kafka import KafkaProducer\n",
    "import time\n",
    "import os\n",
    "import chardet\n",
    "\n",
    "def send_file_to_kafka(file_path: str, topic: str, bootstrap_servers: str):\n",
    "    '''\n",
    "    Send a file to a Kafka topic\n",
    "    :param file_path: path to the local file\n",
    "    :param topic: Kafka topic to which the data should be sent\n",
    "    :param bootstrap_servers: address of the Kafka cluster\n",
    "    '''\n",
    "    # 创建一个KafkaProducer实例\n",
    "    producer = KafkaProducer(bootstrap_servers=bootstrap_servers)\n",
    "    # 获取文件大小\n",
    "    file_size = os.path.getsize(file_path)\n",
    "\n",
    "    # 检测文件编码\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        result = chardet.detect(f.read())\n",
    "        encoding = result[\"encoding\"]\n",
    "\n",
    "    # 获取文件行数\n",
    "    with open(file_path, \"r\", encoding=encoding) as f:\n",
    "        lines_total = len(f.readlines())\n",
    "\n",
    "    lines_send = 0\n",
    "    while True:\n",
    "        # 打开文件\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            while True:\n",
    "                # 读取文件10行\n",
    "                data = f.readlines(10)\n",
    "                if not data:\n",
    "                    break\n",
    "                # 将数据转换为字符串\n",
    "                data_str = str(data)\n",
    "                # 将字符串转换为字节\n",
    "                data_bytes = data_str.encode()\n",
    "                # 将字节发送到Kafka\n",
    "                producer.send(topic, data_bytes)\n",
    "                # 记录发送的行数\n",
    "                lines_send += 10\n",
    "                # 计算已发送的百分比\n",
    "                percent_sent = (lines_send / lines_total) * 100                \n",
    "                # 计算已发送的字节数\n",
    "                bytes_sent = len(data_bytes)\n",
    "                print(f\"Sent {bytes_sent} bytes {topic} {percent_sent:.2f}% sent\")\n",
    "                # 每3秒检查一次\n",
    "                time.sleep(3)\n",
    "                \n",
    "        # 询问是否继续发送\n",
    "        user_input = input(\"Press 'c' to continue sending the file or 'q' to quit: \")\n",
    "        if user_input == \"q\":\n",
    "            break\n",
    "# 调用send_file_to_kafka函数，将文件data.csv发送到Kafka主题data，Kafka集群的地址为localhost:9092\n",
    "send_file_to_kafka(\"./data.csv\",  \"data\", \"localhost:9092\")\n",
    "\n",
    "# 解释以上代码\n",
    "# 在这个代码中，send_file_to_kafka 函数接受三个参数：file_path、topic 和 bootstrap_servers。\n",
    "# file_path 是本地文件的路径，topic 是要将数据发送到的 Kafka 主题，bootstrap_servers 是 Kafka 集群的地址。\n",
    "# 该函数使用 with 语句打开文件，读取其内容，并将其作为流数据发送到指定的 Kafka 主题。在发送过程中，它会打印出传输进度，并使用 time.sleep 方法暂停 3 秒以控制发送速率。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 输出年份数值\n",
    "\n",
    "StreamShowerWithFlinkCSV.py 是一个使用 DataStream 处理 CSV 文件的 Python 脚本。实际上，下面的代码使用 re 函数。\n",
    "但这不重要，只是对从 CSV 文件生成的 DataStream 随便试试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StreamShowerWithFlinkCSV.py\n",
    "\n",
    "# 导入正则表达式模块、参数解析模块、日志模块、系统模块、numpy模块、pandas模块、pyflink模块\n",
    "import re\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from pyflink.table import StreamTableEnvironment\n",
    "from pyflink.common import WatermarkStrategy, Encoder, Types\n",
    "from pyflink.datastream import StreamExecutionEnvironment, RuntimeExecutionMode\n",
    "from pyflink.datastream.connectors.file_system import FileSource, StreamFormat, FileSink, OutputFileConfig, RollingPolicy\n",
    "from pyflink.common import Types, SimpleStringSchema\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.datastream.connectors.kafka import FlinkKafkaProducer, FlinkKafkaConsumer\n",
    "\n",
    "# 定义split函数，用于将字符串拆分成单个单词\n",
    "def split(line):\n",
    "    yield from line.split()\n",
    "\n",
    "# 定义read_from_kafka函数，用于从Kafka读取数据\n",
    "def read_from_kafka():\n",
    "    # 定义Kafka消费的起始年份\n",
    "    Year_Begin =1999\n",
    "    # 定义Kafka消费的结束年份\n",
    "    Year_End = 2023\n",
    "    # 获取StreamExecutionEnvironment实例\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()    \n",
    "    # 添加jars包\n",
    "    env.add_jars(\"file:///home/hadoop/Desktop/PyFlink-Tutorial/flink-sql-connector-kafka-3.1-SNAPSHOT.jar\")\n",
    "    # 打印开始读取Kafka数据\n",
    "    print(\"start reading data from kafka\")\n",
    "\n",
    "    # 创建Kafka消费者，用于从Kafka读取数据\n",
    "    kafka_consumer = FlinkKafkaConsumer(\n",
    "        topics='data', # The topic to consume messages from\n",
    "        deserialization_schema= SimpleStringSchema('UTF-8'), # The schema to deserialize messages\n",
    "        properties={'bootstrap.servers': 'localhost:9092', 'group.id': 'my-group'} # The Kafka broker address and consumer group ID\n",
    "    )\n",
    "\n",
    "    # 从最早的偏移量开始读取Kafka数据\n",
    "    kafka_consumer.set_start_from_earliest()\n",
    "\n",
    "    # 添加Kafka消费者，并过滤掉不在指定年份范围内的数据\n",
    "    env.add_source(kafka_consumer).map(lambda x: ' '.join(re.findall(r'\\d+', x))).filter(lambda x: any([Year_Begin <= int(i) <= Year_End for i in x.split()])).map(lambda x:  [i for i in x.split() if Year_Begin <= int(i) <= Year_End][0]).print()\n",
    "    # 执行StreamExecutionEnvironment\n",
    "    env.execute()\n",
    "\n",
    "# 调用read_from_kafka函数\n",
    "if __name__ == '__main__':\n",
    "    read_from_kafka()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MapFunction`: 将一个元素作为输入并将一个元素作为输出的函数。通过对每个元素应用转换，它可用于转换数据流。\n",
    "`FlatMapFunction`：将一个元素作为输入，并将零个、一个或多个元素作为输出的函数。它可通过对每个元素应用变换来转换数据流。\n",
    "`FilterFunction`: 将一个元素作为输入并返回一个布尔值的函数。它可用于删除不符合特定条件的元素，从而过滤数据流。\n",
    "`KeySelector`: 从元素中提取键的函数。它可用于按键对数据流中的元素进行分组。\n",
    "`ReduceFunction`: 还原函数 将两个元素作为输入并将一个元素作为输出的函数。它可以通过组合共享一个共同键的元素来聚合数据流。\n",
    "`WindowFunction`: 将元素窗口作为输入并将一个或多个元素作为输出的函数。它可用于在数据流上定义窗口，并对每个窗口内的元素进行转换。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
